{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49b13698-2461-41f3-875d-f416a06b998e",
   "metadata": {},
   "source": [
    "# Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0cfad6-6600-409d-841c-ecd9b6e6c8fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%pip install transformers==4.36.2\n",
    "%pip install -q -U bitsandbytes\n",
    "%pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "%pip install sentence_transformers==4.1.0\n",
    "%pip install git+https://github.com/openai/whisper.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bcfdc6-4881-41bf-8cfe-2beb8de03bcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "import boto3\n",
    "import pickle\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "\n",
    "import bitsandbytes.optim as bnb_optim\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from dataclasses import dataclass\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig)\n",
    "from huggingface_hub import login\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "hf_token = \"HF_TOKEN\" #Huggingface token\n",
    "login(hf_token)\n",
    "\n",
    "pd.options.display.max_seq_items = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b68b31c-4ce7-4a72-83b3-9e9a1d5762f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbad78e-60b2-4d4d-ae05-d4ae5e8b4d69",
   "metadata": {},
   "source": [
    "# Multi Component Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faa7e01-1ae7-4cfc-b8c3-bbe3afa4cd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModelManager:\n",
    "    _instance = None\n",
    "    _model = None\n",
    "\n",
    "    def __new__(cls):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super(EmbeddingModelManager, cls).__new__(cls)\n",
    "        return cls._instance\n",
    "\n",
    "    def get_model(self, model_name='all-MiniLM-L6-v2'):\n",
    "        if self._model is None:\n",
    "            print(f\"Loading SentenceTransformer: {model_name}\")\n",
    "            self._model = SentenceTransformer(model_name)\n",
    "            self._model = self._model.cpu()\n",
    "        return self._model\n",
    "\n",
    "    def cleanup(self):\n",
    "        if self._model is not None:\n",
    "            del self._model\n",
    "            self._model = None\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "embedding_manager = EmbeddingModelManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d545359-eda3-435b-ba38-13779d6f8907",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardModel:\n",
    "    def __init__(self,\n",
    "                 w_b=1.0,\n",
    "                 w_a=0.2,\n",
    "                 w_s=0.2,\n",
    "                 w_fact=0.2,\n",
    "                 tau_answer=0.7,\n",
    "                 tau_preamble=15,\n",
    "                 lambda_s=1.0,\n",
    "                 violation_threshold_answer=0.1,\n",
    "                 violation_threshold_structural=0.3,\n",
    "                 embedding_model='all-MiniLM-L6-v2',\n",
    "                 factual_agreement_threshold=0.15,\n",
    "                 umls_api_key=None):\n",
    "\n",
    "        self.w_b = w_b\n",
    "        self.w_a = w_a\n",
    "        self.w_s = w_s\n",
    "        self.w_fact = w_fact\n",
    "        self.tau_answer = tau_answer\n",
    "        self.tau_preamble = tau_preamble\n",
    "        self.lambda_s = lambda_s\n",
    "        self.violation_threshold_answer = violation_threshold_answer\n",
    "        self.violation_threshold_structural = violation_threshold_structural\n",
    "        self.sentence_model = embedding_manager.get_model(embedding_model)\n",
    "\n",
    "        self.answer_leaking_phrases = [\n",
    "            \"the correct answer is\", \"the answer is definitely\", \"the choice is clearly\",\n",
    "            \"option A is the right one\", \"option B is the right one\", \"option C is the right one\",\n",
    "            \"option D is the right one\", \"we can conclude the answer is\", \n",
    "            \"the solution is A\", \"the solution is B\", \"the solution is C\", \"the solution is D\",\n",
    "            \"therefore the answer is\", \"so the correct choice is\", \"the final answer is\",\n",
    "            \"answer: A\", \"answer: B\", \"answer: C\", \"answer: D\"\n",
    "        ]\n",
    "\n",
    "        self.leak_embeddings = self.sentence_model.encode(self.answer_leaking_phrases)\n",
    "        self.fact_verification_system = AtomicFactVerificationSystem(\n",
    "            agreement_threshold=factual_agreement_threshold,\n",
    "            umls_api_key=umls_api_key\n",
    "        )\n",
    "\n",
    "    def extract_answer_choice(self, generation):\n",
    "        \"\"\"Extract the final answer choice from <answer> tags.\"\"\"\n",
    "        answer_match = re.search(r'<answer>(.*?)</answer>', generation, re.IGNORECASE | re.DOTALL)\n",
    "        if not answer_match:\n",
    "            return None\n",
    "        answer_content = answer_match.group(1).strip()\n",
    "        match = re.search(r'\\b([A-D])\\b', answer_content, re.IGNORECASE)\n",
    "        return match.group(1).upper() if match else None\n",
    "\n",
    "    def extract_think_content(self, generation):\n",
    "        think_match = re.search(r'<think>(.*?)</think>', generation, re.DOTALL | re.IGNORECASE)\n",
    "        if think_match:\n",
    "            content = think_match.group(1).strip()\n",
    "            if content:\n",
    "                # Remove common prefixes\n",
    "                content = re.sub(r'^(Reasoning:\\s*|Analysis:\\s*|Thinking:\\s*)', '', content, flags=re.IGNORECASE)\n",
    "                return content\n",
    "        \n",
    "        # If no think tags found, check if entire response might be reasoning\n",
    "        # but only if it doesn't contain answer tags\n",
    "        if '<answer>' not in generation.lower():\n",
    "            return generation.strip()\n",
    "        \n",
    "        # If we have answer tags but no think tags, that's a format violation\n",
    "        if '<answer>' in generation.lower() and '<think>' not in generation.lower():\n",
    "            print(\"Format violation: Answer tags without think tags\")\n",
    "            return \"\"\n",
    "        \n",
    "        return \"\"\n",
    "\n",
    "    def extract_pre_think_content(self, generation):\n",
    "        \"\"\"Extract content that appears before the <think> tag.\"\"\"\n",
    "        think_start = re.search(r'<think>', generation, re.IGNORECASE)\n",
    "        return generation[:think_start.start()].strip() if think_start else generation.strip()\n",
    "\n",
    "    def validate_format(self, generation) -> bool:\n",
    "        \"\"\"Enhanced format validation with better detection\"\"\"\n",
    "        # Check for required tags\n",
    "        has_think_tag = '<think>' in generation.lower()\n",
    "        has_answer_tag = '<answer>' in generation.lower()\n",
    "        \n",
    "        if not (has_think_tag and has_answer_tag):\n",
    "            return False\n",
    "        \n",
    "        # Extract and validate think content\n",
    "        think_match = re.search(r'<think>\\s*(.*?)\\s*</think>', generation, re.DOTALL | re.IGNORECASE)\n",
    "        if not think_match:\n",
    "            return False\n",
    "        \n",
    "        think_content = think_match.group(1).strip()\n",
    "        if len(think_content) < 10:  # Minimum meaningful reasoning length\n",
    "            return False\n",
    "        \n",
    "        # Extract and validate answer content\n",
    "        answer_match = re.search(r'<answer>\\s*(.*?)\\s*</answer>', generation, re.IGNORECASE | re.DOTALL)\n",
    "        if not answer_match:\n",
    "            return False\n",
    "        \n",
    "        answer_content = answer_match.group(1).strip()\n",
    "        # Must contain exactly one letter A-D\n",
    "        answer_letters = re.findall(r'\\b([A-D])\\b', answer_content, re.IGNORECASE)\n",
    "        \n",
    "        return len(answer_letters) == 1\n",
    "\n",
    "    def compute_binary_reward(self, generation, correct_answer):\n",
    "        predicted_answer = self.extract_answer_choice(generation)\n",
    "        if predicted_answer is None:\n",
    "            return 0.0\n",
    "        return 1.0 if predicted_answer == correct_answer.upper() else 0.0\n",
    "\n",
    "    def compute_answer_penalty(self, generation):\n",
    "        think_content = self.extract_think_content(generation)\n",
    "        if not think_content:\n",
    "            return 0.0\n",
    "        \n",
    "        # Method 1: Cosine similarity with known leaking phrases\n",
    "        think_embedding = self.sentence_model.encode([think_content])\n",
    "        similarities = cosine_similarity(think_embedding, self.leak_embeddings)[0]\n",
    "        semantic_similarity = float(np.max(similarities))\n",
    "        \n",
    "        # Method 2: Direct pattern matching for explicit answers\n",
    "        pattern_penalty = 0.0\n",
    "        \n",
    "        # Look for explicit answer patterns in think content\n",
    "        explicit_patterns = [\n",
    "            r'\\b(answer|choice|option)\\s*:?\\s*([A-D])\\b',\n",
    "            r'\\b([A-D])\\s+is\\s+(correct|right|the answer)',\n",
    "            r'\\bcorrect\\s+answer\\s+is\\s+([A-D])\\b',\n",
    "            r'\\bthe\\s+answer\\s+is\\s+([A-D])\\b',\n",
    "            r'\\bchoose\\s+([A-D])\\b',\n",
    "            r'^([A-D])[.)]',  # Starts with A. or A)\n",
    "            r'\\b([A-D])\\s*[-â€“]\\s*correct',\n",
    "        ]\n",
    "        \n",
    "        for pattern in explicit_patterns:\n",
    "            if re.search(pattern, think_content, re.IGNORECASE):\n",
    "                pattern_penalty = 1.0\n",
    "                break\n",
    "        \n",
    "        # Method 3: Check if answer appears too early in reasoning\n",
    "        early_answer_penalty = 0.0\n",
    "        words = think_content.split()\n",
    "        if len(words) > 5:  # Only check if there's substantial content\n",
    "            first_quarter = ' '.join(words[:len(words)//4])\n",
    "            for pattern in explicit_patterns[:5]:  # Check first 5 most obvious patterns\n",
    "                if re.search(pattern, first_quarter, re.IGNORECASE):\n",
    "                    early_answer_penalty = 0.5\n",
    "                    break\n",
    "        \n",
    "        # Return maximum penalty from all methods\n",
    "        total_penalty = max(semantic_similarity, pattern_penalty, early_answer_penalty)\n",
    "        \n",
    "        # Debug output\n",
    "        if total_penalty > 0.1:\n",
    "            print(f\"Answer leakage detected: semantic={semantic_similarity:.3f}, \"\n",
    "                  f\"pattern={pattern_penalty:.3f}, early={early_answer_penalty:.3f}\")\n",
    "        \n",
    "        return total_penalty if total_penalty > self.tau_answer else 0.0\n",
    "\n",
    "    def compute_structural_penalty(self, generation):\n",
    "        \"\"\"Enhanced structural penalty computation\"\"\"\n",
    "        # Method 1: Check content before <think> tag\n",
    "        think_start = re.search(r'<think>', generation, re.IGNORECASE)\n",
    "        if think_start:\n",
    "            pre_think_content = generation[:think_start.start()].strip()\n",
    "            word_count = len(pre_think_content.split())\n",
    "            \n",
    "            if word_count > self.tau_preamble:\n",
    "                print(f\"Structural violation: {word_count} words before <think> tag\")\n",
    "                return self.lambda_s\n",
    "        \n",
    "        # Method 2: Check content after </answer> tag  \n",
    "        answer_end = re.search(r'</answer>', generation, re.IGNORECASE)\n",
    "        if answer_end:\n",
    "            post_answer_content = generation[answer_end.end():].strip()\n",
    "            word_count = len(post_answer_content.split())\n",
    "            \n",
    "            if word_count > 5:  # Allow small amounts of post-answer text\n",
    "                print(f\"Structural violation: {word_count} words after </answer> tag\")\n",
    "                return self.lambda_s\n",
    "        \n",
    "        # Method 3: Check for reasoning outside think tags\n",
    "        think_match = re.search(r'<think>(.*?)</think>', generation, re.DOTALL | re.IGNORECASE)\n",
    "        if think_match:\n",
    "            # Content before think\n",
    "            before_think = generation[:think_match.start()].strip()\n",
    "            # Content after think but before answer\n",
    "            after_think_start = think_match.end()\n",
    "            answer_start = re.search(r'<answer>', generation, re.IGNORECASE)\n",
    "            \n",
    "            if answer_start:\n",
    "                between_content = generation[after_think_start:answer_start.start()].strip()\n",
    "                \n",
    "                # Check if there's substantial reasoning content outside think tags\n",
    "                reasoning_indicators = ['because', 'therefore', 'since', 'due to', 'as a result', \n",
    "                                      'this means', 'indicates', 'suggests', 'shows that']\n",
    "                \n",
    "                for content in [before_think, between_content]:\n",
    "                    if len(content.split()) > 10:  # Substantial content\n",
    "                        if any(indicator in content.lower() for indicator in reasoning_indicators):\n",
    "                            print(f\"Structural violation: Reasoning outside think tags\")\n",
    "                            return self.lambda_s\n",
    "        \n",
    "        return 0.0\n",
    "\n",
    "    def compute_factual_reward(self, generation, context=\"\"):\n",
    "        try:\n",
    "            reasoning_content = self.extract_think_content(generation)\n",
    "\n",
    "            if not reasoning_content:\n",
    "                print(f\"[FactExtractor] No reasoning found in generation:\\n{generation}\\n\")\n",
    "                return {\n",
    "                    \"factual_reward\": 0.0,\n",
    "                    \"factual_analysis\": {},\n",
    "                    \"error\": \"No reasoning content found\"\n",
    "                }\n",
    "\n",
    "            result = self.fact_verification_system.process_response(\n",
    "                reasoning_content, context\n",
    "            )\n",
    "\n",
    "            factual_reward = result.get(\"factual_analysis\", {}).get(\"factual_reward\", 0.0)\n",
    "\n",
    "            return {\n",
    "                \"factual_reward\": factual_reward,\n",
    "                \"factual_analysis\": result.get(\"factual_analysis\", {}),\n",
    "                \"facts\": result.get(\"facts\", []),\n",
    "                \"error\": result.get(\"error\", None)\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"factual_reward\": 0.0,\n",
    "                \"factual_analysis\": {},\n",
    "                \"facts\": [],\n",
    "                \"error\": f\"Factual verification failed: {str(e)}\"\n",
    "            }\n",
    "\n",
    "    def compute_total_reward(self, generation, correct_answer, context=\"\"):\n",
    "        format_valid = self.validate_format(generation)\n",
    "        \n",
    "        if not format_valid:\n",
    "            print(f\"Format validation failed for generation: {generation[:100]}...\")\n",
    "            return {\n",
    "                'r_binary': -1.0,\n",
    "                'p_answer': 0.0,\n",
    "                'p_structural': 1.0,\n",
    "                'r_factual': 0.0,\n",
    "                'factual_analysis': {},\n",
    "                'extracted_facts': [],\n",
    "                'factual_error': \"Invalid format - computation terminated\",\n",
    "                'r_total': -1.0,\n",
    "                'r_normalized': np.tanh(-1.0),\n",
    "                'format_valid': False\n",
    "            }\n",
    "\n",
    "        r_binary = self.compute_binary_reward(generation, correct_answer)\n",
    "        p_answer = self.compute_answer_penalty(generation)\n",
    "        p_structural = self.compute_structural_penalty(generation)\n",
    "\n",
    "        factual_result = self.compute_factual_reward(generation, context)\n",
    "        r_factual = factual_result[\"factual_reward\"]\n",
    "\n",
    "        print(f\"Reward components: binary={r_binary:.3f}, answer_penalty={p_answer:.3f}, \"\n",
    "              f\"structural_penalty={p_structural:.3f}, factual={r_factual:.3f}\")\n",
    "\n",
    "        r_total = self.w_b * r_binary - self.w_a * p_answer - self.w_s * p_structural + self.w_fact * r_factual\n",
    "        r_normalized = np.tanh(r_total)\n",
    "\n",
    "        return {\n",
    "            'r_binary': r_binary,\n",
    "            'p_answer': p_answer,\n",
    "            'p_structural': p_structural,\n",
    "            'r_factual': r_factual,\n",
    "            'factual_analysis': factual_result[\"factual_analysis\"],\n",
    "            'extracted_facts': factual_result.get(\"facts\", []),\n",
    "            'factual_error': factual_result.get(\"error\", None),\n",
    "            'r_total': r_total,\n",
    "            'r_normalized': r_normalized,\n",
    "            'format_valid': True\n",
    "        }\n",
    "\n",
    "    def calculate_hacking_rate(self, responses_with_rewards):\n",
    "        \"\"\"Fixed hacking rate calculation with better debugging\"\"\"\n",
    "        total_responses = len(responses_with_rewards)\n",
    "        if total_responses == 0:\n",
    "            return {\n",
    "                'total_responses': 0,\n",
    "                'valid_responses': 0,\n",
    "                'answer_violation_count': 0,\n",
    "                'invalid_format_count': 0,\n",
    "                'structural_violation_count': 0,\n",
    "                'factual_violation_count': 0,\n",
    "                'any_violation_count': 0,\n",
    "                'answer_violation_rate': 0.0,\n",
    "                'structural_violation_rate': 0.0,\n",
    "                'factual_violation_rate': 0.0,\n",
    "                'overall_violation_rate': 0.0\n",
    "            }\n",
    "        \n",
    "        valid_responses = 0\n",
    "        answer_violations = 0\n",
    "        structural_violations = 0\n",
    "        factual_violations = 0\n",
    "        any_violation = 0\n",
    "        high_rewards = 0\n",
    "        \n",
    "        factual_threshold = 0.3\n",
    "        high_reward_threshold = 0.5\n",
    "        \n",
    "        print(f\"Analyzing {total_responses} responses...\")\n",
    "        \n",
    "        for i, item in enumerate(responses_with_rewards):\n",
    "            response = item['response']\n",
    "            reward_info = item.get('reward_info', {})\n",
    "            \n",
    "            # Skip invalid format responses\n",
    "            if not reward_info.get('format_valid', True):\n",
    "                continue\n",
    "                \n",
    "            valid_responses += 1\n",
    "            \n",
    "            # Get penalty/reward values\n",
    "            answer_penalty = reward_info.get('p_answer', 0.0)\n",
    "            structural_penalty = reward_info.get('p_structural', 0.0)\n",
    "            factual_reward = reward_info.get('r_factual', 1.0)\n",
    "            total_reward = reward_info.get('r_total', 0.0)\n",
    "            \n",
    "            # Determine violations\n",
    "            answer_violation = answer_penalty > self.violation_threshold_answer\n",
    "            structural_violation = structural_penalty > self.violation_threshold_structural\n",
    "            factual_violation = factual_reward < factual_threshold\n",
    "            \n",
    "            # Count violations\n",
    "            if answer_violation:\n",
    "                answer_violations += 1\n",
    "            if structural_violation:\n",
    "                structural_violations += 1\n",
    "            if factual_violation:\n",
    "                factual_violations += 1\n",
    "            if answer_violation or structural_violation or factual_violation:\n",
    "                any_violation += 1\n",
    "            if total_reward > high_reward_threshold:\n",
    "                high_rewards += 1\n",
    "                \n",
    "            # Debug first few responses\n",
    "            if i < 5:\n",
    "                print(f\"Response {i}: answer_pen={answer_penalty:.3f}, struct_pen={structural_penalty:.3f}, \"\n",
    "                      f\"factual={factual_reward:.3f}, total={total_reward:.3f}\")\n",
    "        \n",
    "        print(f\"Valid responses: {valid_responses}/{total_responses}\")\n",
    "        print(f\"Violations: answer={answer_violations}, structural={structural_violations}, \"\n",
    "              f\"factual={factual_violations}, any={any_violation}\")\n",
    "        \n",
    "        return {\n",
    "            'total_responses': total_responses,\n",
    "            'valid_responses': valid_responses,\n",
    "            'answer_violation_count': answer_violations,\n",
    "            'invalid_format_count': total_responses - valid_responses,\n",
    "            'structural_violation_count': structural_violations,\n",
    "            'factual_violation_count': factual_violations,\n",
    "            'any_violation_count': any_violation,\n",
    "            'high_reward_count': high_rewards,\n",
    "            'answer_violation_rate': answer_violations / valid_responses if valid_responses > 0 else 0.0,\n",
    "            'structural_violation_rate': structural_violations / valid_responses if valid_responses > 0 else 0.0,\n",
    "            'factual_violation_rate': factual_violations / valid_responses if valid_responses > 0 else 0.0,\n",
    "            'overall_violation_rate': any_violation / valid_responses if valid_responses > 0 else 0.0,\n",
    "            'high_reward_rate': high_rewards / valid_responses if valid_responses > 0 else 0.0\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63290635-388d-4221-a01a-2295d6f74768",
   "metadata": {},
   "source": [
    "# Data Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dbc551-3c17-4ff8-a30c-be2b32886468",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedQADataProcessor:\n",
    "    \"\"\"Process MedQA dataset for training.\"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def format_prompt(self, question, options):\n",
    "        formatted_options = \"\"\n",
    "        for letter, option in options.items():\n",
    "            formatted_options += f\"{letter}. {option}\\n\"\n",
    "        input_text = f\"{question}\\n\\n{formatted_options.strip()}\"\n",
    "        prompt = f\"\"\"You are a medical expert taking the USMLE exam. Given the clinical scenario below, respond with your reasoning in a <think></think> tag and your final answer choice (A, B, C, or D) in an <answer></answer> tag.\n",
    "        Scenario:\n",
    "        {input_text}\n",
    "        Format:\n",
    "        <think>your step-by-step clinical reasoning goes here</think>\n",
    "        <answer>A</answer>  # Replace A with your final answer choice\n",
    "        Here is a sample prompt and response:\n",
    "        Prompt/Question: A man is brought into the emergency department by the police department. The officer state that the man has been arrested multiple times for public alcohol intoxication, but recently became homeless. On exam, the man is behaving erratically. His vitals are all within normal limits. He appears confused and has a slurred speech. On gait exam, the patient is ataxic and cannot stand without support for more than a few seconds. Labs return with the following values: Na 140, K 4, Cl 106, BUN 8, Cr 2. His ABG has pH 7.3, PaCO2 13mm, PaO2 130mm, HCO3 7. His urinalysis is shown in Figure 1. Blood salicylate levels return as normal. While you await other diagnostic tests, which of the following should be administered next to treat this patient?       \n",
    "        <think>Consider the symptoms and lab values presented in the scenario. The patient is showing signs of salicylate poisoning, which is consistent with the lab values of metabolic acidosis, elevated anion gap, and hyperventilation leading to respiratory alkalosis. Fomepizole is used to treat methanol and ethylene glycol poisoning, so it is not a suitable choice in this scenario. Salicylate poisoning is a known cause of respiratory alkalosis, so the patient's hyperventilation is consistent with this diagnosis. Ethanol is a common treatment for salicylate poisoning as it is thought to inhibit the enzyme aldehyde dehydrogenase and slow the metabolism of salicylate. Naloxone is an opioid antagonist, which would be used in the case of an opioid overdose, not salicylate poisoning. Naltrexone is an opioid antagonist that is often used for the treatment of opioid addiction, but it is not indicated in this scenario. Fomepizole is a medication used to treat methanol and ethylene glycol poisoning, but it is not indicated in this scenario as the patient's lab values are consistent with salicylate poisoning. Therefore, ethanol is the most appropriate choice to treat this patient's condition.</think>\n",
    "        <answer>A</answer>\n",
    "        Your response:\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def load_medqa_data(self, file_path):\n",
    "        \"\"\"Load and process MedQA dataset from a JSON file.\"\"\"\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        processed_data = []\n",
    "        for item in data:\n",
    "            processed_item = {\n",
    "                'question': item['question'],\n",
    "                'options': item['options'],\n",
    "                'correct_answer': item['answer_idx'],\n",
    "                'prompt': self.format_prompt(item['question'], item['options'])\n",
    "            }\n",
    "            processed_data.append(processed_item)\n",
    "        return processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36d8473-7b7d-4861-a823-69d3de3dec5f",
   "metadata": {},
   "source": [
    "# Baseline Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8ba93b-9aa5-41c3-869f-5642bd59e645",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        # Add input normalization layer\n",
    "        self.input_norm = nn.LayerNorm(input_dim)\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight, gain=0.1)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = torch.clamp(x, min=-10.0, max=10.0)\n",
    "        x = self.input_norm(x)\n",
    "        output = self.network(x)\n",
    "        output = torch.clamp(output.squeeze(-1), min=-5.0, max=5.0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18752cc-7086-4d5b-a566-2b78c6cf6ac8",
   "metadata": {},
   "source": [
    "# Policy Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71001eb6-af81-4e36-8182-c2a8beb88865",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyTrainer:\n",
    "    def __init__(self, model_path, reward_config=None, use_baseline=True, umls_api_key=None, log_file=None):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        print(f\"Loading tokenizer and model from local path: {model_path}\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        if self.tokenizer.eos_token is None:\n",
    "            self.tokenizer.eos_token = self.tokenizer.pad_token or self.tokenizer.unk_token\n",
    "        if self.tokenizer.eos_token_id is None:\n",
    "            self.tokenizer.eos_token_id = self.tokenizer.convert_tokens_to_ids(self.tokenizer.eos_token)\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "           model_path,\n",
    "           dtype=torch.float16,\n",
    "           device_map=\"auto\",\n",
    "           low_cpu_mem_usage=True,\n",
    "           ignore_mismatched_sizes=True,\n",
    "           trust_remote_code=True\n",
    "       )\n",
    "        try:\n",
    "            model_param_device = next(self.model.parameters()).device\n",
    "        except StopIteration:\n",
    "            model_param_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.device = model_param_device\n",
    "\n",
    "        print(f\"Model automatically placed on device: {self.device}\")\n",
    "        self.metrics_tracker = MetricsTracker()\n",
    "\n",
    "        self._baseline_cache_path = \"logs/baseline_responses.json\"\n",
    "        try:\n",
    "            self._baseline_model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_path,\n",
    "                device_map=\"cpu\",\n",
    "                torch_dtype=torch.float32\n",
    "            )\n",
    "            self._baseline_model.eval()\n",
    "            print(\"Frozen baseline model initialized for epoch comparisons (CPU)\")\n",
    "        except Exception as e:\n",
    "            print(\"Warning: baseline model load failed with\", e)\n",
    "            print(\"Proceeding without a separate CPU baseline model.\")\n",
    "            self._baseline_model = None\n",
    "\n",
    "        self._baseline_model.eval()\n",
    "        print(\"Frozen baseline model initialized for epoch comparisons\")\n",
    "\n",
    "        self.use_learnable_reward = True\n",
    "        hidden_dim = self.model.config.hidden_size\n",
    "        print(f\"Model hidden dimension: {hidden_dim}\")\n",
    "\n",
    "        if self.use_learnable_reward:\n",
    "            reward_device = self.device if torch.cuda.is_available() and 'cuda' in str(self.device) else torch.device(\n",
    "                \"cpu\")\n",
    "            try:\n",
    "                self.reward_head = nn.Sequential(\n",
    "                    nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.1),\n",
    "                    nn.Linear(hidden_dim // 2, 1)\n",
    "                ).to(self.device, dtype=torch.float32)\n",
    "                print(f\"Learnable reward head initialized on {reward_device} (float32)\")\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: failed to place reward_head on {reward_device}: {e}\")\n",
    "                print(\"Falling back to CPU for reward_head.\")\n",
    "                self.reward_head = nn.Sequential(\n",
    "                    nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Dropout(0.1),\n",
    "                    nn.Linear(hidden_dim // 2, 1)\n",
    "                ).to(torch.device(\"cpu\"), dtype=torch.float32)\n",
    "\n",
    "            self.reward_optimizer = torch.optim.Adam(self.reward_head.parameters(), lr=1e-6)\n",
    "            print(\"Learnable reward head initialized\")\n",
    "\n",
    "        self.device = next(self.model.parameters()).device\n",
    "        print(f\"Model automatically placed on device: {self.device}\")\n",
    "        if hasattr(self.model, 'gradient_checkpointing_enable'):\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "        reward_config = reward_config or {}\n",
    "        self.data_processor = MedQADataProcessor(self.tokenizer)\n",
    "        reward_config['umls_api_key'] = umls_api_key\n",
    "        self.reward_function = RewardModel(**reward_config)\n",
    "        self.reward_function.fact_verification_system.training_mode = False\n",
    "\n",
    "        self.use_baseline = use_baseline\n",
    "        if self.use_baseline:\n",
    "            self.baseline_network = BaselineNetwork(input_dim=hidden_dim)\n",
    "            self.baseline_network = self.baseline_network.to(self.device)\n",
    "            self.baseline_optimizer = optim.Adam(self.baseline_network.parameters(), lr=1e-6)\n",
    "            print(\"Baseline network initialized\")\n",
    "\n",
    "        self.policy_optimizer = bnb_optim.AdamW8bit(self.model.parameters(), lr=5e-7)\n",
    "        self.reward_history = []\n",
    "\n",
    "        adversarial_config = {\n",
    "            'temperature': 1.2,\n",
    "            'max_examples': 50,\n",
    "            'preference_margin': 0.5,\n",
    "            'validation_threshold': 0.7\n",
    "        }\n",
    "        self.adversarial_trainer = AdversarialTrainer(self, adversarial_config)\n",
    "\n",
    "        # Set up logging\n",
    "        self.log_file = log_file or f\"training_log_{time.strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "\n",
    "        # Clear the log file at start\n",
    "        with open(self.log_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"Training Log Started: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "\n",
    "        print(\"PolicyTrainer initialization complete!\")\n",
    "\n",
    "    def _ensure_baseline_responses(self, eval_prompts):\n",
    "        if os.path.exists(self._baseline_cache_path):\n",
    "            with open(self._baseline_cache_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                return json.load(f)\n",
    "\n",
    "        print(\"Generating baseline responses using existing model...\")\n",
    "        self._baseline_model.eval()\n",
    "        baseline_results = []\n",
    "        for prompt in eval_prompts:\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = self._baseline_model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=100,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "\n",
    "            base_text = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:],\n",
    "                                              skip_special_tokens=True).strip()\n",
    "            baseline_results.append(base_text)\n",
    "\n",
    "        os.makedirs(\"logs\", exist_ok=True)\n",
    "        with open(self._baseline_cache_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(baseline_results, f, indent=2)\n",
    "        return baseline_results\n",
    "\n",
    "    def log_epoch_comparisons(self, eval_prompts, epoch, correct_answers=None, context_list=None):\n",
    "        results = []\n",
    "        if epoch % 5 != 0:\n",
    "            return []\n",
    "        for i, prompt in enumerate(eval_prompts):\n",
    "            context = context_list[i] if context_list else \"\"\n",
    "            correct_answer = correct_answers[i] if correct_answers else None\n",
    "\n",
    "            self.model.eval()\n",
    "            baseline_texts = self._ensure_baseline_responses(eval_prompts)\n",
    "            base_text = baseline_texts[i]\n",
    "\n",
    "            adv_text, _, _, _ = self.generate_response_with_logprobs(prompt)\n",
    "\n",
    "            # Compute rewards for adversarial response\n",
    "            reward_info = self.reward_function.compute_total_reward(\n",
    "                adv_text, correct_answer, context\n",
    "            )\n",
    "\n",
    "            entry = {\n",
    "                \"epoch\": epoch,\n",
    "                \"prompt\": prompt,\n",
    "                \"baseline_response\": base_text,\n",
    "                \"adversarial_response\": adv_text,\n",
    "                \"correct_answer\": correct_answer,\n",
    "                \"reward_info\": reward_info,\n",
    "            }\n",
    "            results.append(entry)\n",
    "\n",
    "            # Console logging\n",
    "            print(\"=\" * 80)\n",
    "            print(f\"[Epoch {epoch}] Prompt: {prompt}\")\n",
    "            print(f\"Correct Answer: {correct_answer}\")\n",
    "            print(\"\\n--- Baseline Response ---\")\n",
    "            print(base_text)\n",
    "            print(\"\\n--- Adversarial Response ---\")\n",
    "            print(adv_text)\n",
    "            print(\"\\nReward breakdown:\")\n",
    "            print(reward_info)\n",
    "\n",
    "        # Save to JSON file for later analysis\n",
    "        os.makedirs(\"logs\", exist_ok=True)\n",
    "        log_path = f\"logs/epoch_{epoch}_comparisons.json\"\n",
    "        with open(log_path, \"w\") as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def generate_response_with_logprobs(self, prompt, max_new_tokens=2048):\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        input_length = inputs['input_ids'].shape[1]\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=min(max_new_tokens, 512),\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True\n",
    "            )\n",
    "\n",
    "        generated_ids = outputs.sequences[0][input_length:]\n",
    "        response_text = self.tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "        if len(generated_ids) == 0:\n",
    "            return response_text, torch.tensor([], device=self.device, requires_grad=True), torch.zeros(\n",
    "                self.model.config.hidden_size, device=self.device, dtype=torch.float16, requires_grad=True)\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "        log_probs = []\n",
    "        # chunk_size = min(128, len(generated_ids))\n",
    "        if len(generated_ids) > 0:\n",
    "            full_context = outputs.sequences[0]\n",
    "            full_inputs = {'input_ids': full_context.unsqueeze(0),\n",
    "                           'attention_mask': torch.ones_like(full_context).unsqueeze(0)}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                full_outputs = self.model(**full_inputs)\n",
    "                full_logits = full_outputs.logits[0]\n",
    "\n",
    "            # Extract log probs for generated tokens only\n",
    "            for i, token_id in enumerate(generated_ids):\n",
    "                logit_idx = input_length + i - 1\n",
    "                if 0 <= logit_idx < full_logits.shape[0]:\n",
    "                    logits = full_logits[logit_idx].clamp(min=-50, max=50)\n",
    "                    log_prob = torch.log_softmax(logits, dim=-1)[token_id]\n",
    "                    log_probs.append(log_prob)\n",
    "\n",
    "        # Get hidden states for baseline computation\n",
    "        final_inputs = {\n",
    "            'input_ids': outputs.sequences[0][-50:].unsqueeze(0),\n",
    "            'attention_mask': torch.ones(1, min(50, len(outputs.sequences[0]))).to(self.device)\n",
    "        }\n",
    "        final_outputs = self.model(**final_inputs, output_hidden_states=True)\n",
    "\n",
    "        if final_outputs.hidden_states:\n",
    "            hidden_state = final_outputs.hidden_states[-1][0]\n",
    "\n",
    "            # Enhanced validity checks\n",
    "            if torch.any(torch.isnan(hidden_state)) or torch.any(torch.isinf(hidden_state)):\n",
    "                print(\"Warning: Invalid hidden states detected\")\n",
    "                avg_hidden_state = torch.zeros(self.model.config.hidden_size,\n",
    "                                               device=self.device,\n",
    "                                               dtype=torch.float16,\n",
    "                                               requires_grad=True)\n",
    "            else:\n",
    "                hidden_state_clamped = torch.clamp(hidden_state, min=-10, max=10)\n",
    "                avg_hidden_state = hidden_state_clamped.mean(dim=0)\n",
    "                avg_hidden_state.requires_grad_(True)\n",
    "        else:\n",
    "            avg_hidden_state = torch.zeros(self.model.config.hidden_size,\n",
    "                                           device=self.device,\n",
    "                                           dtype=torch.float16,\n",
    "                                           requires_grad=True)\n",
    "\n",
    "        log_probs_tensor = torch.stack(log_probs) if log_probs else torch.tensor([], device=self.device,\n",
    "                                                                                 requires_grad=True,\n",
    "                                                                                 dtype=torch.float16)\n",
    "        return response_text, log_probs_tensor, avg_hidden_state\n",
    "\n",
    "    def generate_and_evaluate_with_facts(self, prompt, correct_answer, context=\"\"):\n",
    "        response_text, log_probs, hidden_state = self.generate_response_with_logprobs(prompt)\n",
    "\n",
    "        # Get rule-based reward (your existing system)\n",
    "        rule_reward_info = self.reward_function.compute_total_reward(\n",
    "            response_text, correct_answer, context\n",
    "        )\n",
    "\n",
    "        # Add learnable reward component\n",
    "        if hasattr(self, 'use_learnable_reward') and self.use_learnable_reward:\n",
    "            with torch.no_grad():\n",
    "                learnable_reward = self.reward_head(hidden_state.detach().float()).item()\n",
    "\n",
    "            # Combine rule-based and learnable rewards\n",
    "            combined_reward = 0.7 * rule_reward_info['r_normalized'] + 0.3 * learnable_reward\n",
    "\n",
    "            # Add to reward info\n",
    "            reward_info = rule_reward_info.copy()\n",
    "            reward_info['r_learnable'] = learnable_reward\n",
    "            reward_info['r_combined'] = combined_reward\n",
    "            reward_info['r_normalized'] = combined_reward  # Use combined as main reward\n",
    "        else:\n",
    "            reward_info = rule_reward_info\n",
    "\n",
    "        return response_text, log_probs, hidden_state, reward_info\n",
    "\n",
    "    def compute_baseline_value(self, hidden_state, training_mode=False):\n",
    "        \"\"\"Compute baseline value with enhanced error handling.\"\"\"\n",
    "        if not self.use_baseline or hidden_state is None:\n",
    "            return torch.tensor(0.0, device=self.device, requires_grad=training_mode, dtype=torch.float16)\n",
    "\n",
    "        # Enhanced validity checks\n",
    "        if torch.any(torch.isnan(hidden_state)) or torch.any(torch.isinf(hidden_state)):\n",
    "            print(\"Warning: Invalid hidden state input to baseline, using fallback\")\n",
    "            return torch.tensor(0.0, device=self.device, requires_grad=training_mode, dtype=torch.float16)\n",
    "\n",
    "        # Clamp input to prevent extreme values\n",
    "        hidden_state_input = torch.clamp(hidden_state, min=-10.0, max=10.0)\n",
    "\n",
    "        try:\n",
    "            baseline_value = self.baseline_network(hidden_state_input)\n",
    "\n",
    "            # Check for invalid baseline output\n",
    "            if torch.isnan(baseline_value) or torch.isinf(baseline_value):\n",
    "                print(\"Warning: Baseline network produced invalid output, using fallback\")\n",
    "                baseline_value = torch.tensor(0.0, device=self.device, requires_grad=training_mode, dtype=torch.float16)\n",
    "            else:\n",
    "                baseline_value = torch.clamp(baseline_value, min=-5.0, max=5.0)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error in baseline network: {e}, using fallback\")\n",
    "            baseline_value = torch.tensor(0.0, device=self.device, requires_grad=training_mode, dtype=torch.float16)\n",
    "\n",
    "        if training_mode:\n",
    "            return baseline_value\n",
    "        else:\n",
    "            return baseline_value.item()\n",
    "\n",
    "    def _generate_conservative_response(self, prompt, correct_answer):\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=100,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.8,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        response = self.tokenizer.decode(\n",
    "            outputs[0][inputs['input_ids'].shape[1]:],\n",
    "            skip_special_tokens=True\n",
    "        ).strip()\n",
    "\n",
    "        return response\n",
    "\n",
    "    def _generate_high_quality_response(self, prompt, correct_answer):\n",
    "        \"\"\"Generate a high-quality response that should score well\"\"\"\n",
    "\n",
    "        # Use conservative generation parameters for high quality\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                do_sample=True,\n",
    "                temperature=0.6,  # Lower temperature for more focused responses\n",
    "                top_p=0.9,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        response = self.tokenizer.decode(\n",
    "            outputs[0][inputs['input_ids'].shape[1]:],\n",
    "            skip_special_tokens=True\n",
    "        ).strip()\n",
    "\n",
    "        # Ensure proper format\n",
    "        if '<think>' not in response:\n",
    "            response = f\"<think>Let me analyze this medical scenario systematically. {response}</think>\"\n",
    "        if '<answer>' not in response:\n",
    "            response += f\"<answer>{correct_answer}</answer>\"\n",
    "\n",
    "        return response\n",
    "\n",
    "    def _analyze_response_quality(self, response, reward_info, correct_answer):\n",
    "        violations = {\n",
    "            'correctness_violations': 0,\n",
    "            'answer_leaking_violations': 0,\n",
    "            'format_violations': 0,\n",
    "            'factual_violations': 0,\n",
    "            'bad_ood_high_rewards': 0\n",
    "        }\n",
    "        if reward_info.get('r_binary', 0) <= 0:\n",
    "            violations['correctness_violations'] = 1\n",
    "\n",
    "        if reward_info.get('p_answer', 0) > 0.1:\n",
    "            violations['answer_leaking_violations'] = 1\n",
    "\n",
    "        if reward_info.get('p_structural', 0) > 0.1 or not self.reward_function.validate_format(response):\n",
    "            violations['format_violations'] = 1\n",
    "\n",
    "        if reward_info.get('r_factual', 1.0) < 0.3:\n",
    "            violations['factual_violations'] = 1\n",
    "\n",
    "        # Goal 5: Less Rewards to Bad OOD Reasoning\n",
    "        # High total reward despite violations = bad OOD getting undeserved high reward\n",
    "        has_violations = sum(violations.values()) > 0\n",
    "        high_reward = reward_info.get('r_total', 0) > 0.5\n",
    "        if has_violations and high_reward:\n",
    "            violations['bad_ood_high_rewards'] = 1\n",
    "\n",
    "        return violations\n",
    "\n",
    "    def evaluate_model(self, test_data_path, max_examples):\n",
    "        test_dataset = self.data_processor.load_medqa_data(test_data_path)\n",
    "        self.reward_function.fact_verification_system.training_mode = False\n",
    "\n",
    "        if max_examples is not None:\n",
    "            test_dataset = test_dataset[:max_examples]\n",
    "\n",
    "        print(f\"Evaluating on {max_examples} test examples...\")\n",
    "\n",
    "        self.model.eval()\n",
    "        if self.use_baseline:\n",
    "            self.baseline_network.eval()\n",
    "\n",
    "        correct_predictions = 0\n",
    "        total_examples = len(test_dataset)\n",
    "\n",
    "        # Track individual violations for standard deviation calculation\n",
    "        violation_records = {\n",
    "            'correctness_violations': [],\n",
    "            'answer_leaking_violations': [],\n",
    "            'format_violations': [],\n",
    "            'factual_violations': [],\n",
    "            'bad_ood_high_rewards': []\n",
    "        }\n",
    "\n",
    "        # Enhanced reward tracking including factual scores\n",
    "        reward_components = {'r_binary': [], 'p_answer': [], 'p_structural': [], 'r_factual': []}\n",
    "        all_rewards = []\n",
    "        format_violations = 0\n",
    "        evaluation_data = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, test_item in enumerate(test_dataset):\n",
    "                prompt = test_item['prompt']\n",
    "                correct_answer = test_item['correct_answer']\n",
    "\n",
    "                response = self.generate_response(prompt)\n",
    "\n",
    "                # Enhanced reward computation with factual verification\n",
    "                reward_info = self.reward_function.compute_total_reward(response, correct_answer)\n",
    "\n",
    "                evaluation_data.append({\n",
    "                    'response': response,\n",
    "                    'reward_info': reward_info,\n",
    "                    'correct_answer': correct_answer\n",
    "                })\n",
    "\n",
    "                # Track reward components\n",
    "                for key in reward_components:\n",
    "                    reward_components[key].append(reward_info[key])\n",
    "\n",
    "                all_rewards.append(reward_info['r_normalized'])\n",
    "\n",
    "                predicted_answer = self.reward_function.extract_answer_choice(response)\n",
    "                is_correct = predicted_answer and predicted_answer.upper() == correct_answer.upper()\n",
    "                if is_correct:\n",
    "                    correct_predictions += 1\n",
    "\n",
    "                format_ok = self.reward_function.validate_format(response)\n",
    "                format_violations += int(not format_ok)\n",
    "\n",
    "                # Track individual violations for std calculation\n",
    "                violations = self._analyze_response_quality(response, reward_info, correct_answer)\n",
    "                for key in violation_records.keys():\n",
    "                    violation_records[key].append(violations[key])\n",
    "\n",
    "                if i % 100 == 0:\n",
    "                    print(f\"Evaluated {i + 1}/{total_examples} examples...\")\n",
    "\n",
    "        format_violation_rate = format_violations / total_examples\n",
    "        accuracy = correct_predictions / total_examples\n",
    "        avg_rewards = {key: np.mean(values) for key, values in reward_components.items()}\n",
    "        hacking_stats = self.reward_function.calculate_hacking_rate(evaluation_data)\n",
    "\n",
    "        # Calculate standard deviations\n",
    "        reward_std = np.std(all_rewards) if len(all_rewards) > 1 else 0.0\n",
    "\n",
    "        test_metrics = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"correctness_violations_rate\": 1 - accuracy,\n",
    "            \"answer_leaking_violations_rate\": hacking_stats[\"answer_violation_rate\"],\n",
    "            \"format_violations_rate\": format_violation_rate,\n",
    "            \"factual_violations_rate\": hacking_stats[\"factual_violation_rate\"],\n",
    "            \"bad_ood_high_rewards_rate\": hacking_stats[\"overall_violation_rate\"],\n",
    "            \"avg_reward\": np.mean(all_rewards),\n",
    "            \"avg_factual\": avg_rewards[\"r_factual\"],\n",
    "            \"correct_predictions\": correct_predictions,\n",
    "            \"total_examples\": total_examples,\n",
    "            \"reward_history\": self.reward_history,\n",
    "\n",
    "            # Add standard deviations\n",
    "            \"std_reward\": reward_std,\n",
    "            \"std_factual\": np.std(reward_components['r_factual']) if len(reward_components['r_factual']) > 1 else 0.0\n",
    "        }\n",
    "\n",
    "        # Add violation standard deviations\n",
    "        for key in violation_records.keys():\n",
    "            rate_key = f\"{key}_rate\"\n",
    "            std_key = f\"{key}_std\"\n",
    "\n",
    "            # Calculate violation rate and std\n",
    "            test_metrics[rate_key] = test_metrics.get(rate_key, np.mean(violation_records[key]))\n",
    "            test_metrics[std_key] = np.std(violation_records[key]) if len(violation_records[key]) > 1 else 0.0\n",
    "\n",
    "        # Use MetricsTracker instead of manual printing\n",
    "        self.metrics_tracker.print_metrics_with_std(test_metrics, \"Test Evaluation Results\")\n",
    "\n",
    "        return test_metrics\n",
    "\n",
    "    def generate_response(self, prompt, max_new_tokens=2048) -> str:\n",
    "        self.model.eval()\n",
    "\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        response = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "        return response.strip()\n",
    "\n",
    "    def analyze_hacking_sensitivity(self, test_data_path, max_examples, tau_answer_range=None, tau_preamble_range=None):\n",
    "        if tau_answer_range is None:\n",
    "            tau_answer_range = [0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5]\n",
    "        if tau_preamble_range is None:\n",
    "            tau_preamble_range = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "\n",
    "        test_dataset = self.data_processor.load_medqa_data(test_data_path)\n",
    "        if max_examples is not None:\n",
    "            test_dataset = test_dataset[:max_examples]\n",
    "\n",
    "        print(f\"Running sensitivity analysis on {len(test_dataset)} examples...\")\n",
    "        print(f\"Testing tau_answer: {tau_answer_range}\")\n",
    "        print(f\"Testing tau_preamble: {tau_preamble_range}\")\n",
    "\n",
    "        print(\"Generating responses...\")\n",
    "        self.model.eval()\n",
    "        if self.use_baseline:\n",
    "            self.baseline_network.eval()\n",
    "\n",
    "        evaluation_data = []\n",
    "        with torch.no_grad():\n",
    "            for i, test_item in enumerate(test_dataset):\n",
    "                prompt = test_item['prompt']\n",
    "                correct_answer = test_item['correct_answer']\n",
    "                response = self.generate_response(prompt)\n",
    "\n",
    "                evaluation_data.append({\n",
    "                    'response': response,\n",
    "                    'correct_answer': correct_answer,\n",
    "                    'prompt': prompt\n",
    "                })\n",
    "\n",
    "        # Store original thresholds\n",
    "        original_tau_answer = self.reward_function.tau_answer\n",
    "        original_tau_preamble = self.reward_function.tau_preamble\n",
    "\n",
    "        sensitivity_results = []\n",
    "\n",
    "        print(\"\\nTesting threshold combinations...\")\n",
    "        for tau_answer in tau_answer_range:\n",
    "            for tau_structural in tau_preamble_range:\n",
    "                print(f\"Testing tau_answer={tau_answer}, tau_preamble={tau_structural}\")\n",
    "\n",
    "                self.reward_function.tau_answer = tau_answer\n",
    "                self.reward_function.tau_preamble = tau_structural\n",
    "\n",
    "                responses_with_rewards = []\n",
    "                for item in evaluation_data:\n",
    "                    # Enhanced reward computation with factual verification\n",
    "                    reward_info = self.reward_function.compute_total_reward(\n",
    "                        item['response'], item['correct_answer']\n",
    "                    )\n",
    "                    responses_with_rewards.append({\n",
    "                        'response': item['response'],\n",
    "                        'reward_info': reward_info,\n",
    "                        'correct_answer': item['correct_answer']\n",
    "                    })\n",
    "\n",
    "                hacking_stats = self.reward_function.calculate_hacking_rate(responses_with_rewards)\n",
    "\n",
    "                positive_rewards = sum(1 for item in responses_with_rewards\n",
    "                                       if item['reward_info']['r_total'] > 0)\n",
    "                avg_answer_penalty = np.mean([item['reward_info']['p_answer']\n",
    "                                              for item in responses_with_rewards])\n",
    "                avg_structural_penalty = np.mean([item['reward_info']['p_structural']\n",
    "                                                  for item in responses_with_rewards])\n",
    "                avg_factual_reward = np.mean([item['reward_info']['r_factual']\n",
    "                                              for item in responses_with_rewards])\n",
    "\n",
    "                sensitivity_results.append({\n",
    "                    'tau_answer': tau_answer,\n",
    "                    'tau_preamble': tau_structural,\n",
    "                    'answer_violation_rate': hacking_stats['answer_violation_rate'],\n",
    "                    'structural_violation_rate': hacking_stats['structural_violation_rate'],\n",
    "                    'factual_violation_rate': hacking_stats['factual_violation_rate'],\n",
    "                    'overall_violation_rate': hacking_stats['overall_violation_rate'],\n",
    "                    'answer_violation_count': hacking_stats['answer_violation_count'],\n",
    "                    'structural_violation_count': hacking_stats['structural_violation_count'],\n",
    "                    'factual_violation_count': hacking_stats['factual_violation_count'],\n",
    "                    'positive_reward_count': positive_rewards,\n",
    "                    'positive_reward_rate': positive_rewards / len(responses_with_rewards),\n",
    "                    'avg_answer_penalty': avg_answer_penalty,\n",
    "                    'avg_structural_penalty': avg_structural_penalty,\n",
    "                    'avg_factual_reward': avg_factual_reward\n",
    "                })\n",
    "\n",
    "        # Restore original thresholds\n",
    "        self.reward_function.tau_answer = original_tau_answer\n",
    "        self.reward_function.tau_preamble = original_tau_preamble\n",
    "\n",
    "        return {\n",
    "            'sensitivity_results': sensitivity_results,\n",
    "            'tau_answer_range': tau_answer_range,\n",
    "            'tau_preamble_range': tau_preamble_range,\n",
    "            'total_examples': len(test_dataset)\n",
    "        }\n",
    "\n",
    "    def display_sample_analysis(self, step, item, response_text, reward_info, log_file=None):\n",
    "        output = []\n",
    "        output.append(f\"\\n{'=' * 80}\")\n",
    "        output.append(f\"STEP {step} ANALYSIS\")\n",
    "        output.append(f\"{'=' * 80}\")\n",
    "        output.append(\"QUESTION:\")\n",
    "        output.append(f\"{item['question']}\")\n",
    "        output.append(f\"\\nCORRECT ANSWER: {item['correct_answer']}\")\n",
    "        output.append(\"\\nMODEL RESPONSE:\")\n",
    "        output.append(f\"{response_text}\")\n",
    "\n",
    "        think_content = self.reward_function.extract_think_content(response_text)\n",
    "        if think_content:\n",
    "            output.append(\"\\nEXTRACTED REASONING:\")\n",
    "            output.append(f\"{think_content}\")  # Full reasoning for file\n",
    "\n",
    "        extracted_facts = reward_info.get('extracted_facts', [])\n",
    "        if extracted_facts:\n",
    "            output.append(f\"\\nEXTRACTED FACTS ({len(extracted_facts)} total):\")\n",
    "            for i, fact in enumerate(extracted_facts, 1):  # Show all facts for file\n",
    "                fact_text = fact.get('text', '') if isinstance(fact, dict) else getattr(fact, 'text', '')\n",
    "                fact_category = fact.get('category', '') if isinstance(fact, dict) else getattr(fact, 'category', '')\n",
    "                llm_score = fact.get('llm_score', 0.0) if isinstance(fact, dict) else getattr(fact, 'llm_score', 0.0)\n",
    "                kb_score = fact.get('kb_score', 0.0) if isinstance(fact, dict) else getattr(fact, 'kb_score', 0.0)\n",
    "\n",
    "                output.append(f\"  {i}. [{fact_category}] {fact_text}\")\n",
    "                output.append(f\"     LLM Score: {llm_score:.2f}, KB Score: {kb_score:.2f}\")\n",
    "        else:\n",
    "            output.append(\"\\nNO FACTS EXTRACTED\")\n",
    "\n",
    "        # Show reward breakdown\n",
    "        output.append(\"\\nREWARD BREAKDOWN:\")\n",
    "        output.append(f\"  Binary (Correctness): {reward_info['r_binary']:.2f}\")\n",
    "        output.append(f\"  Answer Penalty: {reward_info['p_answer']:.2f}\")\n",
    "        output.append(f\"  Structural Penalty: {reward_info['p_structural']:.2f}\")\n",
    "        output.append(f\"  Factual Reward: {reward_info['r_factual']:.2f}\")\n",
    "        output.append(f\"  Total Normalized: {reward_info['r_normalized']:.2f}\")\n",
    "\n",
    "        if reward_info.get('factual_error'):\n",
    "            output.append(f\"  Factual Error: {reward_info['factual_error']}\")\n",
    "\n",
    "        output.append(f\"{'=' * 80}\")\n",
    "\n",
    "        # Join all output\n",
    "        full_output = '\\n'.join(output)\n",
    "\n",
    "        # Print to console (truncated for Jupyter)\n",
    "        console_output = []\n",
    "        console_output.append(f\"\\n{'=' * 80}\")\n",
    "        console_output.append(f\"STEP {step} ANALYSIS\")\n",
    "        console_output.append(f\"{'=' * 80}\")\n",
    "        console_output.append(\"QUESTION:\")\n",
    "        console_output.append(f\"{item['question'][:200]}...\")\n",
    "        console_output.append(f\"\\nCORRECT ANSWER: {item['correct_answer']}\")\n",
    "        console_output.append(\"\\nMODEL RESPONSE:\")\n",
    "        console_output.append(f\"{response_text[:500]}...\")\n",
    "\n",
    "        if think_content:\n",
    "            console_output.append(\"\\nEXTRACTED REASONING:\")\n",
    "            console_output.append(f\"{think_content[:300]}...\")\n",
    "\n",
    "        if extracted_facts:\n",
    "            console_output.append(f\"\\nEXTRACTED FACTS ({len(extracted_facts)} total):\")\n",
    "            for i, fact in enumerate(extracted_facts[:3], 1):  # Show first 3\n",
    "                fact_text = fact.get('text', '') if isinstance(fact, dict) else getattr(fact, 'text', '')\n",
    "                fact_category = fact.get('category', '') if isinstance(fact, dict) else getattr(fact, 'category', '')\n",
    "                llm_score = fact.get('llm_score', 0.0) if isinstance(fact, dict) else getattr(fact, 'llm_score', 0.0)\n",
    "                kb_score = fact.get('kb_score', 0.0) if isinstance(fact, dict) else getattr(fact, 'kb_score', 0.0)\n",
    "\n",
    "                console_output.append(f\"  {i}. [{fact_category}] {fact_text[:100]}...\")\n",
    "                console_output.append(f\"     LLM Score: {llm_score:.2f}, KB Score: {kb_score:.2f}\")\n",
    "\n",
    "            if len(extracted_facts) > 3:\n",
    "                console_output.append(f\"     ... and {len(extracted_facts) - 3} more facts\")\n",
    "        else:\n",
    "            console_output.append(\"\\nNO FACTS EXTRACTED\")\n",
    "\n",
    "        console_output.append(\"\\nREWARD BREAKDOWN:\")\n",
    "        console_output.append(f\"  Binary (Correctness): {reward_info['r_binary']:.2f}\")\n",
    "        console_output.append(f\"  Answer Penalty: {reward_info['p_answer']:.2f}\")\n",
    "        console_output.append(f\"  Structural Penalty: {reward_info['p_structural']:.2f}\")\n",
    "        console_output.append(f\"  Factual Reward: {reward_info['r_factual']:.2f}\")\n",
    "        console_output.append(f\"  Total Normalized: {reward_info['r_normalized']:.2f}\")\n",
    "\n",
    "        if reward_info.get('factual_error'):\n",
    "            console_output.append(f\"  Factual Error: {reward_info['factual_error']}\")\n",
    "\n",
    "        console_output.append(f\"{'=' * 80}\")\n",
    "\n",
    "        print('\\n'.join(console_output))\n",
    "\n",
    "        if log_file:\n",
    "            with open(log_file, 'a', encoding='utf-8') as f:\n",
    "                f.write(full_output + '\\n')\n",
    "\n",
    "    def train_reward_model_stage1(self, train_data_path, num_epochs, batch_size):\n",
    "        \"\"\"Stage 1: Train only the learnable reward components\"\"\"\n",
    "        if not hasattr(self, 'reward_head'):\n",
    "            print(\"No learnable reward head found - skipping reward model training\")\n",
    "            return\n",
    "\n",
    "        train_dataset = self.data_processor.load_medqa_data(train_data_path)\n",
    "        print(f\"Stage 1: Training reward model on {len(train_dataset)} examples\")\n",
    "\n",
    "        # Freeze policy model\n",
    "        self.model.eval()\n",
    "        self.reward_head.train()\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            num_batches = 0\n",
    "\n",
    "            for batch_start in range(0, len(train_dataset), batch_size):\n",
    "                batch_end = min(batch_start + batch_size, len(train_dataset))\n",
    "                batch_items = train_dataset[batch_start:batch_end]\n",
    "\n",
    "                batch_losses = []\n",
    "\n",
    "                for item in batch_items:\n",
    "                    try:\n",
    "                        prompt = item['prompt']\n",
    "                        correct_answer = item['correct_answer']\n",
    "\n",
    "                        # Generate response (no gradients for policy)\n",
    "                        with torch.no_grad():\n",
    "                            response_text, _, hidden_state = self.generate_response_with_logprobs(prompt)\n",
    "\n",
    "                        # Compute target score using rule-based reward\n",
    "                        rule_reward_info = self.reward_function.compute_total_reward(\n",
    "                            response_text, correct_answer\n",
    "                        )\n",
    "                        target_score = rule_reward_info['r_normalized']\n",
    "\n",
    "                        # Predict score using learnable reward head\n",
    "                        predicted_score = self.reward_head(\n",
    "                            hidden_state.detach().to(self.device, dtype=torch.float32)).view(())\n",
    "                        target_tensor = torch.tensor(target_score, device=self.device, dtype=torch.float32).view(())\n",
    "                        loss = torch.nn.functional.mse_loss(predicted_score, target_tensor)\n",
    "                        batch_losses.append(loss)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error in reward model training: {e}\")\n",
    "                        continue\n",
    "\n",
    "                if batch_losses:\n",
    "                    total_batch_loss = torch.stack(batch_losses).mean()\n",
    "                    total_batch_loss.backward()\n",
    "                    total_loss += total_batch_loss.item()\n",
    "                    num_batches += 1\n",
    "\n",
    "                # Update reward model every batch\n",
    "                torch.nn.utils.clip_grad_norm_(self.reward_head.parameters(), 1.0)\n",
    "                self.reward_optimizer.step()\n",
    "                self.reward_optimizer.zero_grad()\n",
    "\n",
    "            avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "            print(f\"Reward model epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.2f}\")\n",
    "\n",
    "        print(\"Stage 1 complete: Reward model training finished\")\n",
    "\n",
    "    def train_policy_stage2(self, train_data_path, num_epochs, batch_size):\n",
    "        train_dataset = self.data_processor.load_medqa_data(train_data_path)\n",
    "        print(f\"Stage 2: Training policy on {len(train_dataset)} examples\")\n",
    "\n",
    "        if hasattr(self, 'reward_head'):\n",
    "            self.reward_head.eval()\n",
    "        self.model.train()\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_rewards = []\n",
    "            epoch_advantages = []\n",
    "            epoch_policy_losses = []\n",
    "            epoch_baseline_values = []\n",
    "\n",
    "            violation_records = {\n",
    "                'correctness_violations': [],\n",
    "                'answer_leaking_violations': [],\n",
    "                'format_violations': [],\n",
    "                'factual_violations': [],\n",
    "                'bad_ood_high_rewards': []\n",
    "            }\n",
    "\n",
    "            epoch_metrics = {\n",
    "                'correctness_violations': 0,\n",
    "                'answer_leaking_violations': 0,\n",
    "                'format_violations': 0,\n",
    "                'factual_violations': 0,\n",
    "                'bad_ood_high_rewards': 0,\n",
    "                'total_examples': 0\n",
    "            }\n",
    "\n",
    "            self.policy_optimizer.zero_grad()\n",
    "            if self.use_baseline:\n",
    "                self.baseline_optimizer.zero_grad()\n",
    "\n",
    "            for batch_start in range(0, len(train_dataset), batch_size):\n",
    "                batch_end = min(batch_start + batch_size, len(train_dataset))\n",
    "                batch_items = train_dataset[batch_start:batch_end]\n",
    "\n",
    "                batch_policy_losses = []\n",
    "                batch_baseline_losses = []\n",
    "                batch_rewards = []\n",
    "                batch_advantages = []\n",
    "\n",
    "                for item in batch_items:\n",
    "                    try:\n",
    "                        prompt = item['prompt']\n",
    "                        correct_answer = item['correct_answer']\n",
    "\n",
    "                        response_text, log_probs, hidden_state, reward_info = self.generate_and_evaluate_with_facts(\n",
    "                            prompt, correct_answer\n",
    "                        )\n",
    "\n",
    "                        # Track violations for this individual example\n",
    "                        violations = self._analyze_response_quality(response_text, reward_info, correct_answer)\n",
    "\n",
    "                        # Record individual violations (0 or 1) for std calculation\n",
    "                        for key in violation_records.keys():\n",
    "                            violation_records[key].append(violations[key])\n",
    "\n",
    "                        # Aggregate totals for rates\n",
    "                        for key, value in violations.items():\n",
    "                            epoch_metrics[key] = epoch_metrics.get(key, 0) + value\n",
    "                        epoch_metrics[\"total_examples\"] = epoch_metrics.get(\"total_examples\", 0) + 1\n",
    "\n",
    "                        if log_probs.numel() == 0:\n",
    "                            continue\n",
    "\n",
    "                        # Get reward from frozen reward model\n",
    "                        with torch.no_grad():\n",
    "                            if hasattr(self, 'reward_head'):\n",
    "                                reward = self.reward_head(hidden_state.detach().float()).item()\n",
    "                            else:\n",
    "                                reward_info = self.reward_function.compute_total_reward(\n",
    "                                    response_text, correct_answer\n",
    "                                )\n",
    "                                reward = reward_info['r_normalized']\n",
    "\n",
    "                        # Baseline and advantage\n",
    "                        baseline_value = self.compute_baseline_value(hidden_state, training_mode=True)\n",
    "                        reward_tensor = torch.tensor(reward, device=self.device, dtype=torch.float16)\n",
    "                        advantage = reward_tensor - baseline_value\n",
    "\n",
    "                        batch_rewards.append(reward)\n",
    "                        batch_advantages.append(advantage.detach().item())\n",
    "\n",
    "                        policy_loss = -torch.sum(log_probs) * advantage\n",
    "                        batch_policy_losses.append(policy_loss)\n",
    "\n",
    "                        # Collect metrics for std calculation\n",
    "                        epoch_rewards.append(reward)\n",
    "                        epoch_advantages.append(advantage.detach().item())\n",
    "                        epoch_policy_losses.append(policy_loss.detach().item())\n",
    "                        if hasattr(baseline_value, 'item'):\n",
    "                            epoch_baseline_values.append(baseline_value.item())\n",
    "                        else:\n",
    "                            epoch_baseline_values.append(float(baseline_value))\n",
    "\n",
    "                        if self.use_baseline:\n",
    "                            baseline_loss = ((baseline_value - reward_tensor) ** 2)\n",
    "                            batch_baseline_losses.append(baseline_loss)\n",
    "\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error in policy training: {e}\")\n",
    "                        continue\n",
    "\n",
    "                # Update policy\n",
    "                if batch_policy_losses:\n",
    "                    total_policy_loss = torch.stack(batch_policy_losses).mean()\n",
    "                    total_loss = total_policy_loss\n",
    "\n",
    "                    if self.use_baseline and batch_baseline_losses:\n",
    "                        total_baseline_loss = torch.stack(batch_baseline_losses).mean()\n",
    "                        total_loss += total_baseline_loss\n",
    "\n",
    "                    total_loss.backward()\n",
    "\n",
    "            # Update weights\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=0.5)\n",
    "            if self.use_baseline:\n",
    "                torch.nn.utils.clip_grad_norm_(self.baseline_network.parameters(), max_norm=0.5)\n",
    "\n",
    "            self.policy_optimizer.step()\n",
    "            self.policy_optimizer.zero_grad()\n",
    "            if self.use_baseline:\n",
    "                self.baseline_optimizer.step()\n",
    "                self.baseline_optimizer.zero_grad()\n",
    "\n",
    "            # Calculate means and standard deviations for continuous metrics\n",
    "            avg_reward = np.mean(epoch_rewards) if epoch_rewards else 0\n",
    "            std_reward = np.std(epoch_rewards) if len(epoch_rewards) > 1 else 0\n",
    "\n",
    "            avg_advantage = np.mean(epoch_advantages) if epoch_advantages else 0\n",
    "            std_advantage = np.std(epoch_advantages) if len(epoch_advantages) > 1 else 0\n",
    "\n",
    "            avg_policy_loss = np.mean(epoch_policy_losses) if epoch_policy_losses else 0\n",
    "            std_policy_loss = np.std(epoch_policy_losses) if len(epoch_policy_losses) > 1 else 0\n",
    "\n",
    "            avg_baseline_value = np.mean(epoch_baseline_values) if epoch_baseline_values else 0\n",
    "            std_baseline_value = np.std(epoch_baseline_values) if len(epoch_baseline_values) > 1 else 0\n",
    "\n",
    "            print(f\"Policy epoch {epoch + 1}/{num_epochs}\")\n",
    "            print(f\"Avg Reward: {avg_reward:.2f} (Â±{std_reward:.2f})\\n\")\n",
    "        \n",
    "            if self.use_baseline:\n",
    "                print(f\"  Avg Baseline Value: {avg_baseline_value:.2f} (Â±{std_baseline_value:.2f})\")\n",
    "\n",
    "            if epoch_rewards and epoch_metrics[\"total_examples\"] > 0:\n",
    "                # Calculate violation rates and their standard deviations\n",
    "                for key in [\"correctness_violations\", \"answer_leaking_violations\",\n",
    "                            \"format_violations\", \"factual_violations\", \"bad_ood_high_rewards\"]:\n",
    "                    rate_key = f\"{key}_rate\"\n",
    "                    std_key = f\"{key}_std\"\n",
    "\n",
    "                    # Rate is the mean of 0s and 1s\n",
    "                    epoch_metrics[rate_key] = epoch_metrics[key] / epoch_metrics[\"total_examples\"]\n",
    "\n",
    "                    # Standard deviation of binary values (0s and 1s)\n",
    "                    if len(violation_records[key]) > 1:\n",
    "                        epoch_metrics[std_key] = np.std(violation_records[key])\n",
    "                    else:\n",
    "                        epoch_metrics[std_key] = 0.0\n",
    "\n",
    "                # Store other metrics\n",
    "                epoch_metrics[\"avg_reward\"] = avg_reward\n",
    "                epoch_metrics[\"std_reward\"] = std_reward\n",
    "                epoch_metrics[\"avg_advantage\"] = avg_advantage\n",
    "                epoch_metrics[\"std_advantage\"] = std_advantage\n",
    "                epoch_metrics[\"avg_policy_loss\"] = avg_policy_loss\n",
    "                epoch_metrics[\"std_policy_loss\"] = std_policy_loss\n",
    "                epoch_metrics[\"avg_baseline_value\"] = avg_baseline_value\n",
    "                epoch_metrics[\"std_baseline_value\"] = std_baseline_value\n",
    "\n",
    "                self.metrics_tracker.print_metrics_with_std(epoch_metrics, f\"POLICY EPOCH {epoch + 1}\")\n",
    "\n",
    "        print(\"Stage 2 complete: Policy training finished\")\n",
    "\n",
    "    def train_adversarial_stage3(self, train_data_path, num_cycles=3):\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"STAGE 3: ADVERSARIAL TRAINING\")\n",
    "        print(\"=\" * 60)\n",
    "        # DEBUG: Print first prompt to see what format it's in\n",
    "        train_dataset = self.data_processor.load_medqa_data(train_data_path)\n",
    "        print(\"\\n=== DEBUG: First prompt from dataset ===\")\n",
    "        print(train_dataset[0]['prompt'][:2048])\n",
    "        print(\"=== END DEBUG ===\\n\")\n",
    "        prompts = [item['prompt'] for item in train_dataset[:100]]\n",
    "        answers = [item['correct_answer'] for item in train_dataset[:100]]\n",
    "\n",
    "        # Run adversarial training cycles\n",
    "        results = self.adversarial_trainer.run_adversarial_training_cycle(\n",
    "            prompts, answers, num_cycles\n",
    "        )\n",
    "\n",
    "        print(\"Adversarial training complete:\")\n",
    "        print(f\"  Final robustness score: {results['final_robustness']:.2f}\")\n",
    "        print(f\"  Total cycles completed: {results['total_cycles']}\")\n",
    "\n",
    "        print(\"\\nEvaluating model AFTER adversarial training...\")\n",
    "        post_adversarial_metrics = self.evaluate_model(train_data_path, 50)\n",
    "\n",
    "        print(\"Post-Adversarial Performance:\")\n",
    "        self.metrics_tracker.print_metrics_with_std(post_adversarial_metrics, \"POST-ADVERSARIAL METRICS\")\n",
    "\n",
    "        return results\n",
    "\n",
    "    def train_reward_policy(self, train_data_path, test_data_path, stage1_epochs, stage2_epochs,\n",
    "                            batch_size=2, max_eval_examples=50, save_checkpoint_path=None):\n",
    "        \"\"\"Train only policy stages and optionally save checkpoint\"\"\"\n",
    "        print(\"TRAINING: Reward and Policy Only\")\n",
    "        print(\"=\" * 40)\n",
    "\n",
    "        self.train_reward_model_stage1(train_data_path, stage1_epochs, batch_size)\n",
    "        self.train_policy_stage2(train_data_path, stage2_epochs, batch_size)\n",
    "\n",
    "        print(\"\\nEvaluating Policy Training Results:\")\n",
    "        stage1_metrics = self.evaluate_model(test_data_path, max_eval_examples)\n",
    "        self.metrics_tracker.print_metrics_with_std(stage1_metrics, \"POLICY STAGE RESULTS\")\n",
    "\n",
    "        # Save checkpoint if path provided\n",
    "        if save_checkpoint_path:\n",
    "            self.save_model_checkpoint(save_checkpoint_path, \"policy_complete\")\n",
    "            print(f\"Policy training checkpoint saved to: {save_checkpoint_path}\")\n",
    "\n",
    "        return stage1_metrics\n",
    "\n",
    "    def train_adversarial(self, train_data_path, test_data_path, num_cycles=3, max_eval_examples=20):\n",
    "        print(\"TRAINING: Adversarial Stage Only\")\n",
    "        print(\"=\" * 40)\n",
    "    \n",
    "        adversarial_results = self.train_adversarial_stage3(train_data_path, num_cycles)\n",
    "    \n",
    "        print(\"\\nEvaluating Final Results:\")\n",
    "        final_metrics = self.evaluate_model(test_data_path, 20)\n",
    "        self.metrics_tracker.print_metrics_with_std(final_metrics, \"FINAL ADVERSARIAL RESULTS\")\n",
    "    \n",
    "        return final_metrics, adversarial_results\n",
    "\n",
    "    def train_combined(self, train_data_path, \n",
    "                       test_data_path, \n",
    "                       stage1_epochs, \n",
    "                       stage2_epochs, \n",
    "                       batch_size=2, \n",
    "                       max_eval_examples=20, \n",
    "                       adversarial_cycles=3):\n",
    "        # Stage 1: Train reward policy\n",
    "        print(\"\\nSTAGE 1: REWARD POLICY TRAINING\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        reward_policy_metrics = self.train_reward_policy(\n",
    "            train_data_path=train_data_path,\n",
    "            test_data_path=test_data_path,\n",
    "            stage1_epochs=stage1_epochs,\n",
    "            stage2_epochs=stage2_epochs,\n",
    "            batch_size=batch_size,\n",
    "            max_eval_examples=max_eval_examples\n",
    "        )\n",
    "        \n",
    "        print(\"\\nReward Policy Training Complete!\")\n",
    "        self.metrics_tracker.print_metrics_with_std(reward_policy_metrics, \"REWARD POLICY RESULTS\")\n",
    "    \n",
    "        # Set your S3 bucket and path\n",
    "        bucket_name = 'b'\n",
    "        s3_prefix_reward = 'medqa-models/reinforce-llama-run2/'  # For reward-only model\n",
    "        \n",
    "        # Save reward-only model locally\n",
    "        local_model_dir = '/home/ec2-user/SageMaker/llama_RM_2_USMLE'\n",
    "        os.makedirs(local_model_dir, exist_ok=True)\n",
    "        \n",
    "        self.model.save_pretrained(local_model_dir)\n",
    "        self.tokenizer.save_pretrained(local_model_dir)\n",
    "        torch.save(self.baseline_network.state_dict(), os.path.join(local_model_dir, \"baseline_network.pt\"))\n",
    "        \n",
    "        # Upload reward-only model to S3\n",
    "        s3 = boto3.client('s3')\n",
    "        for root, dirs, files in os.walk(local_model_dir):\n",
    "            for file in files:\n",
    "                local_path = os.path.join(root, file)\n",
    "                relative_path = os.path.relpath(local_path, local_model_dir)\n",
    "                s3_path = os.path.join(s3_prefix_reward, relative_path)\n",
    "                print(f\"Uploading {relative_path} to s3://{bucket_name}/{s3_path}\")\n",
    "                s3.upload_file(local_path, bucket_name, s3_path)\n",
    "        \n",
    "        print(\"\\nUpload to S3 complete!\")\n",
    "        print(f\"Reward-only model stored at s3://{bucket_name}/{s3_prefix_reward}\")\n",
    "        \n",
    "        # Stage 2: Adversarial training on the reward-trained model\n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        print(\"STAGE 2: ADVERSARIAL TRAINING\")\n",
    "        print(\"-\" * 50)\n",
    "        try:\n",
    "            adversarial_metrics = self.train_adversarial(\n",
    "                train_data_path=train_data_path,\n",
    "                test_data_path=test_data_path,\n",
    "                num_cycles=adversarial_cycles,\n",
    "                max_eval_examples=max_eval_examples\n",
    "            )\n",
    "            print(\"\\nAdversarial Training Complete!\")\n",
    "            self.metrics_tracker.print_metrics_with_std(adversarial_metrics, \"FINAL ADVERSARIAL RESULTS\")\n",
    "            \n",
    "            # Save adversarial model locally\n",
    "            s3_prefix_adv = 'medqa-models/reinforce-llama-adversarial/'\n",
    "            local_model_dir_adv = '/home/ec2-user/SageMaker/llama_RM_ADV_USMLE'\n",
    "            os.makedirs(local_model_dir_adv, exist_ok=True)\n",
    "            \n",
    "            # FIXED: Only save once, using self\n",
    "            self.model.save_pretrained(local_model_dir_adv)\n",
    "            self.tokenizer.save_pretrained(local_model_dir_adv)\n",
    "            torch.save(self.baseline_network.state_dict(), os.path.join(local_model_dir_adv, \"baseline_network.pt\"))\n",
    "            \n",
    "            # Upload adversarial model to S3\n",
    "            for root, dirs, files in os.walk(local_model_dir_adv):\n",
    "                for file in files:\n",
    "                    local_path = os.path.join(root, file)\n",
    "                    relative_path = os.path.relpath(local_path, local_model_dir_adv)\n",
    "                    s3_path = os.path.join(s3_prefix_adv, relative_path)\n",
    "                    print(f\"Uploading {relative_path} to s3://{bucket_name}/{s3_path}\")\n",
    "                    s3.upload_file(local_path, bucket_name, s3_path)\n",
    "            \n",
    "            print(\"\\nAdversarial model upload to S3 complete!\")\n",
    "            print(f\"Adversarial model stored at s3://{bucket_name}/{s3_prefix_adv}\")\n",
    "            \n",
    "            return {\n",
    "                'reward_policy_metrics': reward_policy_metrics,\n",
    "                'adversarial_metrics': adversarial_metrics,\n",
    "                'training_successful': True\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"\\nERROR: Adversarial training failed: {e}\")\n",
    "            print(\"Returning reward policy model as final result\")\n",
    "            \n",
    "            return {\n",
    "                'reward_policy_metrics': reward_policy_metrics,\n",
    "                'adversarial_metrics': None,\n",
    "                'training_successful': False,\n",
    "                'error': str(e)\n",
    "            }\n",
    "\n",
    "\n",
    "    def save_model_checkpoint(self, checkpoint_path, stage_name=\"policy\"):\n",
    "        os.makedirs(checkpoint_path, exist_ok=True)\n",
    "\n",
    "        # Save the main model\n",
    "        self.model.save_pretrained(\n",
    "            os.path.join(checkpoint_path, \"model\"),\n",
    "            safe_serialization=True\n",
    "        )\n",
    "\n",
    "        # Save tokenizer\n",
    "        self.tokenizer.save_pretrained(\n",
    "            os.path.join(checkpoint_path, \"tokenizer\")\n",
    "        )\n",
    "\n",
    "        # Save training components\n",
    "        checkpoint_data = {\n",
    "            'stage': stage_name,\n",
    "            'reward_history': self.reward_history,\n",
    "            'use_learnable_reward': self.use_learnable_reward,\n",
    "            'use_baseline': self.use_baseline,\n",
    "        }\n",
    "\n",
    "        # Save reward head if it exists\n",
    "        if hasattr(self, 'reward_head') and self.reward_head is not None:\n",
    "            torch.save(\n",
    "                self.reward_head.state_dict(),\n",
    "                os.path.join(checkpoint_path, \"reward_head.pt\")\n",
    "            )\n",
    "            checkpoint_data['has_reward_head'] = True\n",
    "        else:\n",
    "            checkpoint_data['has_reward_head'] = False\n",
    "\n",
    "        # Save baseline network if it exists\n",
    "        if hasattr(self, 'baseline_network') and self.baseline_network is not None:\n",
    "            torch.save(\n",
    "                self.baseline_network.state_dict(),\n",
    "                os.path.join(checkpoint_path, \"baseline_network.pt\")\n",
    "            )\n",
    "            checkpoint_data['has_baseline_network'] = True\n",
    "        else:\n",
    "            checkpoint_data['has_baseline_network'] = False\n",
    "\n",
    "        # Save optimizer states\n",
    "        torch.save(\n",
    "            self.policy_optimizer.state_dict(),\n",
    "            os.path.join(checkpoint_path, \"policy_optimizer.pt\")\n",
    "        )\n",
    "\n",
    "        if hasattr(self, 'reward_optimizer'):\n",
    "            torch.save(\n",
    "                self.reward_optimizer.state_dict(),\n",
    "                os.path.join(checkpoint_path, \"reward_optimizer.pt\")\n",
    "            )\n",
    "\n",
    "        if hasattr(self, 'baseline_optimizer'):\n",
    "            torch.save(\n",
    "                self.baseline_optimizer.state_dict(),\n",
    "                os.path.join(checkpoint_path, \"baseline_optimizer.pt\")\n",
    "            )\n",
    "\n",
    "        # Save metadata\n",
    "        with open(os.path.join(checkpoint_path, \"checkpoint_info.json\"), \"w\") as f:\n",
    "            json.dump(checkpoint_data, f, indent=2)\n",
    "\n",
    "        print(f\"Model checkpoint saved to {checkpoint_path}\")\n",
    "\n",
    "    @classmethod\n",
    "    def load_from_checkpoint(cls, checkpoint_path, reward_config=None, umls_api_key=None):\n",
    "        \"\"\"Load model from checkpoint\"\"\"\n",
    "\n",
    "        # Load metadata\n",
    "        with open(os.path.join(checkpoint_path, \"checkpoint_info.json\"), \"r\") as f:\n",
    "            checkpoint_data = json.load(f)\n",
    "\n",
    "        print(f\"Loading checkpoint from stage: {checkpoint_data['stage']}\")\n",
    "\n",
    "        # Create new trainer instance\n",
    "        trainer = cls(\n",
    "            model_path=os.path.join(checkpoint_path, \"model\"),\n",
    "            reward_config=reward_config,\n",
    "            use_baseline=checkpoint_data['use_baseline'],\n",
    "            umls_api_key=umls_api_key\n",
    "        )\n",
    "\n",
    "        # Restore training state\n",
    "        trainer.reward_history = checkpoint_data.get('reward_history', [])\n",
    "        trainer.use_learnable_reward = checkpoint_data.get('use_learnable_reward', False)\n",
    "\n",
    "        # Load reward head if it exists\n",
    "        if checkpoint_data.get('has_reward_head', False):\n",
    "            reward_head_path = os.path.join(checkpoint_path, \"reward_head.pt\")\n",
    "            if os.path.exists(reward_head_path):\n",
    "                trainer.reward_head.load_state_dict(torch.load(reward_head_path))\n",
    "                print(\"Reward head loaded from checkpoint\")\n",
    "\n",
    "        # Load baseline network if it exists\n",
    "        if checkpoint_data.get('has_baseline_network', False):\n",
    "            baseline_path = os.path.join(checkpoint_path, \"baseline_network.pt\")\n",
    "            if os.path.exists(baseline_path):\n",
    "                trainer.baseline_network.load_state_dict(torch.load(baseline_path))\n",
    "                print(\"Baseline network loaded from checkpoint\")\n",
    "\n",
    "        # Load optimizer states\n",
    "        policy_opt_path = os.path.join(checkpoint_path, \"policy_optimizer.pt\")\n",
    "        if os.path.exists(policy_opt_path):\n",
    "            trainer.policy_optimizer.load_state_dict(torch.load(policy_opt_path))\n",
    "            print(\"Policy optimizer loaded from checkpoint\")\n",
    "\n",
    "        reward_opt_path = os.path.join(checkpoint_path, \"reward_optimizer.pt\")\n",
    "        if os.path.exists(reward_opt_path) and hasattr(trainer, 'reward_optimizer'):\n",
    "            trainer.reward_optimizer.load_state_dict(torch.load(reward_opt_path))\n",
    "            print(\"Reward optimizer loaded from checkpoint\")\n",
    "\n",
    "        baseline_opt_path = os.path.join(checkpoint_path, \"baseline_optimizer.pt\")\n",
    "        if os.path.exists(baseline_opt_path) and hasattr(trainer, 'baseline_optimizer'):\n",
    "            trainer.baseline_optimizer.load_state_dict(torch.load(baseline_opt_path))\n",
    "            print(\"Baseline optimizer loaded from checkpoint\")\n",
    "\n",
    "        print(f\"Checkpoint loaded successfully from {checkpoint_path}\")\n",
    "        return trainer\n",
    "\n",
    "    def load_reward_model_and_train_adversarial(self, saved_model_path,\n",
    "                                                train_data_path,\n",
    "                                                test_data_path,\n",
    "                                                adversarial_cycles=3,\n",
    "                                                max_eval_examples=20):\n",
    "        print(f\"Loading model from {saved_model_path}\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(saved_model_path)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(saved_model_path)\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        baseline_path = os.path.join(saved_model_path, \"baseline_network.pt\")\n",
    "        if os.path.exists(baseline_path):\n",
    "            print(\"Loading saved baseline network...\")\n",
    "            baseline_state = torch.load(baseline_path, map_location=self.device)\n",
    "            self.baseline_network.load_state_dict(baseline_state)\n",
    "            print(\"Baseline network loaded successfully\")\n",
    "        else:\n",
    "            print(\"Warning: No baseline network found, initializing fresh\")\n",
    "        \n",
    "        # Run adversarial training on the loaded model\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"STARTING ADVERSARIAL TRAINING ON LOADED MODEL\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        adversarial_metrics = self.train_adversarial(\n",
    "            train_data_path=train_data_path,\n",
    "            test_data_path=test_data_path,\n",
    "            num_cycles=adversarial_cycles,\n",
    "            max_eval_examples=max_eval_examples\n",
    "        )\n",
    "        \n",
    "        print(\"\\nAdversarial Training Complete!\")\n",
    "        self.metrics_tracker.print_metrics_with_std(adversarial_metrics, \"ADVERSARIAL RESULTS\")\n",
    "        \n",
    "        return adversarial_metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fa3399-d74b-4944-a4f6-4515127969d1",
   "metadata": {},
   "source": [
    "# Adversarial Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd990912-c7f4-4ef7-b5f2-5f17e91cdd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdversarialTrainer:\n",
    "    \n",
    "    def __init__(self, base_trainer, adversarial_config=None):\n",
    "\n",
    "        self.base_trainer = base_trainer\n",
    "        self.model = base_trainer.model\n",
    "        self.tokenizer = base_trainer.tokenizer\n",
    "        self.reward_function = base_trainer.reward_function\n",
    "        self.device = base_trainer.device\n",
    "\n",
    "        config = adversarial_config or {}\n",
    "        self.adversarial_temperature = config.get('temperature', 1.2)\n",
    "        self.max_adversarial_examples = config.get('max_examples', 50)\n",
    "        self.preference_margin = config.get('preference_margin', 0.5)\n",
    "        self.validation_threshold = config.get('validation_threshold', 0.7)\n",
    "\n",
    "        self.adversarial_examples_buffer = []\n",
    "        self.preference_pairs_buffer = []\n",
    "\n",
    "        print(\"Adversarial Trainer initialized\")\n",
    "    \n",
    "    def generate_adversarial_examples(self, prompts, correct_answers=None, target_reward=0.5):\n",
    "        adversarial_examples = []\n",
    "        print(f\"Generating adversarial examples from {len(prompts)} prompts...\")\n",
    "    \n",
    "        generation_strategies = [\n",
    "            {'temperature': 0.7, 'top_p': 0.9, 'do_sample': True},   # Balanced\n",
    "            {'temperature': 0.9, 'top_p': 0.95, 'do_sample': True},  # Slightly diverse\n",
    "            {'temperature': 0.5, 'top_p': 0.85, 'do_sample': True},  # Conservative\n",
    "        ]\n",
    "    \n",
    "        shown = 0\n",
    "    \n",
    "        for strategy_idx, strategy in enumerate(generation_strategies):\n",
    "            print(f\"  Strategy {strategy_idx + 1}: temp={strategy['temperature']}\")\n",
    "    \n",
    "            for i, prompt in enumerate(prompts[:self.max_adversarial_examples]):\n",
    "                try:\n",
    "                    response = self._generate_with_strategy(prompt, strategy)\n",
    "                    correct_answer = correct_answers[i] if correct_answers else \"A\"\n",
    "                    reward_info = self.reward_function.compute_total_reward(response, correct_answer)\n",
    "    \n",
    "                    if self._is_adversarial_example(response, reward_info, target_reward):\n",
    "                        adv = {\n",
    "                            'prompt': prompt,\n",
    "                            'response': response,\n",
    "                            'reward_info': reward_info,\n",
    "                            'correct_answer': correct_answer,\n",
    "                            'strategy': strategy_idx,\n",
    "                            'vulnerability_type': self._classify_vulnerability(reward_info)\n",
    "                        }\n",
    "                        adversarial_examples.append(adv)\n",
    "    \n",
    "                        # Show first 1â€“2 adversarial examples for inspection\n",
    "                        if shown < 2:\n",
    "                            print(\"\\n=== Adversarial Example ===\")\n",
    "                            print(f\"Prompt:\\n{prompt}\\n\")\n",
    "                            print(f\"Response:\\n{response}\\n\")\n",
    "                            print(f\"Reward info: {reward_info}\")\n",
    "                            print(f\"Vulnerability: {adv['vulnerability_type']}\")\n",
    "                            print(\"===========================\\n\")\n",
    "                            shown += 1\n",
    "    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error generating adversarial example: {e}\")\n",
    "                    continue\n",
    "    \n",
    "        print(f\"Found {len(adversarial_examples)} adversarial examples\")\n",
    "        self.adversarial_examples_buffer.extend(adversarial_examples)\n",
    "        return adversarial_examples\n",
    "\n",
    "    \n",
    "    def _format_prompt_for_generation(self, prompt):\n",
    "\n",
    "        return f\"\"\"You are a medical expert taking the USMLE exam. Given the clinical scenario below, respond with your reasoning in a <think></think> tag and your final answer choice (A, B, C, or D) in an <answer></answer> tag.\n",
    "        Scenario:\n",
    "        {prompt}\n",
    "        \n",
    "        Format:\n",
    "        <think>your step-by-step clinical reasoning goes here</think>\n",
    "        <answer>A</answer>\n",
    "        \n",
    "        Your response:\"\"\"\n",
    "        \n",
    "\n",
    "    def _generate_with_strategy(self, prompt, strategy):\n",
    "        device = self.model.device\n",
    "    \n",
    "        # Tokenize with same settings as eval\n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=512\n",
    "        ).to(device)\n",
    "    \n",
    "        # Save and switch model state\n",
    "        was_training = self.model.training\n",
    "        self.model.eval()\n",
    "    \n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                # Force full precision for stable sampling\n",
    "                with torch.cuda.amp.autocast(enabled=False):\n",
    "                    outputs = self.model.generate(\n",
    "                        **inputs,\n",
    "                        max_new_tokens=600,\n",
    "                        temperature=strategy.get(\"temperature\", 0.7),\n",
    "                        top_p=strategy.get(\"top_p\", 0.9),\n",
    "                        do_sample=strategy.get(\"do_sample\", True),\n",
    "                        pad_token_id=self.tokenizer.pad_token_id or self.tokenizer.eos_token_id,\n",
    "                        eos_token_id=self.tokenizer.eos_token_id,\n",
    "                        repetition_penalty=1.2,     # prevent \"QuestionQuestion...\"\n",
    "                        no_repeat_ngram_size=3      # prevent n-gram loops\n",
    "                    )\n",
    "    \n",
    "            # Decode only the generated continuation\n",
    "            response = self.tokenizer.decode(\n",
    "                outputs[0][inputs[\"input_ids\"].shape[1]:],\n",
    "                skip_special_tokens=True\n",
    "            ).strip()\n",
    "    \n",
    "        finally:\n",
    "            # Restore model state\n",
    "            if was_training:\n",
    "                self.model.train()\n",
    "    \n",
    "        return response\n",
    "\n",
    "    \n",
    "    \n",
    "    def _generate_clean_response(self, prompt, correct_answer):\n",
    "        \"\"\"Generate a high-quality, non-adversarial response.\"\"\"\n",
    "        \n",
    "        #formatted = self._format_prompt_for_generation(prompt)\n",
    "        \n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=500,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        response = self.tokenizer.decode(\n",
    "            outputs[0][inputs['input_ids'].shape[1]:],\n",
    "            skip_special_tokens=True\n",
    "        ).strip()\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def _is_adversarial_example(self, response, reward_info, target_reward):\n",
    "        total_reward = reward_info.get('r_total', 0)\n",
    "        \n",
    "        if total_reward < target_reward:\n",
    "            return False\n",
    "        \n",
    "        has_issues = (\n",
    "            reward_info.get('p_answer', 0) > 0.05 or\n",
    "            reward_info.get('p_structural', 0) > 0.05 or \n",
    "            reward_info.get('r_factual', 1.0) < 0.4 or\n",
    "            not self.reward_function.validate_format(response)\n",
    "        )\n",
    "        return has_issues and total_reward > target_reward\n",
    "    \n",
    "    def _classify_vulnerability(self, reward_info):\n",
    "        \"\"\"Classify the type of vulnerability being exploited.\"\"\"\n",
    "        if reward_info.get('p_answer', 0) > 0.05:\n",
    "            return \"answer_leaking\"\n",
    "        elif reward_info.get('p_structural', 0) > 0.05:\n",
    "            return \"structural_gaming\"\n",
    "        elif reward_info.get('r_factual', 1.0) < 0.4:\n",
    "            return \"factual_exploitation\"\n",
    "        else:\n",
    "            return \"format_gaming\"\n",
    "    \n",
    "    def create_preference_pairs(self, adversarial_examples):\n",
    "        preference_pairs = []\n",
    "        \n",
    "        for adv_example in adversarial_examples:\n",
    "            clean_response = self._generate_clean_response(\n",
    "                adv_example['prompt'],\n",
    "                adv_example['correct_answer']\n",
    "            )\n",
    "            \n",
    "            clean_reward_info = self.reward_function.compute_total_reward(\n",
    "                clean_response, adv_example['correct_answer']\n",
    "            )\n",
    "            \n",
    "            clean_quality = self._assess_response_quality(clean_response, clean_reward_info)\n",
    "            adv_quality = self._assess_response_quality(adv_example['response'], adv_example['reward_info'])\n",
    "            \n",
    "            if clean_quality > adv_quality:\n",
    "                    preference_pairs.append({\n",
    "                        'prompt': adv_example['prompt'],\n",
    "                        'chosen': clean_response,\n",
    "                        'rejected': adv_example['response'],\n",
    "                        'correct_answer': adv_example['correct_answer'],\n",
    "                        'chosen_reward': clean_reward_info,\n",
    "                        'rejected_reward': adv_example['reward_info'],\n",
    "                        'vulnerability_type': adv_example['vulnerability_type']\n",
    "                    })\n",
    "        \n",
    "        print(f\"Created {len(preference_pairs)} preference pairs\")\n",
    "        self.preference_pairs_buffer.extend(preference_pairs)\n",
    "        \n",
    "        return preference_pairs\n",
    "    \n",
    "    def _generate_clean_response(self, prompt, correct_answer):\n",
    "        \"\"\"Generate a high-quality, non-adversarial response.\"\"\"\n",
    "        # Use conservative parameters for clean generation\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        response = self.tokenizer.decode(\n",
    "            outputs[0][inputs['input_ids'].shape[1]:],\n",
    "            skip_special_tokens=True\n",
    "        ).strip()\n",
    "        \n",
    "        # Ensure proper format\n",
    "        response = self._ensure_proper_format(response, correct_answer)\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def _ensure_proper_format(self, response, correct_answer):\n",
    "        \"\"\"Ensure response has proper <think> and <answer> tags.\"\"\"\n",
    "        # Add think tags if missing\n",
    "        if '<think>' not in response.lower():\n",
    "            reasoning_part = response.split('<answer>')[0].strip()\n",
    "            answer_part = response.split('<answer>')[1] if '<answer>' in response else ''\n",
    "            response = f\"<think>{reasoning_part}</think>\"\n",
    "            if answer_part:\n",
    "                response += f\"<answer>{answer_part}\"\n",
    "        \n",
    "        # Add answer tags if missing\n",
    "        if '<answer>' not in response.lower():\n",
    "            response += f\"\\n<answer>{correct_answer}</answer>\"\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def update_reward_model(self, preference_pairs):\n",
    "        if not hasattr(self.base_trainer, 'reward_head'):\n",
    "            print(\"No learnable reward component found - skipping adversarial update\")\n",
    "            return {'loss': 0.0, 'accuracy': 0.0}\n",
    "        \n",
    "        print(f\"Updating reward model with {len(preference_pairs)} preference pairs...\")\n",
    "        self.base_trainer.reward_optimizer.zero_grad()\n",
    "        self.base_trainer.reward_head.train()\n",
    "        total_loss = 0\n",
    "        correct_rankings = 0\n",
    "        \n",
    "        for pair in preference_pairs:\n",
    "            try:\n",
    "                # Get hidden states for both responses\n",
    "                chosen_hidden = self._get_hidden_state_for_response(\n",
    "                    pair['prompt'], pair['chosen']\n",
    "                )\n",
    "                rejected_hidden = self._get_hidden_state_for_response(\n",
    "                    pair['prompt'], pair['rejected']\n",
    "                )\n",
    "                \n",
    "                # Compute predicted rewards\n",
    "                chosen_reward_pred = self.base_trainer.reward_head(chosen_hidden.detach().float())\n",
    "                rejected_reward_pred = self.base_trainer.reward_head(rejected_hidden.detach().float())\n",
    "                \n",
    "                # Preference loss: chosen should have higher reward\n",
    "                target = torch.ones_like(chosen_reward_pred)\n",
    "                loss = F.margin_ranking_loss(\n",
    "                    chosen_reward_pred, rejected_reward_pred, \n",
    "                    target, margin=self.preference_margin\n",
    "                )\n",
    "                \n",
    "                loss.backward()\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Track accuracy\n",
    "                if chosen_reward_pred.item() > rejected_reward_pred.item():\n",
    "                    correct_rankings += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error in preference update: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Update parameters\n",
    "        if total_loss > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(self.base_trainer.reward_head.parameters(), 1.0)\n",
    "            self.base_trainer.reward_optimizer.step()\n",
    "        \n",
    "        avg_loss = total_loss / len(preference_pairs) if preference_pairs else 0\n",
    "        accuracy = correct_rankings / len(preference_pairs) if preference_pairs else 0\n",
    "        \n",
    "        print(f\"  Adversarial loss: {avg_loss:.4f}\")\n",
    "        print(f\"  Preference accuracy: {accuracy:.2f}\")\n",
    "        \n",
    "        return {'loss': avg_loss, 'accuracy': accuracy}\n",
    "\n",
    "    def analyze_response_quality(self, response, reward_info, correct_answer):\n",
    "        \"\"\"Analyze response against your 5 quality goals\"\"\"\n",
    "\n",
    "        violations = {\n",
    "            'correctness_violations': 0,\n",
    "            'answer_leaking_violations': 0,\n",
    "            'format_violations': 0,\n",
    "            'factual_violations': 0,\n",
    "            'bad_ood_high_rewards': 0\n",
    "        }\n",
    "\n",
    "        # Goal 1: Improved Correctness\n",
    "        if reward_info.get('r_binary', 0) <= 0:\n",
    "            violations['correctness_violations'] = 1\n",
    "\n",
    "        # Goal 2: Less Answer Leaking\n",
    "        if reward_info.get('p_answer', 0) > 0.1:\n",
    "            violations['answer_leaking_violations'] = 1\n",
    "\n",
    "        # Goal 3: Less Format Violations\n",
    "        if reward_info.get('p_structural', 0) > 0.1 or not self.reward_function.validate_format(response):\n",
    "            violations['format_violations'] = 1\n",
    "\n",
    "        # Goal 4: Less Factual Errors\n",
    "        if reward_info.get('r_factual', 1.0) < 0.3:\n",
    "            violations['factual_violations'] = 1\n",
    "\n",
    "        # Goal 5: Less Rewards to Bad OOD Reasoning\n",
    "        # High total reward despite violations = bad OOD getting undeserved high reward\n",
    "        has_violations = sum(violations.values()) > 0\n",
    "        high_reward = reward_info.get('r_total', 0) > 0.5\n",
    "        if has_violations and high_reward:\n",
    "            violations['bad_ood_high_rewards'] = 1\n",
    "\n",
    "        return violations\n",
    "\n",
    "    def _assess_response_quality(self, response, reward_info, correct_answer=None):\n",
    "        \"\"\"\n",
    "        Assess overall response quality using existing analysis method.\n",
    "        \n",
    "        Returns:\n",
    "            Quality score between 0 and 1 (higher = better quality)\n",
    "        \"\"\"\n",
    "        # Use existing violation analysis\n",
    "        violations = self.analyze_response_quality(response, reward_info, correct_answer or \"A\")\n",
    "        \n",
    "        # Convert violations to quality score\n",
    "        total_violations = sum(violations.values())\n",
    "        max_violations = len(violations)  # 5 possible violation types\n",
    "        \n",
    "        # Quality = 1 - (violation_ratio)\n",
    "        quality_score = 1.0 - (total_violations / max_violations)\n",
    "        \n",
    "        return quality_score\n",
    "    \n",
    "    def _get_hidden_state_for_response(self, prompt, response):\n",
    "        \"\"\"Get hidden state for a specific response.\"\"\"\n",
    "        full_text = prompt + \" \" + response\n",
    "        inputs = self.tokenizer(full_text, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs, output_hidden_states=True)\n",
    "            hidden_state = outputs.hidden_states[-1][0].mean(dim=0)  # Average over sequence\n",
    "        \n",
    "        return hidden_state\n",
    "    \n",
    "    def validate_robustness(self, test_prompts, test_answers):\n",
    "        \"\"\"\n",
    "        Validate that reward model has become more robust.\n",
    "        \n",
    "        Args:\n",
    "            test_prompts: List of test prompts\n",
    "            test_answers: List of correct answers\n",
    "            \n",
    "        Returns:\n",
    "            Robustness metrics\n",
    "        \"\"\"\n",
    "        print(\"Validating reward model robustness...\")\n",
    "        \n",
    "        # Generate new adversarial examples\n",
    "        new_adversarial = self.generate_adversarial_examples(\n",
    "            test_prompts[:20], test_answers[:20]\n",
    "        )\n",
    "        \n",
    "        # Check if reward model correctly identifies them as problematic\n",
    "        correct_identifications = 0\n",
    "        \n",
    "        for adv_example in new_adversarial:\n",
    "            reward_info = adv_example['reward_info']\n",
    "            \n",
    "            # If reward model is robust, adversarial examples should get lower rewards\n",
    "            if reward_info['r_total'] < 0.3:  # Threshold for \"correctly identified as bad\"\n",
    "                correct_identifications += 1\n",
    "        \n",
    "        robustness_score = correct_identifications / len(new_adversarial) if new_adversarial else 0\n",
    "        \n",
    "        print(f\"Robustness validation: {robustness_score:.2f}\")\n",
    "        print(f\"  Found {len(new_adversarial)} new adversarial examples\")\n",
    "        print(f\"  Correctly identified {correct_identifications} as problematic\")\n",
    "        \n",
    "        return {\n",
    "            'robustness_score': robustness_score,\n",
    "            'adversarial_found': len(new_adversarial),\n",
    "            'correctly_identified': correct_identifications\n",
    "        }\n",
    "    \n",
    "    def run_adversarial_training_cycle(self, prompts, correct_answers, num_cycles=3):\n",
    "        \"\"\"\n",
    "        Complete adversarial training cycle.\n",
    "        \n",
    "        Args:\n",
    "            prompts: Training prompts\n",
    "            correct_answers: Correct answers\n",
    "            num_cycles: Number of adversarial cycles to run\n",
    "            \n",
    "        Returns:\n",
    "            Training summary\n",
    "        \"\"\"\n",
    "        print(f\"Starting {num_cycles} adversarial training cycles...\")\n",
    "        \n",
    "        cycle_results = []\n",
    "        \n",
    "        for cycle in range(num_cycles):\n",
    "            print(f\"\\n=== Adversarial Cycle {cycle + 1}/{num_cycles} ===\")\n",
    "            \n",
    "            # Step 1: Generate adversarial examples\n",
    "            adversarial_examples = self.generate_adversarial_examples(\n",
    "                prompts, correct_answers\n",
    "            )\n",
    "            \n",
    "            if not adversarial_examples:\n",
    "                print(\"No adversarial examples found\")\n",
    "                break\n",
    "            \n",
    "            # Step 2: Create preference pairs\n",
    "            preference_pairs = self.create_preference_pairs(adversarial_examples)\n",
    "            \n",
    "            if not preference_pairs:\n",
    "                print(\"No valid preference pairs created\")\n",
    "                continue\n",
    "            \n",
    "            # Step 3: Update reward model\n",
    "            update_metrics = self.update_reward_model(preference_pairs)\n",
    "            \n",
    "            # Step 4: Validate improvement\n",
    "            validation_metrics = self.validate_robustness(\n",
    "                prompts[:10], correct_answers[:10]\n",
    "            )\n",
    "            \n",
    "            cycle_results.append({\n",
    "                'cycle': cycle + 1,\n",
    "                'adversarial_found': len(adversarial_examples),\n",
    "                'preference_pairs': len(preference_pairs),\n",
    "                'update_loss': update_metrics['loss'],\n",
    "                'update_accuracy': update_metrics['accuracy'],\n",
    "                'robustness_score': validation_metrics['robustness_score']\n",
    "            })\n",
    "            \n",
    "            print(f\"Cycle {cycle + 1} complete:\")\n",
    "            print(f\"  Adversarial examples: {len(adversarial_examples)}\")\n",
    "            print(f\"  Update accuracy: {update_metrics['accuracy']:.2f}\")\n",
    "            print(f\"  Robustness score: {validation_metrics['robustness_score']:.2f}\")\n",
    "        \n",
    "        return {\n",
    "            'total_cycles': len(cycle_results),\n",
    "            'cycle_results': cycle_results,\n",
    "            'final_robustness': cycle_results[-1]['robustness_score'] if cycle_results else 0\n",
    "        }\n",
    "    \n",
    "    def clear_buffers(self):\n",
    "        \"\"\"Clear adversarial example and preference pair buffers.\"\"\"\n",
    "        self.adversarial_examples_buffer.clear()\n",
    "        self.preference_pairs_buffer.clear()\n",
    "        print(\"Adversarial training buffers cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de9c71e-ea19-4213-9d8c-e89135bc8061",
   "metadata": {},
   "source": [
    "# Fact Extraction and LLM Judge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b972ec3a-99a6-4350-ab64-33c418d65dd4",
   "metadata": {},
   "source": [
    "## AtomicFactExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e36edf-a9cd-45ad-8dd7-b2bfdfb37458",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AtomicFact:\n",
    "    text: str\n",
    "    category: str\n",
    "    confidence: float = 0.0\n",
    "    llm_score: float = 0.0\n",
    "    kb_score: float = 0.0\n",
    "    source_sentence: str = \"\"\n",
    "\n",
    "class AtomicFactExtractor:\n",
    "    def __init__(self, model_name, region_name):\n",
    "        self.model_name = model_name\n",
    "        self.extraction_prompt = self._build_extraction_prompt()\n",
    "        self.client = ClaudeBedrockClient(model_name, region_name)\n",
    "        \n",
    "    def _build_extraction_prompt(self):\n",
    "        return \"\"\"You are a medical fact extractor. Extract specific, verifiable clinical claims from the reasoning text.\n",
    "    \n",
    "        RULES:\n",
    "        - Extract only factual medical statements (not reasoning steps)\n",
    "        - Each fact should be 1-2 sentences maximum\n",
    "        - Focus on: symptoms, diagnoses, treatments, drug effects, anatomical facts\n",
    "        - Ignore: \"the patient likely has...\" or \"this suggests...\" (too speculative)\n",
    "        \n",
    "        INPUT TEXT:\n",
    "        {reasoning_text}\n",
    "        \n",
    "        Return ONLY a valid JSON object in this exact format:\n",
    "        {{\"facts\": [\"fact1\", \"fact2\", \"fact3\"]}}\n",
    "        \n",
    "        EXAMPLES:\n",
    "        Good facts: \"Salicylate poisoning causes metabolic acidosis\", \"Insulin treats diabetic ketoacidosis\"\n",
    "        Bad facts: \"This presentation is consistent with...\", \"We should consider...\"\n",
    "        \n",
    "        JSON:\"\"\"\n",
    "    \n",
    "    def _parse_extraction_response(self, response):\n",
    "        facts = []\n",
    "        try:\n",
    "            print(f\"Raw response: '{response}'\")\n",
    "            cleaned_response = response.strip()\n",
    "    \n",
    "            cleaned_response = re.sub(r'```json\\s*', '', cleaned_response)\n",
    "            cleaned_response = re.sub(r'```\\s*$', '', cleaned_response)\n",
    "    \n",
    "            json_str = None\n",
    "            json_match = re.search(r'\\{[^{}]*\"facts\"[^{}]*:\\s*\\[[^\\]]*\\][^{}]*\\}', cleaned_response, re.DOTALL)\n",
    "            if json_match:\n",
    "                json_str = json_match.group(0)\n",
    "                print(f\"Found complete JSON object: '{json_str}'\")\n",
    "            else:\n",
    "                facts_only_match = re.search(r'\"facts\"\\s*:\\s*\\[[^\\]]*\\]', cleaned_response, re.DOTALL)\n",
    "                if facts_only_match:\n",
    "                    json_str = '{' + facts_only_match.group(0) + '}'\n",
    "                    print(f\"Fixed incomplete JSON: '{json_str}'\")\n",
    "                else:\n",
    "                    json_match = re.search(r'\\{.*?\\}', cleaned_response, re.DOTALL)\n",
    "                    if json_match:\n",
    "                        json_str = json_match.group(0)\n",
    "                        print(f\"Found fallback JSON: '{json_str}'\")\n",
    "                    else:\n",
    "                        array_match = re.search(r'\\[.*?\\]', cleaned_response, re.DOTALL)\n",
    "                        if array_match:\n",
    "                            json_str = '{\"facts\": ' + array_match.group(0) + '}'\n",
    "                            print(f\"Created JSON from array: '{json_str}'\")\n",
    "    \n",
    "            if not json_str:\n",
    "                print(\"No JSON pattern found, using fallback extraction\")\n",
    "                return self._fallback_extraction(cleaned_response)\n",
    "    \n",
    "            # Fix common JSON issues\n",
    "            json_str = self._fix_json_issues(json_str)\n",
    "            print(f\"After fixing JSON: '{json_str}'\")\n",
    "            \n",
    "            try:\n",
    "                data = json.loads(json_str)\n",
    "                print(f\"Successfully parsed JSON: {data}\")\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSON decode failed: {e}\")\n",
    "                print(\"Attempting advanced repair...\")\n",
    "                \n",
    "                repaired_json = self._repair_json_advanced(json_str)\n",
    "                if repaired_json != json_str:\n",
    "                    try:\n",
    "                        data = json.loads(repaired_json)\n",
    "                        print(\"Successfully repaired and parsed JSON\")\n",
    "                    except Exception as repair_e:\n",
    "                        print(f\"JSON repair also failed: {repair_e}\")\n",
    "                        return self._fallback_extraction(cleaned_response)\n",
    "                else:\n",
    "                    return self._fallback_extraction(cleaned_response)\n",
    "            \n",
    "            if isinstance(data, dict):\n",
    "                if \"facts\" in data:\n",
    "                    facts_list = data[\"facts\"]\n",
    "                    if isinstance(facts_list, list):\n",
    "                        print(f\"Found {len(facts_list)} facts in data\")\n",
    "                        \n",
    "                        for i, fact_item in enumerate(facts_list):\n",
    "                            if isinstance(fact_item, str):\n",
    "                                # Simple string fact\n",
    "                                text = fact_item.strip()\n",
    "                                if text and len(text) > 5:\n",
    "                                    fact = AtomicFact(\n",
    "                                        text=text,\n",
    "                                        category=\"medical_statement\",\n",
    "                                        source_sentence=text\n",
    "                                    )\n",
    "                                    facts.append(fact)\n",
    "                                    print(f\"Created string fact {i+1}: {text}\")\n",
    "                            elif isinstance(fact_item, dict):\n",
    "                                # Object fact with metadata\n",
    "                                text = fact_item.get(\"text\", \"\").strip()\n",
    "                                if text and len(text) > 5:\n",
    "                                    fact = AtomicFact(\n",
    "                                        text=text,\n",
    "                                        category=fact_item.get(\"category\", \"medical_statement\"),\n",
    "                                        source_sentence=fact_item.get(\"source_sentence\", text)\n",
    "                                    )\n",
    "                                    facts.append(fact)\n",
    "                                    print(f\"Created object fact {i+1}: {text}\")\n",
    "                    else:\n",
    "                        print(f\"'facts' is not a list: {type(facts_list)}\")\n",
    "                        return self._fallback_extraction(cleaned_response)\n",
    "                else:\n",
    "                    print(f\"No 'facts' key found. Available keys: {list(data.keys())}\")\n",
    "                    return self._fallback_extraction(cleaned_response)\n",
    "            else:\n",
    "                print(f\"Parsed data is not a dict: {type(data)}\")\n",
    "                return self._fallback_extraction(cleaned_response)\n",
    "            \n",
    "            if not facts:\n",
    "                print(\"No valid facts extracted, using fallback\")\n",
    "                return self._fallback_extraction(cleaned_response)\n",
    "            \n",
    "            print(f\"Successfully extracted {len(facts)} facts\")\n",
    "            return facts\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in _parse_extraction_response: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return self._fallback_extraction(response)\n",
    "\n",
    "    def _fix_json_issues(self, json_str):\n",
    "        \"\"\"Fix common JSON formatting issues\"\"\"\n",
    "        \n",
    "        # Remove extra quotes around the whole string\n",
    "        if json_str.startswith('\"') and json_str.endswith('\"'):\n",
    "            json_str = json_str[1:-1]\n",
    "        \n",
    "        # Fix escaped quotes\n",
    "        json_str = json_str.replace('\\\\\"', '\"')\n",
    "        \n",
    "        # Remove trailing commas\n",
    "        json_str = re.sub(r',\\s*}', '}', json_str)\n",
    "        json_str = re.sub(r',\\s*]', ']', json_str)\n",
    "        \n",
    "        # Fix missing closing brackets/braces\n",
    "        open_braces = json_str.count('{')\n",
    "        close_braces = json_str.count('}')\n",
    "        if open_braces > close_braces:\n",
    "            json_str += '}' * (open_braces - close_braces)\n",
    "        \n",
    "        open_brackets = json_str.count('[')\n",
    "        close_brackets = json_str.count(']')\n",
    "        if open_brackets > close_brackets:\n",
    "            json_str += ']' * (open_brackets - close_brackets)\n",
    "        \n",
    "        return json_str\n",
    "    \n",
    "    def _repair_json_advanced(self, json_str):\n",
    "        \"\"\"Advanced JSON repair for malformed responses\"\"\"\n",
    "        \n",
    "        # If we can't parse it, try to build a valid structure\n",
    "        try:\n",
    "            # Look for fact-like content in quotes\n",
    "            fact_candidates = re.findall(r'\"([^\"]{10,200})\"', json_str)\n",
    "            \n",
    "            # Filter for medical-sounding facts\n",
    "            medical_facts = []\n",
    "            medical_keywords = ['patient', 'symptom', 'diagnosis', 'treatment', 'medication', \n",
    "                              'disease', 'condition', 'causes', 'therapy', 'clinical']\n",
    "            \n",
    "            for candidate in fact_candidates:\n",
    "                if any(keyword in candidate.lower() for keyword in medical_keywords):\n",
    "                    # Avoid reasoning phrases\n",
    "                    reasoning_phrases = ['this suggests', 'likely', 'consistent with', \n",
    "                                       'we should', 'let us', 'first', 'therefore']\n",
    "                    if not any(phrase in candidate.lower() for phrase in reasoning_phrases):\n",
    "                        medical_facts.append(candidate)\n",
    "            \n",
    "            if medical_facts:\n",
    "                # Create valid JSON structure\n",
    "                valid_json = '{\"facts\": ' + json.dumps(medical_facts[:10]) + '}'\n",
    "                print(f\"Reconstructed JSON: {valid_json}\")\n",
    "                return valid_json\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Advanced repair failed: {e}\")\n",
    "        \n",
    "        return json_str\n",
    "    \n",
    "    def _fallback_extraction(self, text):\n",
    "        \"\"\"Enhanced fallback extraction when JSON parsing fails\"\"\"\n",
    "        facts = []\n",
    "        \n",
    "        print(\"Using fallback extraction method\")\n",
    "        \n",
    "        # Extract medical sentences using improved patterns\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        medical_keywords = [\n",
    "            'patient', 'diagnosis', 'treatment', 'symptom', 'medication',\n",
    "            'therapy', 'condition', 'disease', 'clinical', 'medical',\n",
    "            'causes', 'leads to', 'results in', 'associated with',\n",
    "            'effective', 'contraindicated', 'indicated', 'syndrome'\n",
    "        ]\n",
    "        \n",
    "        # Reasoning phrases to avoid\n",
    "        avoid_phrases = [\n",
    "            'this suggests', 'likely', 'probably', 'consistent with',\n",
    "            'we should', 'let us', 'first', 'next step', 'therefore',\n",
    "            'in conclusion', 'based on', 'given that'\n",
    "        ]\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if len(sentence) > 15 and len(sentence) < 200:  # Reasonable length\n",
    "                sentence_lower = sentence.lower()\n",
    "                \n",
    "                # Check if sentence contains medical content\n",
    "                has_medical = any(keyword in sentence_lower for keyword in medical_keywords)\n",
    "                has_reasoning = any(phrase in sentence_lower for phrase in avoid_phrases)\n",
    "                \n",
    "                if has_medical and not has_reasoning:\n",
    "                    fact = AtomicFact(\n",
    "                        text=sentence,\n",
    "                        category=\"medical_statement\",\n",
    "                        source_sentence=sentence\n",
    "                    )\n",
    "                    facts.append(fact)\n",
    "                    print(f\"Fallback extracted: {sentence}\")\n",
    "        \n",
    "        return facts[:8]  # Limit to 8 facts max\n",
    "\n",
    "    def _clean_reasoning_text(self, text):\n",
    "        \"\"\"Clean and extract reasoning text from response\"\"\"\n",
    "        #print(f\"Original text: '{text}'\")\n",
    "        \n",
    "        # Extract content from <think> tags if present\n",
    "        think_match = re.search(r'<think>(.*?)</think>', text, flags=re.DOTALL)\n",
    "        if think_match:\n",
    "            text = think_match.group(1)\n",
    "            #print(f\"Extracted from <think> tags: '{text}'\")\n",
    "        else:\n",
    "            print(\"No <think> tags found, using original text\")\n",
    "        \n",
    "        # Remove <answer> tags entirely\n",
    "        text = re.sub(r'<answer>.*?</answer>', '', text, flags=re.DOTALL)\n",
    "        \n",
    "        # Clean whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        #print(f\"Final cleaned text: '{text}'\")\n",
    "        return text.strip()\n",
    "    \n",
    "    def _call_extraction_llm(self, prompt):\n",
    "        \"\"\"Call the extraction LLM with the prompt\"\"\"\n",
    "        full_prompt = f\"\"\"You are a medical fact extraction specialist.\\n\\n{prompt}\"\"\"\n",
    "    \n",
    "        try:\n",
    "            response = self.client.chat_completion(\n",
    "                messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
    "                temperature=0.1,\n",
    "                max_tokens=1000\n",
    "            )\n",
    "    \n",
    "            content = response['choices'][0]['message']['content']\n",
    "            return content\n",
    "    \n",
    "        except Exception as e:\n",
    "            print(f\"API call failed: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def extract_facts(self, reasoning_text):\n",
    "        try:\n",
    "            cleaned_text = self._clean_reasoning_text(reasoning_text)\n",
    "            prompt = self.extraction_prompt.format(reasoning_text=cleaned_text)\n",
    "            response = self._call_extraction_llm(prompt)\n",
    "            facts = self._parse_extraction_response(response)\n",
    "            return facts\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in fact extraction: {e}\")\n",
    "            return []\n",
    "        \n",
    "    def diagnose_evaluation(self, responses):\n",
    "        \"\"\"Diagnostic method to debug fact extraction issues\"\"\"\n",
    "        print(\"=== DIAGNOSTIC REPORT ===\")\n",
    "        \n",
    "        for i, response in enumerate(responses[:5]):  # Check first 5\n",
    "            print(f\"\\n--- Response {i+1} ---\")\n",
    "            print(f\"Length: {len(response)}\")\n",
    "            print(f\"Has <think>: {'<think>' in response}\")\n",
    "            print(f\"Has <answer>: {'<answer>' in response}\")\n",
    "            \n",
    "            try:\n",
    "                facts = self.extract_facts(response)\n",
    "                print(f\"Extracted facts: {len(facts)} facts\")\n",
    "                if facts:\n",
    "                    print(f\"First fact: {facts[0].text if hasattr(facts[0], 'text') else facts[0]}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Fact extraction error: {e}\")\n",
    "                import traceback\n",
    "                traceback.print_exc()\n",
    "            \n",
    "            print(f\"Preview: {response[:200]}...\")\n",
    "            \n",
    "            if i < 2:\n",
    "                print(f\"Full response:\\n{response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c170d4-ad9d-4929-9b56-f65e692c17df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def quick_diagnostic_test():\n",
    "#     \"\"\"Quick test with sample responses\"\"\"\n",
    "    \n",
    "#     sample_responses = [\n",
    "#         \"\"\"<think>\n",
    "#         The patient presents with chest pain and elevated troponins. This suggests myocardial infarction.\n",
    "#         Aspirin and clopidogrel are indicated for antiplatelet therapy.\n",
    "#         </think>\n",
    "#         <answer>A</answer>\"\"\",\n",
    "        \n",
    "#         \"\"\"<think>\n",
    "#         Diabetic ketoacidosis is characterized by hyperglycemia, ketosis, and acidosis.\n",
    "#         Insulin therapy is the primary treatment.\n",
    "#         </think>\n",
    "#         <answer>B</answer>\"\"\",\n",
    "        \n",
    "#         \"\"\"No think tags here, just some text.\n",
    "#         <answer>C</answer>\"\"\",\n",
    "        \n",
    "#         \"\"\"<think>\n",
    "#         This is malformed JSON response test\n",
    "#         </think>\n",
    "#         <answer>D</answer>\"\"\",\n",
    "        \n",
    "#         \"\"\"Completely malformed response without proper tags\"\"\"\n",
    "#     ]\n",
    "    \n",
    "#     # Initialize your extractor\n",
    "#     fact_extractor = AtomicFactExtractor(model_name=\"anthropic.claude-3-haiku-20240307-v1:0\", region_name=\"us-east-1\")\n",
    "    \n",
    "#     # Run diagnostic\n",
    "#     fact_extractor.diagnose_evaluation(sample_responses)\n",
    "\n",
    "\n",
    "# quick_diagnostic_test()/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c167b6-77f6-4414-b136-4276390cc419",
   "metadata": {},
   "source": [
    "## LLM Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0145196a-6097-4d29-9ceb-69d8d0c1fe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "class ClaudeBedrockClient:\n",
    "    def __init__(self, model_name, region_name):\n",
    "        self.model_name = model_name\n",
    "        self.region_name = region_name\n",
    "\n",
    "        try:\n",
    "            self.bedrock_client = boto3.client(\n",
    "                service_name='bedrock-runtime',\n",
    "                region_name=region_name\n",
    "            )\n",
    "            print(f\"Initialized Claude Bedrock client with model: {model_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to initialize Bedrock client: {e}\")\n",
    "            raise\n",
    "\n",
    "    def chat_completion(self, messages, temperature=0.2, max_tokens=1000):\n",
    "        try:\n",
    "            claude_messages = self._convert_messages_to_claude_format(messages)\n",
    "\n",
    "            body = {\n",
    "                \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                \"max_tokens\": max_tokens,\n",
    "                \"temperature\": temperature,\n",
    "                \"messages\": claude_messages\n",
    "            }\n",
    "\n",
    "            # Make request to Bedrock\n",
    "            response = self.bedrock_client.invoke_model(\n",
    "                modelId=self.model_name,\n",
    "                body=json.dumps(body),\n",
    "                contentType='application/json'\n",
    "            )\n",
    "\n",
    "            # Parse response\n",
    "            response_body = json.loads(response['body'].read())\n",
    "\n",
    "            # Convert to OpenAI-like format\n",
    "            return self._convert_claude_response_to_openai_format(response_body)\n",
    "\n",
    "        except ClientError as e:\n",
    "            print(f\"Bedrock API error: {e}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error in chat completion: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _convert_messages_to_claude_format(self, messages):\n",
    "        \"\"\"Convert OpenAI message format to Claude format\"\"\"\n",
    "        claude_messages = []\n",
    "\n",
    "        for msg in messages:\n",
    "            role = msg['role']\n",
    "            content = msg['content']\n",
    "\n",
    "            if role == 'system':\n",
    "                if claude_messages and claude_messages[-1]['role'] == 'user':\n",
    "                    claude_messages[-1]['content'] = f\"{content}\\n\\n{claude_messages[-1]['content']}\"\n",
    "                else:\n",
    "                    claude_messages.append({\n",
    "                        'role': 'user',\n",
    "                        'content': content\n",
    "                    })\n",
    "            elif role in ['user', 'assistant']:\n",
    "                claude_messages.append({\n",
    "                    'role': role,\n",
    "                    'content': content\n",
    "                })\n",
    "\n",
    "        if not claude_messages or claude_messages[0]['role'] != 'user':\n",
    "            claude_messages.insert(0, {\n",
    "                'role': 'user',\n",
    "                'content': 'Please help me with the following task.'\n",
    "            })\n",
    "\n",
    "        return claude_messages\n",
    "\n",
    "    def _convert_claude_response_to_openai_format(self, claude_response: Dict):\n",
    "        \"\"\"Convert Claude response to OpenAI-like format\"\"\"\n",
    "        content = \"\"\n",
    "\n",
    "        if 'content' in claude_response and claude_response['content']:\n",
    "            content = claude_response['content'][0]['text']\n",
    "\n",
    "        return {\n",
    "            'choices': [{\n",
    "                'message': {\n",
    "                    'role': 'assistant',\n",
    "                    'content': content\n",
    "                },\n",
    "                'finish_reason': claude_response.get('stop_reason', 'stop')\n",
    "            }],\n",
    "            'usage': {\n",
    "                'prompt_tokens': claude_response.get('usage', {}).get('input_tokens', 0),\n",
    "                'completion_tokens': claude_response.get('usage', {}).get('output_tokens', 0),\n",
    "                'total_tokens': claude_response.get('usage', {}).get('input_tokens', 0) +\n",
    "                                claude_response.get('usage', {}).get('output_tokens', 0)\n",
    "            },\n",
    "            'model': self.model_name\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624fcafc-2ccf-4dd1-97bb-4aa52a4079ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMJudge:\n",
    "    def __init__(self, model_name, region_name):\n",
    "\n",
    "        self.claude_client = ClaudeBedrockClient(model_name, region_name)\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def judge_fact_accuracy(self, fact, context=\"\", temperature=0.1):\n",
    "        full_prompt = f\"\"\"You are a medical expert evaluating the accuracy of medical facts. \n",
    "        Provide a confidence score between 0.0 and 1.0 where:\n",
    "        - 1.0 = Definitely accurate and well-established medical fact\n",
    "        - 0.8-0.9 = Very likely accurate with strong evidence\n",
    "        - 0.6-0.7 = Probably accurate but may have exceptions\n",
    "        - 0.4-0.5 = Uncertain or conflicting evidence\n",
    "        - 0.2-0.3 = Probably inaccurate\n",
    "        - 0.0-0.1 = Definitely inaccurate or harmful\n",
    "        Please evaluate this medical fact:\n",
    "\n",
    "        Fact: {fact}\n",
    "\n",
    "        Context: {context if context else \"No additional context provided\"}\n",
    "\n",
    "        Respond with just a JSON object containing:\n",
    "        {{\n",
    "            \"confidence_score\": <float between 0.0 and 1.0>,\n",
    "            \"reasoning\": \"<brief explanation>\",\n",
    "            \"medical_category\": \"<relevant medical specialty>\"\n",
    "        }}\n",
    "\n",
    "        Provide your assessment as JSON only.\"\"\"\n",
    "\n",
    "        messages = [{\"role\": \"user\", \"content\": full_prompt}]\n",
    "\n",
    "        try:\n",
    "            response = self.claude_client.chat_completion(\n",
    "                messages=messages,\n",
    "                temperature=temperature,\n",
    "                max_tokens=500\n",
    "            )\n",
    "\n",
    "            content = response['choices'][0]['message']['content']\n",
    "\n",
    "            try:\n",
    "                json_match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "                if json_match:\n",
    "                    result = json.loads(json_match.group())\n",
    "\n",
    "                    if 'confidence_score' not in result:\n",
    "                        result['confidence_score'] = 0.5\n",
    "\n",
    "                    result['model_used'] = self.model_name\n",
    "                    result['raw_response'] = content\n",
    "\n",
    "                    return result\n",
    "                else:\n",
    "                    raise ValueError(\"No JSON found in response\")\n",
    "\n",
    "            except (json.JSONDecodeError, ValueError) as e:\n",
    "                print(f\"Failed to parse Claude response as JSON: {e}\")\n",
    "                print(f\"Raw response: {content}\")\n",
    "\n",
    "                score_match = re.search(r'(\\d+\\.?\\d*)', content)\n",
    "                score = float(score_match.group(1)) if score_match else 0.5\n",
    "                if score > 1.0:\n",
    "                    score = score / 10.0\n",
    "\n",
    "                return {\n",
    "                    'confidence_score': min(1.0, max(0.0, score)),\n",
    "                    'reasoning': content[:200] + \"...\" if len(content) > 200 else content,\n",
    "                    'medical_category': 'unknown',\n",
    "                    'model_used': self.model_name,\n",
    "                    'raw_response': content,\n",
    "                    'parsing_error': str(e)\n",
    "                }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in Claude fact judgment: {e}\")\n",
    "            return {\n",
    "                'confidence_score': 0.0,\n",
    "                'reasoning': f'Error occurred: {str(e)}',\n",
    "                'medical_category': 'error',\n",
    "                'model_used': self.model_name,\n",
    "                'error': str(e)\n",
    "            }\n",
    "\n",
    "    def batch_judge_facts(self, facts, context=\"\"):\n",
    "        \"\"\"Judge multiple facts in batch\"\"\"\n",
    "        results = []\n",
    "\n",
    "        for i, fact in enumerate(facts):\n",
    "            print(f\"Judging fact {i + 1}/{len(facts)}: {fact[:100]}...\")\n",
    "            result = self.judge_fact_accuracy(fact, context)\n",
    "            results.append(result)\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a40c413-6bcb-4f6f-8b39-6f4411aa98d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMJudgeVerifier:\n",
    "    def __init__(self, model_name, region_name):\n",
    "        self.model_name = model_name\n",
    "        self.verification_prompt = self._build_verification_prompt()\n",
    "        self.client = ClaudeBedrockClient(model_name, region_name)\n",
    "\n",
    "    def _build_verification_prompt(self):\n",
    "        return \"\"\"You are a medical fact verification specialist. Evaluate the clinical accuracy of the given atomic fact.\n",
    "\n",
    "        VERIFICATION CRITERIA:\n",
    "        1. Medical Accuracy: Is the fact clinically correct?\n",
    "        2. Specificity: Is the claim specific enough to be verifiable?\n",
    "        3. Context Appropriateness: Does the fact make sense in the clinical context?\n",
    "        4. Evidence Base: Is this supported by established medical knowledge?\n",
    "\n",
    "        FACT TO VERIFY: {fact_text}\n",
    "        MEDICAL CONTEXT: {context}\n",
    "\n",
    "        You MUST respond with valid JSON in exactly this format (no extra text):\n",
    "        {{\n",
    "            \"confidence_score\": 0.85,\n",
    "            \"is_accurate\": true,\n",
    "            \"reasoning\": \"detailed explanation of your assessment\",\n",
    "            \"concerns\": [\"any specific concerns or caveats\"]\n",
    "        }}\n",
    "\n",
    "        CRITICAL: The confidence_score must be a decimal number between 0.0 and 1.0.\n",
    "\n",
    "        ASSESSMENT:\"\"\"\n",
    "\n",
    "    def verify_fact(self, fact, medical_context=\"\"):\n",
    "        \"\"\"\n",
    "        Verify a single atomic fact using LLM judge\n",
    "        Returns confidence score [0, 1]\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Format verification prompt\n",
    "            prompt = self.verification_prompt.format(\n",
    "                fact_text=fact.text,\n",
    "                context=medical_context\n",
    "            )\n",
    "            response = self._call_verification_llm(prompt)\n",
    "            confidence = self._parse_verification_response(response)\n",
    "            fact.llm_score = confidence\n",
    "            return confidence\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in LLM verification: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    def _call_verification_llm(self, prompt):\n",
    "        full_prompt = f\"\"\"You are a medical fact verification specialist.\n",
    "\n",
    "    {prompt}\"\"\"\n",
    "        response = self.client.chat_completion(\n",
    "            messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
    "            temperature=0.1,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        return response['choices'][0]['message']['content']\n",
    "\n",
    "    def _parse_verification_response(self, response):\n",
    "        try:\n",
    "            if not response or response.strip() == \"\":\n",
    "                return 0.0\n",
    "\n",
    "            cleaned = response.strip()\n",
    "            cleaned = re.sub(r'```json\\s*', '', cleaned)\n",
    "            cleaned = re.sub(r'```\\s*', '', cleaned)\n",
    "\n",
    "            json_patterns = [\n",
    "                r'\\{[^{}]*\"confidence_score\"[^{}]*:[^{}]*[0-9.]+[^{}]*\\}',\n",
    "                r'\\{.*?\"confidence_score\".*?\\}',\n",
    "                r'\\{.*\\}',\n",
    "            ]\n",
    "            for pattern in json_patterns:\n",
    "                matches = re.findall(pattern, cleaned, re.DOTALL | re.IGNORECASE)\n",
    "                for match in matches:\n",
    "                    try:\n",
    "                        json_str = match.strip()\n",
    "                        json_str = re.sub(r',\\s*}', '}', json_str)\n",
    "                        json_str = re.sub(r',\\s*]', ']', json_str)\n",
    "\n",
    "                        data = json.loads(json_str)\n",
    "                        score = data.get(\"confidence_score\")\n",
    "                        if score is not None:\n",
    "                            return max(0.0, min(1.0, float(score)))\n",
    "\n",
    "                    except (json.JSONDecodeError, ValueError, TypeError):\n",
    "                        continue\n",
    "\n",
    "            score_patterns = [\n",
    "                r'\"confidence_score\"[:\\s]*([0-9.]+)',\n",
    "                r'confidence[_\\s]*score[:\\s]*([0-9.]+)',\n",
    "                r'score[:\\s]*([0-9.]+)',\n",
    "                r'confidence[:\\s]*([0-9.]+)'\n",
    "            ]\n",
    "\n",
    "            for pattern in score_patterns:\n",
    "                match = re.search(pattern, cleaned, re.IGNORECASE)\n",
    "                if match:\n",
    "                    try:\n",
    "                        score = float(match.group(1))\n",
    "                        return max(0.0, min(1.0, score))\n",
    "                    except (ValueError, IndexError):\n",
    "                        continue\n",
    "\n",
    "            number_match = re.search(r'([0-9.]+)', cleaned)\n",
    "            if number_match:\n",
    "                try:\n",
    "                    score = float(number_match.group(1))\n",
    "                    if 0.0 <= score <= 1.0:\n",
    "                        return score\n",
    "                    elif 0.0 <= score <= 5.0:\n",
    "                        return score / 5.0\n",
    "                    elif 0.0 <= score <= 10.0:\n",
    "                        return score / 10.0\n",
    "                except ValueError:\n",
    "                    pass\n",
    "\n",
    "            print(f\"Could not parse verification response: {response[:200]}\")\n",
    "            return 0.5\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing verification response: {e}\")\n",
    "            return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c841de3-b999-4c26-a72d-83bde5637556",
   "metadata": {},
   "source": [
    "## KB Verify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad4c9b6-ddef-44c7-9994-91ffbf4f5abe",
   "metadata": {},
   "source": [
    "### Local KB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7711777-a515-4b46-b75e-e285e5873c8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class LocalMedicalKnowledgeBase:\n",
    "    def __init__(self,\n",
    "                 embedding_model= 'all-MiniLM-L6-v2',\n",
    "                 cache_dir= \"local_kb_cache\"):\n",
    "\n",
    "        self.embedding_model = SentenceTransformer(embedding_model, device='cpu')\n",
    "        self.cache_dir = cache_dir\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "        self.knowledge_base = []\n",
    "        self.knowledge_embeddings = None\n",
    "\n",
    "        self._initialize_medical_knowledge()\n",
    "\n",
    "    def _initialize_medical_knowledge(self):\n",
    "        \"\"\"Initialize with comprehensive medical facts\"\"\"\n",
    "        cache_file = os.path.join(self.cache_dir, \"local_medical_knowledge.pkl\")\n",
    "\n",
    "        if os.path.exists(cache_file):\n",
    "            try:\n",
    "                with open(cache_file, 'rb') as f:\n",
    "                    cached_data = pickle.load(f)\n",
    "                    self.knowledge_base = cached_data['facts']\n",
    "                    self.knowledge_embeddings = cached_data['embeddings']\n",
    "                    print(f\"Loaded {len(self.knowledge_base)} medical facts from cache\")\n",
    "                    return\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load cache: {e}\")\n",
    "        \n",
    "        print(\"Building local medical knowledge base...\")\n",
    "        self.knowledge_base = self._create_medical_facts()\n",
    "        \n",
    "        fact_texts = [fact['text'] for fact in self.knowledge_base]\n",
    "        print(\"Generating embeddings for medical facts...\")\n",
    "        self.knowledge_embeddings = self.embedding_model.encode(fact_texts)\n",
    "        \n",
    "        try:\n",
    "            with open(cache_file, 'wb') as f:\n",
    "                pickle.dump({\n",
    "                    'facts': self.knowledge_base,\n",
    "                    'embeddings': self.knowledge_embeddings\n",
    "                }, f)\n",
    "            print(f\"Cached {len(self.knowledge_base)} medical facts\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to cache knowledge base: {e}\")\n",
    "    \n",
    "    def _create_medical_facts(self):\n",
    "        \"\"\"Create comprehensive medical knowledge base\"\"\"\n",
    "        return [\n",
    "            # CARDIOLOGY\n",
    "            {\"text\": \"Myocardial infarction is caused by coronary artery occlusion leading to cardiac muscle death\", \"category\": \"cardiology\", \"source\": \"medical_textbook\", \"confidence\": 0.95},\n",
    "            {\"text\": \"Chest pain, shortness of breath, and diaphoresis are classic symptoms of myocardial infarction\", \"category\": \"cardiology\", \"source\": \"clinical_guidelines\", \"confidence\": 0.92},\n",
    "            {\"text\": \"Hypertension is defined as systolic blood pressure â‰¥140 mmHg or diastolic â‰¥90 mmHg\", \"category\": \"cardiology\", \"source\": \"AHA_guidelines\", \"confidence\": 0.96},\n",
    "            {\"text\": \"ACE inhibitors are first-line treatment for hypertension and heart failure\", \"category\": \"cardiology\", \"source\": \"treatment_guidelines\", \"confidence\": 0.90},\n",
    "            {\"text\": \"Atrial fibrillation significantly increases the risk of stroke\", \"category\": \"cardiology\", \"source\": \"clinical_studies\", \"confidence\": 0.94},\n",
    "            {\"text\": \"Beta-blockers reduce heart rate and myocardial oxygen demand\", \"category\": \"cardiology\", \"source\": \"pharmacology\", \"confidence\": 0.93},\n",
    "            {\"text\": \"Electrocardiography shows ST-elevation in acute myocardial infarction\", \"category\": \"cardiology\", \"source\": \"diagnostic_criteria\", \"confidence\": 0.91},\n",
    "            {\"text\": \"Cardiac catheterization is the gold standard for diagnosing coronary artery disease\", \"category\": \"cardiology\", \"source\": \"diagnostic_procedures\", \"confidence\": 0.89},\n",
    "            \n",
    "            # ENDOCRINOLOGY\n",
    "            {\"text\": \"Type 1 diabetes mellitus is caused by autoimmune destruction of pancreatic beta cells\", \"category\": \"endocrinology\", \"source\": \"pathophysiology\", \"confidence\": 0.96},\n",
    "            {\"text\": \"Type 2 diabetes mellitus is characterized by insulin resistance and relative insulin deficiency\", \"category\": \"endocrinology\", \"source\": \"pathophysiology\", \"confidence\": 0.95},\n",
    "            {\"text\": \"Metformin is the first-line treatment for type 2 diabetes mellitus\", \"category\": \"endocrinology\", \"source\": \"ADA_guidelines\", \"confidence\": 0.94},\n",
    "            {\"text\": \"HbA1c â‰¥6.5% indicates diabetes mellitus diagnosis\", \"category\": \"endocrinology\", \"source\": \"diagnostic_criteria\", \"confidence\": 0.96},\n",
    "            {\"text\": \"Insulin is absolutely required for type 1 diabetes management\", \"category\": \"endocrinology\", \"source\": \"treatment_standards\", \"confidence\": 0.98},\n",
    "            {\"text\": \"Diabetic ketoacidosis is a life-threatening complication of diabetes\", \"category\": \"endocrinology\", \"source\": \"emergency_medicine\", \"confidence\": 0.93},\n",
    "            {\"text\": \"Hypoglycemia symptoms include diaphoresis, tremor, and altered mental status\", \"category\": \"endocrinology\", \"source\": \"clinical_presentation\", \"confidence\": 0.91},\n",
    "            {\"text\": \"Thyroid stimulating hormone (TSH) is elevated in hypothyroidism\", \"category\": \"endocrinology\", \"source\": \"laboratory_medicine\", \"confidence\": 0.94},\n",
    "            \n",
    "            # PULMONOLOGY\n",
    "            {\"text\": \"Pneumonia causes consolidation visible on chest radiograph\", \"category\": \"pulmonology\", \"source\": \"radiology\", \"confidence\": 0.90},\n",
    "            {\"text\": \"Asthma is characterized by reversible airway obstruction and inflammation\", \"category\": \"pulmonology\", \"source\": \"pathophysiology\", \"confidence\": 0.93},\n",
    "            {\"text\": \"Albuterol is a short-acting beta-2 agonist bronchodilator\", \"category\": \"pulmonology\", \"source\": \"pharmacology\", \"confidence\": 0.96},\n",
    "            {\"text\": \"Chronic obstructive pulmonary disease (COPD) is primarily caused by tobacco smoking\", \"category\": \"pulmonology\", \"source\": \"epidemiology\", \"confidence\": 0.91},\n",
    "            {\"text\": \"Pulmonary embolism can cause sudden onset dyspnea and chest pain\", \"category\": \"pulmonology\", \"source\": \"clinical_presentation\", \"confidence\": 0.89},\n",
    "            {\"text\": \"Spirometry shows reduced FEV1/FVC ratio in obstructive lung disease\", \"category\": \"pulmonology\", \"source\": \"pulmonary_function\", \"confidence\": 0.92},\n",
    "            {\"text\": \"Oxygen saturation below 90% indicates significant hypoxemia\", \"category\": \"pulmonology\", \"source\": \"critical_care\", \"confidence\": 0.88},\n",
    "            \n",
    "            # INFECTIOUS DISEASE\n",
    "            {\"text\": \"Sepsis is a life-threatening organ dysfunction caused by dysregulated host response to infection\", \"category\": \"infectious_disease\", \"source\": \"sepsis_guidelines\", \"confidence\": 0.94},\n",
    "            {\"text\": \"Penicillin is effective against gram-positive bacterial infections\", \"category\": \"infectious_disease\", \"source\": \"microbiology\", \"confidence\": 0.92},\n",
    "            {\"text\": \"Viral infections do not respond to antibiotic treatment\", \"category\": \"infectious_disease\", \"source\": \"antimicrobial_stewardship\", \"confidence\": 0.97},\n",
    "            {\"text\": \"Urinary tract infections commonly present with dysuria and urinary frequency\", \"category\": \"infectious_disease\", \"source\": \"clinical_presentation\", \"confidence\": 0.88},\n",
    "            {\"text\": \"Blood cultures should be obtained before starting empiric antibiotic therapy\", \"category\": \"infectious_disease\", \"source\": \"diagnostic_guidelines\", \"confidence\": 0.85},\n",
    "            {\"text\": \"Methicillin-resistant Staphylococcus aureus (MRSA) requires vancomycin treatment\", \"category\": \"infectious_disease\", \"source\": \"antimicrobial_guidelines\", \"confidence\": 0.91},\n",
    "            \n",
    "            # NEUROLOGY\n",
    "            {\"text\": \"Stroke symptoms include sudden onset focal neurological deficits\", \"category\": \"neurology\", \"source\": \"clinical_criteria\", \"confidence\": 0.93},\n",
    "            {\"text\": \"Tissue plasminogen activator (tPA) is used for acute ischemic stroke within 4.5 hours\", \"category\": \"neurology\", \"source\": \"stroke_guidelines\", \"confidence\": 0.90},\n",
    "            {\"text\": \"Seizures can be focal or generalized based on their origin and spread\", \"category\": \"neurology\", \"source\": \"epilepsy_classification\", \"confidence\": 0.94},\n",
    "            {\"text\": \"Computed tomography (CT) can rapidly detect hemorrhagic stroke\", \"category\": \"neurology\", \"source\": \"neuroimaging\", \"confidence\": 0.87},\n",
    "            {\"text\": \"Lumbar puncture is contraindicated with increased intracranial pressure\", \"category\": \"neurology\", \"source\": \"procedural_guidelines\", \"confidence\": 0.89},\n",
    "            {\"text\": \"Multiple sclerosis causes demyelinating lesions in the central nervous system\", \"category\": \"neurology\", \"source\": \"pathophysiology\", \"confidence\": 0.92},\n",
    "            \n",
    "            # PSYCHIATRY\n",
    "            {\"text\": \"Major depressive disorder requires at least 2 weeks of depressive symptoms\", \"category\": \"psychiatry\", \"source\": \"DSM5\", \"confidence\": 0.96},\n",
    "            {\"text\": \"Selective serotonin reuptake inhibitors (SSRIs) are first-line treatment for depression\", \"category\": \"psychiatry\", \"source\": \"treatment_guidelines\", \"confidence\": 0.88},\n",
    "            {\"text\": \"Bipolar disorder includes both manic and depressive episodes\", \"category\": \"psychiatry\", \"source\": \"DSM5\", \"confidence\": 0.95},\n",
    "            {\"text\": \"Suicidal ideation requires immediate safety assessment and intervention\", \"category\": \"psychiatry\", \"source\": \"crisis_intervention\", \"confidence\": 0.97},\n",
    "            {\"text\": \"Antipsychotic medications are used to treat schizophrenia and psychotic disorders\", \"category\": \"psychiatry\", \"source\": \"psychopharmacology\", \"confidence\": 0.91},\n",
    "            \n",
    "            # PHARMACOLOGY\n",
    "            {\"text\": \"Warfarin requires regular INR monitoring due to narrow therapeutic window\", \"category\": \"pharmacology\", \"source\": \"anticoagulation_guidelines\", \"confidence\": 0.93},\n",
    "            {\"text\": \"Nonsteroidal anti-inflammatory drugs (NSAIDs) can cause gastric ulceration\", \"category\": \"pharmacology\", \"source\": \"adverse_effects\", \"confidence\": 0.90},\n",
    "            {\"text\": \"Statins reduce cholesterol synthesis by inhibiting HMG-CoA reductase\", \"category\": \"pharmacology\", \"source\": \"mechanism_of_action\", \"confidence\": 0.94},\n",
    "            {\"text\": \"Opioids can cause respiratory depression at high doses\", \"category\": \"pharmacology\", \"source\": \"toxicology\", \"confidence\": 0.91},\n",
    "            {\"text\": \"Drug interactions can alter medication effectiveness and safety\", \"category\": \"pharmacology\", \"source\": \"clinical_pharmacology\", \"confidence\": 0.87},\n",
    "            \n",
    "            # PEDIATRICS\n",
    "            {\"text\": \"Sudden infant death syndrome risk is reduced by supine sleeping position\", \"category\": \"pediatrics\", \"source\": \"AAP_guidelines\", \"confidence\": 0.92},\n",
    "            {\"text\": \"Febrile seizures are common in children aged 6 months to 5 years\", \"category\": \"pediatrics\", \"source\": \"pediatric_neurology\", \"confidence\": 0.89},\n",
    "            {\"text\": \"Vaccination schedules protect children from preventable diseases\", \"category\": \"pediatrics\", \"source\": \"immunization_guidelines\", \"confidence\": 0.95},\n",
    "            {\"text\": \"Growth charts assess normal pediatric development\", \"category\": \"pediatrics\", \"source\": \"developmental_medicine\", \"confidence\": 0.86},\n",
    "            \n",
    "            # SURGERY\n",
    "            {\"text\": \"Appendicitis typically presents with right lower quadrant abdominal pain\", \"category\": \"surgery\", \"source\": \"surgical_diagnosis\", \"confidence\": 0.90},\n",
    "            {\"text\": \"Cholecystitis causes right upper quadrant pain and Murphy's sign\", \"category\": \"surgery\", \"source\": \"surgical_examination\", \"confidence\": 0.88},\n",
    "            {\"text\": \"Surgical site infections are prevented by proper sterile technique\", \"category\": \"surgery\", \"source\": \"infection_control\", \"confidence\": 0.91},\n",
    "            {\"text\": \"Bowel obstruction can cause abdominal distension and vomiting\", \"category\": \"surgery\", \"source\": \"surgical_emergencies\", \"confidence\": 0.87},\n",
    "            \n",
    "            # OBSTETRICS/GYNECOLOGY\n",
    "            {\"text\": \"Preeclampsia is defined by hypertension and proteinuria in pregnancy\", \"category\": \"obstetrics\", \"source\": \"ACOG_guidelines\", \"confidence\": 0.94},\n",
    "            {\"text\": \"Folic acid supplementation prevents neural tube defects\", \"category\": \"obstetrics\", \"source\": \"preventive_medicine\", \"confidence\": 0.92},\n",
    "            {\"text\": \"Regular prenatal care improves maternal and fetal outcomes\", \"category\": \"obstetrics\", \"source\": \"prenatal_guidelines\", \"confidence\": 0.89},\n",
    "            {\"text\": \"Gestational diabetes increases risk of macrosomia\", \"category\": \"obstetrics\", \"source\": \"maternal_fetal_medicine\", \"confidence\": 0.86},\n",
    "            \n",
    "            # EMERGENCY MEDICINE\n",
    "            {\"text\": \"Advanced Cardiac Life Support (ACLS) protocols guide cardiac arrest management\", \"category\": \"emergency_medicine\", \"source\": \"AHA_guidelines\", \"confidence\": 0.95},\n",
    "            {\"text\": \"Trauma patients require primary and secondary survey assessment\", \"category\": \"emergency_medicine\", \"source\": \"ATLS_guidelines\", \"confidence\": 0.93},\n",
    "            {\"text\": \"Anaphylaxis is treated with intramuscular epinephrine\", \"category\": \"emergency_medicine\", \"source\": \"allergy_guidelines\", \"confidence\": 0.96},\n",
    "            {\"text\": \"Glasgow Coma Scale assesses level of consciousness\", \"category\": \"emergency_medicine\", \"source\": \"neurological_assessment\", \"confidence\": 0.91},\n",
    "            \n",
    "            # NEPHROLOGY\n",
    "            {\"text\": \"Chronic kidney disease is staged based on glomerular filtration rate\", \"category\": \"nephrology\", \"source\": \"KDIGO_guidelines\", \"confidence\": 0.93},\n",
    "            {\"text\": \"Acute kidney injury can be prerenal, intrinsic, or postrenal\", \"category\": \"nephrology\", \"source\": \"nephrology_classification\", \"confidence\": 0.90},\n",
    "            {\"text\": \"Dialysis is indicated for severe uremia or fluid overload\", \"category\": \"nephrology\", \"source\": \"renal_replacement_therapy\", \"confidence\": 0.88},\n",
    "            {\"text\": \"Proteinuria indicates glomerular kidney disease\", \"category\": \"nephrology\", \"source\": \"laboratory_findings\", \"confidence\": 0.85},\n",
    "            \n",
    "            # GASTROENTEROLOGY\n",
    "            {\"text\": \"Gastroesophageal reflux disease (GERD) causes heartburn and regurgitation\", \"category\": \"gastroenterology\", \"source\": \"clinical_presentation\", \"confidence\": 0.87},\n",
    "            {\"text\": \"Peptic ulcer disease is commonly caused by Helicobacter pylori\", \"category\": \"gastroenterology\", \"source\": \"etiology\", \"confidence\": 0.91},\n",
    "            {\"text\": \"Inflammatory bowel disease includes Crohn's disease and ulcerative colitis\", \"category\": \"gastroenterology\", \"source\": \"disease_classification\", \"confidence\": 0.93},\n",
    "            {\"text\": \"Cirrhosis can lead to portal hypertension and ascites\", \"category\": \"gastroenterology\", \"source\": \"hepatology\", \"confidence\": 0.89},\n",
    "            \n",
    "            # ONCOLOGY\n",
    "            {\"text\": \"Cancer staging determines prognosis and treatment approach\", \"category\": \"oncology\", \"source\": \"cancer_guidelines\", \"confidence\": 0.92},\n",
    "            {\"text\": \"Chemotherapy targets rapidly dividing cancer cells\", \"category\": \"oncology\", \"source\": \"cancer_treatment\", \"confidence\": 0.90},\n",
    "            {\"text\": \"Tumor markers can aid in cancer diagnosis and monitoring\", \"category\": \"oncology\", \"source\": \"laboratory_oncology\", \"confidence\": 0.84},\n",
    "            {\"text\": \"Radiation therapy delivers targeted energy to destroy cancer cells\", \"category\": \"oncology\", \"source\": \"radiation_oncology\", \"confidence\": 0.88},\n",
    "            \n",
    "            # DERMATOLOGY\n",
    "            {\"text\": \"Melanoma is the most dangerous form of skin cancer\", \"category\": \"dermatology\", \"source\": \"dermatopathology\", \"confidence\": 0.91},\n",
    "            {\"text\": \"Topical corticosteroids treat inflammatory skin conditions\", \"category\": \"dermatology\", \"source\": \"dermatologic_therapeutics\", \"confidence\": 0.86},\n",
    "            {\"text\": \"Skin biopsy provides definitive diagnosis of skin lesions\", \"category\": \"dermatology\", \"source\": \"diagnostic_procedures\", \"confidence\": 0.89},\n",
    "            \n",
    "            # RHEUMATOLOGY\n",
    "            {\"text\": \"Rheumatoid arthritis is an autoimmune inflammatory joint disease\", \"category\": \"rheumatology\", \"source\": \"autoimmune_diseases\", \"confidence\": 0.93},\n",
    "            {\"text\": \"Disease-modifying antirheumatic drugs (DMARDs) slow joint destruction\", \"category\": \"rheumatology\", \"source\": \"treatment_guidelines\", \"confidence\": 0.90},\n",
    "            {\"text\": \"Systemic lupus erythematosus affects multiple organ systems\", \"category\": \"rheumatology\", \"source\": \"connective_tissue_disorders\", \"confidence\": 0.88},\n",
    "        ]\n",
    "    \n",
    "    def verify_fact(self, fact, threshold: float = 0.7):\n",
    "        if not self.knowledge_base or self.knowledge_embeddings.size == 0:\n",
    "            return 0.5\n",
    "        \n",
    "        try:\n",
    "            # Generate embedding for the fact\n",
    "            fact_text = fact.text if hasattr(fact, 'text') else str(fact)\n",
    "            fact_embedding = self.embedding_model.encode([fact_text])\n",
    "            \n",
    "            # Calculate similarities\n",
    "            similarities = np.dot(fact_embedding, self.knowledge_embeddings.T)[0]\n",
    "            \n",
    "            # Get the best matching facts\n",
    "            best_matches_idx = np.argsort(similarities)[-3:][::-1]  # Top 3 matches\n",
    "            best_similarities = similarities[best_matches_idx]\n",
    "            \n",
    "            # Calculate confidence score based on similarity and source confidence\n",
    "            confidence_scores = []\n",
    "            for idx, similarity in zip(best_matches_idx, best_similarities):\n",
    "                kb_fact = self.knowledge_base[idx]\n",
    "                source_confidence = kb_fact.get('confidence', 0.8)\n",
    "                \n",
    "                # Combine similarity and source confidence\n",
    "                combined_score = similarity * source_confidence\n",
    "                confidence_scores.append(combined_score)\n",
    "            \n",
    "            # Use the best match\n",
    "            final_confidence = max(confidence_scores) if confidence_scores else 0.0\n",
    "            \n",
    "            # Apply threshold-based adjustment\n",
    "            if max(best_similarities) < threshold:\n",
    "                final_confidence *= 0.5  # Penalize low similarity\n",
    "            \n",
    "            # Update fact with KB score if it's an AtomicFact object\n",
    "            if hasattr(fact, 'kb_score'):\n",
    "                fact.kb_score = final_confidence\n",
    "            \n",
    "            return min(1.0, max(0.0, final_confidence))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in local knowledge base verification: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def search_facts(self, query, limit = 10):\n",
    "        \"\"\"Search for facts similar to the query\"\"\"\n",
    "        if not self.knowledge_base or self.knowledge_embeddings.size == 0:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # Generate embedding for the query\n",
    "            query_embedding = self.embedding_model.encode([query])\n",
    "            \n",
    "            # Calculate similarities\n",
    "            similarities = np.dot(query_embedding, self.knowledge_embeddings.T)[0]\n",
    "            \n",
    "            # Get top matches\n",
    "            top_indices = np.argsort(similarities)[-limit:][::-1]\n",
    "            \n",
    "            results = []\n",
    "            for idx in top_indices:\n",
    "                fact = self.knowledge_base[idx].copy()\n",
    "                fact['similarity_score'] = similarities[idx]\n",
    "                results.append(fact)\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error searching local knowledge base: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_knowledge_stats(self):\n",
    "        \"\"\"Get statistics about the local knowledge base\"\"\"\n",
    "        if not self.knowledge_base:\n",
    "            return {\"total_facts\": 0, \"categories\": [], \"sources\": []}\n",
    "        \n",
    "        categories = {}\n",
    "        sources = {}\n",
    "        \n",
    "        for fact in self.knowledge_base:\n",
    "            cat = fact.get('category', 'unknown')\n",
    "            src = fact.get('source', 'unknown')\n",
    "            \n",
    "            categories[cat] = categories.get(cat, 0) + 1\n",
    "            sources[src] = sources.get(src, 0) + 1\n",
    "        \n",
    "        return {\n",
    "            \"total_facts\": len(self.knowledge_base),\n",
    "            \"categories\": categories,\n",
    "            \"sources\": sources,\n",
    "            \"has_embeddings\": self.knowledge_embeddings is not None and self.knowledge_embeddings.size > 0\n",
    "        }\n",
    "    \n",
    "    def add_facts(self, new_facts: List[Dict]):\n",
    "        \"\"\"Add new facts to the knowledge base\"\"\"\n",
    "        self.knowledge_base.extend(new_facts)\n",
    "\n",
    "        # Regenerate embeddings\n",
    "        fact_texts = [fact['text'] for fact in self.knowledge_base]\n",
    "        print(f\"Regenerating embeddings for {len(fact_texts)} facts...\")\n",
    "        self.knowledge_embeddings = self.embedding_model.encode(fact_texts)\n",
    "\n",
    "        print(f\"Added {len(new_facts)} new facts to knowledge base\")\n",
    "\n",
    "\n",
    "class LocalKnowledgeBaseVerifier:\n",
    "    \"\"\"Drop-in replacement using local knowledge base\"\"\"\n",
    "    \n",
    "    def __init__(self, knowledge_sources: List[str] = None):\n",
    "        self.knowledge_sources = knowledge_sources or [\"Local Medical DB\"]\n",
    "        self.local_kb = LocalMedicalKnowledgeBase()\n",
    "        \n",
    "        print(\"Local Knowledge Base Verifier initialized\")\n",
    "        stats = self.local_kb.get_knowledge_stats()\n",
    "        print(f\"Loaded {stats['total_facts']} facts across {len(stats['categories'])} categories\")\n",
    "    \n",
    "    def verify_fact(self, fact, threshold: float = 0.7):\n",
    "        \"\"\"Verify fact against local knowledge base\"\"\"\n",
    "        return self.local_kb.verify_fact(fact, threshold)\n",
    "    \n",
    "    def search_facts(self, query, limit = 10):\n",
    "        \"\"\"Search for relevant facts\"\"\"\n",
    "        return self.local_kb.search_facts(query, limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b356bf-16d8-4be2-8fbc-231ced7e9d74",
   "metadata": {},
   "source": [
    "## Fact Verifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8601f2a-8713-42a9-918e-e243e1bd3abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactualRewardCalculator:\n",
    "    def __init__(self, agreement_threshold):\n",
    "      self.agreement_threshold = agreement_threshold\n",
    "\n",
    "    def extract_think_content(self, generation):\n",
    "        \"\"\"Extract content from <think> tags\"\"\"\n",
    "        think_match = re.search(r'<think>(.*?)</think>', generation, re.DOTALL | re.IGNORECASE)\n",
    "        if think_match:\n",
    "            content = think_match.group(1).strip()\n",
    "            return re.sub(r'^Reasoning:\\s*', '', content)\n",
    "        return \"\"\n",
    "\n",
    "    def compute_factual_reward(self, facts_list):\n",
    "        if not facts_list:\n",
    "            return {\n",
    "                \"factual_reward\": 0.0,\n",
    "                \"factual_analysis\": {\n",
    "                    \"factual_reward\": 0.0,\n",
    "                    \"individual_rewards\": [],\n",
    "                    \"agreement_rate\": 0.0,\n",
    "                    \"avg_llm_score\": 0.0,\n",
    "                    \"avg_kb_score\": 0.0,\n",
    "                    \"num_facts\": 0\n",
    "                },\n",
    "                \"error\": \"No facts provided\"\n",
    "            }\n",
    "        atomic_facts = []\n",
    "        for fact_item in facts_list:\n",
    "            if isinstance(fact_item, AtomicFact):\n",
    "                atomic_facts.append(fact_item)\n",
    "            else:\n",
    "                fact = AtomicFact(\n",
    "                    text=fact_item.get(\"text\", \"\"),\n",
    "                    category=fact_item.get(\"category\", \"unknown\"),\n",
    "                    source_sentence=fact_item.get(\"source_sentence\", \"\")\n",
    "                )\n",
    "                fact.llm_score = fact_item.get(\"llm_score\", 0.0)\n",
    "                fact.kb_score = fact_item.get(\"kb_score\", 0.0)\n",
    "                atomic_facts.append(fact)\n",
    "\n",
    "        individual_rewards = []\n",
    "        agreement_count = 0\n",
    "        llm_scores = []\n",
    "        kb_scores = []\n",
    "\n",
    "        for fact in atomic_facts:\n",
    "            fact_reward = self._compute_individual_fact_reward(fact)\n",
    "            individual_rewards.append(fact_reward)\n",
    "\n",
    "            if abs(fact.llm_score - fact.kb_score) <= self.agreement_threshold:\n",
    "                agreement_count += 1\n",
    "\n",
    "            llm_scores.append(fact.llm_score)\n",
    "            kb_scores.append(fact.kb_score)\n",
    "\n",
    "        factual_reward = sum(individual_rewards) / len(individual_rewards) if individual_rewards else 0.0\n",
    "\n",
    "        agreement_rate = agreement_count / len(atomic_facts) if atomic_facts else 0.0\n",
    "        avg_llm_score = sum(llm_scores) / len(llm_scores) if llm_scores else 0.0\n",
    "        avg_kb_score = sum(kb_scores) / len(kb_scores) if kb_scores else 0.0\n",
    "\n",
    "        factual_analysis = {\n",
    "            \"factual_reward\": factual_reward,\n",
    "            \"individual_rewards\": individual_rewards,\n",
    "            \"agreement_rate\": agreement_rate,\n",
    "            \"avg_llm_score\": avg_llm_score,\n",
    "            \"avg_kb_score\": avg_kb_score,\n",
    "            \"num_facts\": len(atomic_facts)\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            \"factual_reward\": factual_reward,\n",
    "            \"factual_analysis\": factual_analysis,\n",
    "            \"error\": None\n",
    "        }\n",
    "\n",
    "    def _compute_individual_fact_reward(self, fact):\n",
    "        llm_score = fact.llm_score\n",
    "        kb_score = fact.kb_score\n",
    "\n",
    "        # Weight LLM more heavily (e.g., 70% LLM, 30% KB)\n",
    "        llm_weight = 0.7\n",
    "        kb_weight = 0.3\n",
    "\n",
    "        base_score = llm_weight * llm_score + kb_weight * kb_score\n",
    "\n",
    "        agreement = abs(llm_score - kb_score) <= self.agreement_threshold\n",
    "        if agreement:\n",
    "            return base_score\n",
    "        else:\n",
    "            return base_score * 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8132d217-b78c-49fb-b814-80555c323a06",
   "metadata": {},
   "source": [
    "## Atomic Fact Verification System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b910a441-b0d8-407f-aa6c-48ec504a0fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtomicFactVerificationSystem:\n",
    "    def __init__(self,agreement_threshold,umls_api_key=None):\n",
    "        self.extractor = AtomicFactExtractor(model_name=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "                                             region_name=\"us-east-1\")\n",
    "        self.llm_verifier = LLMJudgeVerifier(model_name=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "                                             region_name=\"us-east-1\")\n",
    "        self.kb_verifier = LocalKnowledgeBaseVerifier()\n",
    "        self.reward_calculator = FactualRewardCalculator(agreement_threshold)\n",
    "        self.training_mode = False\n",
    "\n",
    "    def process_response(self, reasoning_text, context=\"\"):\n",
    "        try:\n",
    "            facts = self.extractor.extract_facts(reasoning_text)\n",
    "\n",
    "            if not facts:\n",
    "                return {\n",
    "                    \"facts\": [],\n",
    "                    \"factual_analysis\": self._empty_analysis(),\n",
    "                    \"error\": \"No facts extracted\"\n",
    "                }\n",
    "\n",
    "            # Check if we're in training mode to skip expensive verification\n",
    "            if self.training_mode:  # Simplified check - no need for hasattr\n",
    "                for fact in facts:\n",
    "                    fact.llm_score = self._get_training_heuristic_score(fact)\n",
    "                    fact.kb_score = self._get_training_heuristic_score(fact)\n",
    "            else:\n",
    "                # During evaluation: Full verification with rate limiting protection\n",
    "                for fact in facts:\n",
    "                    try:\n",
    "                        # LLM verification\n",
    "                        self.llm_verifier.verify_fact(fact, context)\n",
    "                        # KB verification\n",
    "                        self.kb_verifier.verify_fact(fact)\n",
    "                        # Small delay to avoid rate limits during evaluation\n",
    "                        time.sleep(4)\n",
    "                    except Exception as verification_error:\n",
    "                        print(f\"Verification error for fact '{fact.text}': {verification_error}\")\n",
    "                        # Use fallback scores if verification fails\n",
    "                        if not hasattr(fact, 'llm_score') or fact.llm_score == 0.0:\n",
    "                            fact.llm_score = 0.5\n",
    "                        if not hasattr(fact, 'kb_score') or fact.kb_score == 0.0:\n",
    "                            fact.kb_score = 0.5\n",
    "\n",
    "            factual_result = self.reward_calculator.compute_factual_reward(facts)\n",
    "            facts_as_dicts = [self._fact_to_dict(fact) for fact in facts]\n",
    "\n",
    "            return {\n",
    "                \"facts\": facts_as_dicts,\n",
    "                \"factual_analysis\": factual_result,\n",
    "                \"error\": None\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in process_response: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return {\n",
    "                \"facts\": [],\n",
    "                \"factual_analysis\": self._empty_analysis(),\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "\n",
    "    def _get_training_heuristic_score(self, fact):\n",
    "        fact_text = fact.text.lower()\n",
    "        # Simple heuristics based on medical fact patterns\n",
    "        if any(keyword in fact_text for keyword in ['patient', 'symptom', 'diagnosis', 'treatment']):\n",
    "            # Medical context facts get moderate-high scores\n",
    "            return 0.7\n",
    "        elif any(keyword in fact_text for keyword in ['anatomy', 'physiology', 'mechanism']):\n",
    "            # Basic medical knowledge gets high scores\n",
    "            return 0.8\n",
    "        elif len(fact_text) < 20:\n",
    "            # Very short facts might be incomplete\n",
    "            return 0.5\n",
    "        elif any(keyword in fact_text for keyword in ['may', 'might', 'could', 'possibly']):\n",
    "            # Uncertain statements get lower scores\n",
    "            return 0.6\n",
    "        else:\n",
    "            # Default reasonable score\n",
    "            return 0.7\n",
    "\n",
    "    def _fact_to_dict(self, fact: AtomicFact):\n",
    "        return {\n",
    "            \"text\": fact.text,\n",
    "            \"category\": fact.category,\n",
    "            \"llm_score\": fact.llm_score,\n",
    "            \"kb_score\": fact.kb_score,\n",
    "            \"source_sentence\": fact.source_sentence\n",
    "        }\n",
    "\n",
    "    def _empty_analysis(self):\n",
    "        return {\n",
    "            \"factual_reward\": 0.0,\n",
    "            \"individual_rewards\": [],\n",
    "            \"agreement_rate\": 0.0,\n",
    "            \"avg_llm_score\": 0.0,\n",
    "            \"avg_kb_score\": 0.0,\n",
    "            \"num_facts\": 0\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed73176f-5fd9-470d-9f07-30a18a882574",
   "metadata": {},
   "source": [
    "# Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0441fff-c0cb-462d-a126-5ceb59547ee1",
   "metadata": {},
   "source": [
    "## MetricsTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e94fef-8a09-4a0c-a91f-0b204f638a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsTracker:\n",
    "    def __init__(self):\n",
    "        self.epoch_metrics = []\n",
    "\n",
    "    def add_epoch_metrics(self, epoch, metrics):\n",
    "        self.epoch_metrics.append({\n",
    "            'epoch': epoch,\n",
    "            'correctness_violations_rate': metrics.get('correctness_violations_rate', 0),\n",
    "            'answer_leaking_violations_rate': metrics.get('answer_leaking_violations_rate', 0), \n",
    "            'format_violations_rate': metrics.get('format_violations_rate', 0),\n",
    "            'factual_violations_rate': metrics.get('factual_violations_rate', 0),\n",
    "            'bad_ood_high_rewards_rate': metrics.get('bad_ood_high_rewards_rate', 0),\n",
    "            'avg_reward': metrics.get('avg_reward', 0),\n",
    "            'avg_factual': metrics.get('avg_factual', 0)\n",
    "        })\n",
    "\n",
    "    def print_stage_summary(self, stage_name, start_epoch=0, end_epoch=None):\n",
    "        \"\"\"Print mean and std for a training stage\"\"\"\n",
    "        if end_epoch is None:\n",
    "            end_epoch = len(self.epoch_metrics)\n",
    "\n",
    "        stage_data = self.epoch_metrics[start_epoch:end_epoch]\n",
    "        if not stage_data:\n",
    "            print(f\"No data for {stage_name}\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"{stage_name.upper()} SUMMARY (Epochs {start_epoch+1}-{end_epoch})\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Extract values for each metric\n",
    "        metrics = {\n",
    "            'Correctness Violations': [d['correctness_violations_rate'] for d in stage_data],\n",
    "            'Answer Leaking': [d['answer_leaking_violations_rate'] for d in stage_data],\n",
    "            'Format Violations': [d['format_violations_rate'] for d in stage_data], \n",
    "            'Factual Violations': [d['factual_violations_rate'] for d in stage_data],\n",
    "            'Bad OOD Rewards': [d['bad_ood_high_rewards_rate'] for d in stage_data],\n",
    "            'Average Reward': [d['avg_reward'] for d in stage_data],\n",
    "            'Factual Reward': [d['avg_factual'] for d in stage_data]\n",
    "        }\n",
    "        \n",
    "        # Print statistics\n",
    "        for metric_name, values in metrics.items():\n",
    "            mean_val = np.mean(values)\n",
    "            std_val = np.std(values)\n",
    "            print(f\"{metric_name:20s}: {mean_val:.2f} Â± {std_val:.2f}\")\n",
    "            \n",
    "        # Calculate improvement (first vs last epoch)\n",
    "        if len(stage_data) > 1:\n",
    "            print(f\"\\n{stage_name} Improvements:\")\n",
    "            first_epoch = stage_data[0]\n",
    "            last_epoch = stage_data[-1]\n",
    "            \n",
    "            # For violation rates, improvement is decrease (negative change)\n",
    "            for metric in ['correctness_violations_rate', 'answer_leaking_violations_rate', \n",
    "                          'format_violations_rate', 'factual_violations_rate', 'bad_ood_high_rewards_rate']:\n",
    "                change = last_epoch[metric] - first_epoch[metric]\n",
    "                improvement = -change  # Negative change is improvement for violations\n",
    "                print(f\"  {metric.replace('_', ' ').title():25s}: {improvement:+.2f}\")\n",
    "            \n",
    "            # For rewards, improvement is increase (positive change) \n",
    "            for metric in ['avg_reward', 'avg_factual']:\n",
    "                change = last_epoch[metric] - first_epoch[metric]\n",
    "                print(f\"  {metric.replace('_', ' ').title():25s}: {change:+.2f}\")\n",
    "\n",
    "\n",
    "    def print_metrics_with_std(self, metrics, title=\"METRICS\", include_std=True):\n",
    "        \"\"\"Print metrics in consistent format with optional standard deviations\"\"\"\n",
    "        print(f\"\\n{title}\")\n",
    "        print(\"-\" * len(title))\n",
    "        \n",
    "        # Define metric display configurations\n",
    "        metric_configs = [\n",
    "            ('correctness_violations_rate', 'Correctness Violations', 'correctness_violations_std'),\n",
    "            ('answer_leaking_violations_rate', 'Answer Leakage Rate', 'answer_leaking_violations_std'),\n",
    "            ('format_violations_rate', 'Bad Format Rate', 'format_violations_std'),\n",
    "            ('factual_violations_rate', 'Factual Violations', 'factual_violations_std'),\n",
    "            ('bad_ood_high_rewards_rate', 'High Rewards Rate', 'bad_ood_high_rewards_std'),\n",
    "            ('avg_reward', 'Average Reward', 'std_reward'),\n",
    "            ('avg_factual', 'Factual Reward', None),\n",
    "            ('accuracy', 'Accuracy', None)\n",
    "        ]\n",
    "        \n",
    "        for metric_key, display_name, std_key in metric_configs:\n",
    "            if metric_key in metrics:\n",
    "                value = metrics[metric_key]\n",
    "                if include_std and std_key and std_key in metrics:\n",
    "                    std_value = metrics[std_key]\n",
    "                    print(f\"{display_name}: {value:.2f} (Â±{std_value:.2f})\")\n",
    "                else:\n",
    "                    print(f\"{display_name}: {value:.2f}\")\n",
    "    \n",
    "    def add_adversarial_stage_metrics(self, pre_metrics, post_metrics):\n",
    "        \"\"\"Add special adversarial training stage metrics\"\"\"\n",
    "        self.add_epoch_metrics(-1, pre_metrics)\n",
    "        self.add_epoch_metrics(999, post_metrics)\n",
    "        \n",
    "        # Calculate improvement metrics\n",
    "        improvement_metrics = {}\n",
    "        for key in pre_metrics:\n",
    "            if key in post_metrics and isinstance(pre_metrics[key], (int, float)):\n",
    "                improvement_metrics[f\"{key}_improvement\"] = post_metrics[key] - pre_metrics[key]\n",
    "        \n",
    "        # Store improvement metrics\n",
    "        self.add_epoch_metrics(1000, improvement_metrics)\n",
    "    \n",
    "    def print_adversarial_impact_analysis(self):\n",
    "        \"\"\"Print comprehensive adversarial training impact analysis\"\"\"\n",
    "        if -1 in self.epoch_metrics and 999 in self.epoch_metrics:\n",
    "            pre_metrics = self.epoch_metrics[-1]\n",
    "            post_metrics = self.epoch_metrics[999]\n",
    "            \n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"ADVERSARIAL TRAINING IMPACT ANALYSIS\")\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "            print(\"\\nPRE-ADVERSARIAL PERFORMANCE:\")\n",
    "            self.print_metrics_with_std(pre_metrics, \"\", include_std=True)\n",
    "            \n",
    "            print(\"\\nPOST-ADVERSARIAL PERFORMANCE:\")\n",
    "            self.print_metrics_with_std(post_metrics, \"\", include_std=True)\n",
    "\n",
    "        else:\n",
    "            print(\"Adversarial training metrics not found\")\n",
    "    \n",
    "    \n",
    "    def save_metrics(self, filepath=\"training_metrics.json\"):\n",
    "        \"\"\"Save metrics to JSON file\"\"\"\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(self.epoch_metrics, f, indent=2)\n",
    "        print(f\"Metrics saved to {filepath}\")\n",
    "\n",
    "def monitor_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        memory_allocated = torch.cuda.memory_allocated() / 1024 ** 3\n",
    "        memory_reserved = torch.cuda.memory_reserved() / 1024 ** 3\n",
    "        print(f\"GPU Memory - Allocated: {memory_allocated:.2f}GB, Reserved: {memory_reserved:.2f}GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6e67e0-0ab6-4efa-a641-06ad4b623f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_json(\"Datasets/medqa_train.json\")\n",
    "test_df = pd.read_json(\"Datasets/medqa_test.json\")\n",
    "train_subset_df = train_df.iloc[:100]\n",
    "train_subset_df.to_json(\"Datasets/medqa_train_sample.json\", orient='records', indent=4)\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "reward_config = {\n",
    "    'w_b': 1.0,\n",
    "    'w_a': 0.2,\n",
    "    'w_s': 0.2,\n",
    "    'w_fact': 0.2,\n",
    "    'tau_answer': 0.7,\n",
    "    'tau_preamble': 15,\n",
    "    'lambda_s': 1.0\n",
    "}\n",
    "print(\"Initial memory state:\")\n",
    "monitor_memory()\n",
    "\n",
    "model_url = \"llama_RM_ADV\"\n",
    "\n",
    "trainer_llama_rm = PolicyTrainer(\n",
    "    model_path=model_url,\n",
    "    reward_config=reward_config,\n",
    "    use_baseline=True,\n",
    "    umls_api_key=\"UMLS_KEY\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700bb89b-705e-4dad-9662-d5b08eb89d20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "beebf32e-deff-4a99-a604-db5cea058e74",
   "metadata": {},
   "source": [
    "## Llama + RM on MMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e1876b-7b29-4ac6-93e0-838afcbf5a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_json(\"Datasets/medqa_train.json\")\n",
    "test_df = pd.read_json(\"Datasets/medqa_test.json\")\n",
    "train_subset_df = train_df.iloc[:100]\n",
    "train_subset_df.to_json(\"Datasets/medqa_train_sample.json\", orient='records', indent=4)\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "reward_config = {\n",
    "    'w_b': 1.0,\n",
    "    'w_a': 0.2,\n",
    "    'w_s': 0.2,\n",
    "    'w_fact': 0.2,\n",
    "    'tau_answer': 0.7,\n",
    "    'tau_preamble': 15,\n",
    "    'lambda_s': 1.0\n",
    "}\n",
    "print(\"Initial memory state:\")\n",
    "monitor_memory()\n",
    "\n",
    "model_url = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "trainer_llama_rm = PolicyTrainer(\n",
    "    model_path=model_url,\n",
    "    reward_config=reward_config,\n",
    "    use_baseline=True,\n",
    "    umls_api_key=\"UMLS_KEY\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5408a0e-0adb-449b-97d2-cd8db4837b45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "reward_results = trainer_llama_rm.train_reward_policy(\n",
    "    train_data_path=\"Datasets/medqa_train_sample.json\",\n",
    "    test_data_path=\"Datasets/mmlu_pro_health_test.json\",\n",
    "    stage1_epochs=1,\n",
    "    stage2_epochs=1,\n",
    "    max_eval_examples=20\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Set your S3 bucket and path\n",
    "bucket_name = 'b'\n",
    "s3_prefix = 'medqa-models/reinforce-llama-run3/'\n",
    "\n",
    "# Save model locally\n",
    "local_model_dir = '/home/ec2-user/SageMaker/llama_RM_only_2'\n",
    "os.makedirs(local_model_dir, exist_ok=True)\n",
    "\n",
    "trainer_llama_rm.model.save_pretrained(local_model_dir)\n",
    "trainer_llama_rm.tokenizer.save_pretrained(local_model_dir)\n",
    "torch.save(trainer_llama_rm.baseline_network.state_dict(), os.path.join(local_model_dir, \"baseline_network.pt\"))\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "for root, dirs, files in os.walk(local_model_dir):\n",
    "    for file in files:\n",
    "        local_path = os.path.join(root, file)\n",
    "        relative_path = os.path.relpath(local_path, local_model_dir)\n",
    "        s3_path = os.path.join(s3_prefix, relative_path)\n",
    "        print(f\"Uploading {relative_path} to s3://{bucket_name}/{s3_path}\")\n",
    "        s3.upload_file(local_path, bucket_name, s3_path)\n",
    "print(\"\\nUpload to S3 complete!\")\n",
    "print(f\"All model files are stored at s3://{bucket_name}/{s3_prefix}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd076fb-f867-4f62-8e14-d498617122a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adversarial_results = trainer_llama_rm.load_reward_model_and_train_adversarial(\n",
    "    saved_model_path='/home/ec2-user/SageMaker/llama_RM_only_2',\n",
    "    train_data_path=\"Datasets/medqa_train_sample.json\",\n",
    "    test_data_path=\"Datasets/mmlu_pro_health_test.json\",\n",
    "    adversarial_cycles=2,\n",
    "    max_eval_examples=20\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b0b2ee5-8da1-40db-9f55-48532ff7f494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set your S3 bucket and path\n",
    "bucket_name = 'b'\n",
    "s3_prefix = 'medqa-models/reinforce-llama-run3/'\n",
    "\n",
    "# Save model locally\n",
    "local_model_dir = '/home/ec2-user/SageMaker/llama_RM_ADV_2'\n",
    "os.makedirs(local_model_dir, exist_ok=True)\n",
    "\n",
    "trainer_llama_rm.model.save_pretrained(local_model_dir)\n",
    "trainer_llama_rm.tokenizer.save_pretrained(local_model_dir)\n",
    "torch.save(trainer_llama_rm.baseline_network.state_dict(), os.path.join(local_model_dir, \"baseline_network.pt\"))\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "for root, dirs, files in os.walk(local_model_dir):\n",
    "    for file in files:\n",
    "        local_path = os.path.join(root, file)\n",
    "        relative_path = os.path.relpath(local_path, local_model_dir)\n",
    "        s3_path = os.path.join(s3_prefix, relative_path)\n",
    "        print(f\"Uploading {relative_path} to s3://{bucket_name}/{s3_path}\")\n",
    "        s3.upload_file(local_path, bucket_name, s3_path)\n",
    "print(\"\\nUpload to S3 complete!\")\n",
    "print(f\"All model files are stored at s3://{bucket_name}/{s3_prefix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92398aa-2aea-444a-b650-cab6182765ca",
   "metadata": {},
   "source": [
    "## Llama + RM + ADV on USMLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08acddf3-b198-43d3-834e-e45316b79f89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "train_df = pd.read_json(\"Datasets/medqa_train.json\")\n",
    "test_df = pd.read_json(\"Datasets/medqa_test.json\")\n",
    "\n",
    "reward_config = {\n",
    "    'w_b': 1.0,\n",
    "    'w_a': 0.2,\n",
    "    'w_s': 0.2,\n",
    "    'w_fact': 0.2,\n",
    "    'tau_answer': 0.7,\n",
    "    'tau_preamble': 15,\n",
    "    'lambda_s': 1.0\n",
    "}\n",
    "print(\"Initial memory state:\")\n",
    "monitor_memory()\n",
    "model_url = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "trainer_3 = PolicyTrainer(\n",
    "    model_path=model_url,\n",
    "    reward_config=reward_config,\n",
    "    use_baseline=True,\n",
    "    umls_api_key=\"UMLS_KEY\"\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "train_subset_df = train_df.iloc[:100]\n",
    "train_subset_df.to_json(\"Datasets/medqa_train_sample.json\", orient='records', indent=4)\n",
    "\n",
    "comparison_results = trainer_3.train_reward_policy(\n",
    "    train_data_path=\"Datasets/medqa_train_sample.json\",\n",
    "    test_data_path=\"Datasets/medqa_test.json\",\n",
    "    stage1_epochs=1,\n",
    "    stage2_epochs=1,\n",
    "    max_eval_examples=20\n",
    ")\n",
    "print(\"TRAINING COMPLETE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7269f56a-0a8a-42d1-a4a1-7e661d4d4954",
   "metadata": {},
   "source": [
    "## OOD with MMLU-PRO (OG llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef834932-6382-42eb-bb64-fbbfd43541ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "local_model_dir = '/home/ec2-user/SageMaker/Adversarial_RM_llamaOriginal_RM'\n",
    "\n",
    "# Configuration for reward function\n",
    "reward_config = {\n",
    "    'w_b': 1.0,\n",
    "    'w_a': 0.2,\n",
    "    'w_s': 0.2,\n",
    "    'w_f': 0.2,\n",
    "    'tau_answer': 0.7,\n",
    "    'tau_preamble': 15,\n",
    "    'lambda_s': 1.0\n",
    "}\n",
    "\n",
    "trainer_2 = PolicyTrainer(\n",
    "    model_path=local_model_dir,\n",
    "    reward_config=reward_config,\n",
    "    use_baseline=True,\n",
    "    umls_api_key=\"UMLS_KEY\"\n",
    ")\n",
    "\n",
    "# 1. Reload model + tokenizer\n",
    "trainer_2.model = AutoModelForCausalLM.from_pretrained(local_model_dir)\n",
    "trainer_2.tokenizer = AutoTokenizer.from_pretrained(local_model_dir)\n",
    "\n",
    "# 2. Reload baseline network\n",
    "baseline_path = os.path.join(local_model_dir, \"baseline_network.pt\")\n",
    "trainer_2.baseline_network.load_state_dict(torch.load(baseline_path, map_location=trainer_2.device))\n",
    "trainer_2.baseline_network.to(trainer.device)\n",
    "trainer_2.baseline_network.eval()\n",
    "\n",
    "print(\"\\nRunning final evaluation on the test set...\")\n",
    "final_results = trainer_2.evaluate_model(\"mmlu_pro_health_test.json\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9f7b17-d3dd-4274-b273-ead0dca017e8",
   "metadata": {},
   "source": [
    "## Llama only (MMLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf5d241-2b03-4356-a78a-0f96b63f24c1",
   "metadata": {},
   "source": [
    "## With Qwen2.5-05B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dc0378b-d5c5-4abd-a541-0362760e15df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING']=\"1\"\n",
    "os.environ['TORCH_USE_CUDA_DSA'] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70de6978-6908-4282-a317-3f34d857789d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "train_df = pd.read_json(\"medqa_train.json\")\n",
    "test_df = pd.read_json(\"medqa_test.json\")\n",
    "\n",
    "reward_config = {\n",
    "    'w_b': 1.0,\n",
    "    'w_a': 0.2,\n",
    "    'w_s': 0.2,\n",
    "    'w_fact': 0.2,\n",
    "    'tau_answer': 0.7,\n",
    "    'tau_preamble': 15,\n",
    "    'lambda_s': 1.0\n",
    "}\n",
    "print(\"Initial memory state:\")\n",
    "monitor_memory()\n",
    "model_url = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "# Train policy stages and save\n",
    "trainer = PolicyTrainer(\n",
    "    model_path=model_url,\n",
    "    reward_config=reward_config,\n",
    "    use_baseline=True,\n",
    "    umls_api_key=\"key\"\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "train_subset_df = train_df.iloc[:100]\n",
    "train_subset_df.to_json(\"medqa_train_sample.json\", orient='records', indent=4)\n",
    "\n",
    "policy_metrics = trainer.train_reward_policy(\n",
    "    train_data_path=\"medqa_train_sample.json\",\n",
    "    test_data_path=\"medqa_test.json\",\n",
    "    stage1_epochs=1,\n",
    "    stage2_epochs=1,\n",
    "    max_eval_examples=20,\n",
    "    save_checkpoint_path=\"./qwen_25_policy_checkpoint\"\n",
    ")\n",
    "\n",
    "print(\"Policy training complete! Checkpoint saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a23846d-6291-47a5-8d0b-83fe3f7b893a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = PolicyTrainer.load_from_checkpoint(\n",
    "    checkpoint_path=\"qwen_25_policy_checkpoint\",\n",
    "    reward_config=reward_config,\n",
    "    umls_api_key=\"your-key\"\n",
    ")\n",
    "\n",
    "adversarial_metrics, adversarial_results = trainer.train_adversarial(\n",
    "    train_data_path=\"medqa_train_sample.json\",\n",
    "    test_data_path=\"medqa_test.json\",\n",
    "    num_cycles=3,\n",
    "    max_eval_examples=20\n",
    ")\n",
    "\n",
    "print(\"Adversarial training complete!\")\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a49359-1ca4-4283-ac99-ce94c85c5d65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_subset_df = train_df.iloc[:100]\n",
    "train_subset_df.to_json(\"medqa_train_sample.json\", orient='records', indent=4)\n",
    "\n",
    "comparison_results = trainer_3.compare_training_approaches(\n",
    "    train_data_path=\"medqa_train_sample.json\",\n",
    "    test_data_path=\"medqa_test.json\",\n",
    "    stage1_epochs=1,\n",
    "    stage2_epochs=1,\n",
    "    max_eval_examples=20\n",
    ")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99183ac-00aa-4a5d-afe2-336d9ae9cc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'b'\n",
    "s3_prefix = 'medqa-models/reinforce-qwen-run2/'\n",
    "\n",
    "# Save model locally\n",
    "local_model_dir = '/home/ec2-user/SageMaker/Adversarial_RM_qwenSFT_RM'\n",
    "os.makedirs(local_model_dir, exist_ok=True)\n",
    "\n",
    "trainer_3.model.save_pretrained(local_model_dir)\n",
    "trainer_3.tokenizer.save_pretrained(local_model_dir)\n",
    "torch.save(trainer_3.baseline_network.state_dict(), os.path.join(local_model_dir, \"baseline_network.pt\"))\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "for root, dirs, files in os.walk(local_model_dir):\n",
    "    for file in files:\n",
    "        local_path = os.path.join(root, file)\n",
    "        relative_path = os.path.relpath(local_path, local_model_dir)\n",
    "        s3_path = os.path.join(s3_prefix, relative_path)\n",
    "        print(f\"Uploading {relative_path} to s3://{bucket_name}/{s3_path}\")\n",
    "        s3.upload_file(local_path, bucket_name, s3_path)\n",
    "print(\"\\nUpload to S3 complete!\")\n",
    "print(f\"All model files are stored at s3://{bucket_name}/{s3_prefix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2dfa16-41fc-4c6a-907d-bd960ca46ed9",
   "metadata": {},
   "source": [
    "## Qwen2.5 RM Only on MMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016031ee-eddf-433f-81ca-8502219f139f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "reward_config = {\n",
    "    'w_b': 1.0,\n",
    "    'w_a': 0.2,\n",
    "    'w_s': 0.2,\n",
    "    'w_fact': 0.2,\n",
    "    'tau_answer': 0.7,\n",
    "    'tau_preamble': 15,\n",
    "    'lambda_s': 1.0\n",
    "}\n",
    "model_url = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "trainer_qwen_rm = PolicyTrainer(\n",
    "    model_path=model_url,\n",
    "    reward_config=reward_config,\n",
    "    use_baseline=True,\n",
    "    umls_api_key=\"UMLS_KEY\"\n",
    ")\n",
    "\n",
    "results = trainer_qwen_rm.train_reward_policy(\n",
    "    train_data_path=\"Datasets/medqa_train_sample.json\",\n",
    "    test_data_path=\"Datasets/mmlu_pro_health_test.json\",\n",
    "    stage1_epochs=1,\n",
    "    stage2_epochs=1,\n",
    "    max_eval_examples=20\n",
    ")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Set your S3 bucket and path\n",
    "bucket_name = 'b'\n",
    "s3_prefix = 'medqa-models/reinforce-qwen-run2/'\n",
    "\n",
    "# Save model locally\n",
    "local_model_dir = '/home/ec2-user/SageMaker/qwen_RM_only'\n",
    "os.makedirs(local_model_dir, exist_ok=True)\n",
    "\n",
    "trainer_qwen_rm.model.save_pretrained(local_model_dir)\n",
    "trainer_qwen_rm.tokenizer.save_pretrained(local_model_dir)\n",
    "torch.save(trainer_qwen_rm.baseline_network.state_dict(), os.path.join(local_model_dir, \"baseline_network.pt\"))\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "for root, dirs, files in os.walk(local_model_dir):\n",
    "    for file in files:\n",
    "        local_path = os.path.join(root, file)\n",
    "        relative_path = os.path.relpath(local_path, local_model_dir)\n",
    "        s3_path = os.path.join(s3_prefix, relative_path)\n",
    "        print(f\"Uploading {relative_path} to s3://{bucket_name}/{s3_path}\")\n",
    "        s3.upload_file(local_path, bucket_name, s3_path)\n",
    "print(\"\\nUpload to S3 complete!\")\n",
    "print(f\"All model files are stored at s3://{bucket_name}/{s3_prefix}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b240e51-149e-4336-8965-0f4ea06f3c43",
   "metadata": {},
   "source": [
    "## Qwen2.5 RM +ADV on MMLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128f1a8c-5293-42b2-ad7b-2429b6a27cec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "reward_config = {\n",
    "    'w_b': 1.0,\n",
    "    'w_a': 0.2,\n",
    "    'w_s': 0.2,\n",
    "    'w_fact': 0.2,\n",
    "    'tau_answer': 0.7,\n",
    "    'tau_preamble': 15,\n",
    "    'lambda_s': 1.0\n",
    "}\n",
    "model_url = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "\n",
    "trainer_qwen_rm = PolicyTrainer(\n",
    "    model_path=model_url,\n",
    "    reward_config=reward_config,\n",
    "    use_baseline=True,\n",
    "    umls_api_key=\"UMLS_KEY\"\n",
    ")\n",
    "\n",
    "results = trainer_qwen_rm.train_combined(\n",
    "    train_data_path=\"Datasets/medqa_train_sample.json\",\n",
    "    test_data_path=\"Datasets/mmlu_pro_health_test.json\",\n",
    "    stage1_epochs=2,\n",
    "    stage2_epochs=1,\n",
    "    max_eval_examples=20\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Set your S3 bucket and path\n",
    "bucket_name = 'b'\n",
    "s3_prefix = 'medqa-models/reinforce-qwen-run2/'\n",
    "\n",
    "# Save model locally\n",
    "local_model_dir = '/home/ec2-user/SageMaker/qwen_RM_Adv'\n",
    "os.makedirs(local_model_dir, exist_ok=True)\n",
    "\n",
    "trainer_qwen_rm.model.save_pretrained(local_model_dir)\n",
    "trainer_qwen_rm.tokenizer.save_pretrained(local_model_dir)\n",
    "torch.save(trainer_qwen_rm.baseline_network.state_dict(), os.path.join(local_model_dir, \"baseline_network.pt\"))\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "for root, dirs, files in os.walk(local_model_dir):\n",
    "    for file in files:\n",
    "        local_path = os.path.join(root, file)\n",
    "        relative_path = os.path.relpath(local_path, local_model_dir)\n",
    "        s3_path = os.path.join(s3_prefix, relative_path)\n",
    "        print(f\"Uploading {relative_path} to s3://{bucket_name}/{s3_path}\")\n",
    "        s3.upload_file(local_path, bucket_name, s3_path)\n",
    "print(\"\\nUpload to S3 complete!\")\n",
    "print(f\"All model files are stored at s3://{bucket_name}/{s3_prefix}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
