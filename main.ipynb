{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49b13698-2461-41f3-875d-f416a06b998e",
   "metadata": {},
   "source": [
    "# Importing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0cfad6-6600-409d-841c-ecd9b6e6c8fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#The code was run inside AWS Sagemaker ml.g5.4xlarge instance\n",
    "#%conda install pytorch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 pytorch-cuda=12.1 -c pytorch -c nvidia -y\n",
    "%pip install -q -U bitsandbytes\n",
    "%pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "%pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "%pip install sentence_transformers==4.1.0\n",
    "%pip install git+https://github.com/openai/whisper.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bcfdc6-4881-41bf-8cfe-2beb8de03bcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import gc\n",
    "import boto3\n",
    "import pickle\n",
    "from typing import Dict, List\n",
    "import numpy as np\n",
    "\n",
    "import bitsandbytes.optim as bnb_optim\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from dataclasses import dataclass\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from transformers import (AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig)\n",
    "from huggingface_hub import login\n",
    "sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "hf_token = \"YOR HUGGINGFACE TOKEN\"\n",
    "login(hf_token)\n",
    "\n",
    "pd.options.display.max_seq_items = 2000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cbad78e-60b2-4d4d-ae05-d4ae5e8b4d69",
   "metadata": {},
   "source": [
    "# Multi Component Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faa7e01-1ae7-4cfc-b8c3-bbe3afa4cd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModelManager:\n",
    "    _instance = None\n",
    "    _model = None\n",
    "\n",
    "    def __new__(cls):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super(EmbeddingModelManager, cls).__new__(cls)\n",
    "        return cls._instance\n",
    "\n",
    "    def get_model(self, model_name='all-MiniLM-L6-v2'):\n",
    "        if self._model is None:\n",
    "            print(f\"Loading SentenceTransformer: {model_name}\")\n",
    "            self._model = SentenceTransformer(model_name)\n",
    "            self._model = self._model.cpu()\n",
    "        return self._model\n",
    "\n",
    "    def cleanup(self):\n",
    "        if self._model is not None:\n",
    "            del self._model\n",
    "            self._model = None\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "embedding_manager = EmbeddingModelManager()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d545359-eda3-435b-ba38-13779d6f8907",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardModel:\n",
    "    def __init__(self,\n",
    "                 w_b=1.0,\n",
    "                 w_a=0.2,\n",
    "                 w_s=0.2,\n",
    "                 w_fact=0.2,\n",
    "                 tau_answer=0.7,\n",
    "                 tau_preamble=15,\n",
    "                 lambda_s=1.0,\n",
    "                 violation_threshold_answer=0.1,\n",
    "                 violation_threshold_structural=0.3,\n",
    "                 embedding_model='all-MiniLM-L6-v2',\n",
    "                 factual_agreement_threshold=0.15,\n",
    "                 umls_api_key=None):\n",
    "\n",
    "        self.w_b = w_b\n",
    "        self.w_a = w_a\n",
    "        self.w_s = w_s\n",
    "        self.w_fact = w_fact\n",
    "        self.tau_answer = tau_answer\n",
    "        self.tau_preamble = tau_preamble\n",
    "        self.lambda_s = lambda_s\n",
    "        self.violation_threshold_answer = violation_threshold_answer\n",
    "        self.violation_threshold_structural = violation_threshold_structural\n",
    "        self.sentence_model = embedding_manager.get_model(embedding_model)\n",
    "        #self.sentence_model = self.sentence_model.cpu()\n",
    "\n",
    "        self.answer_leaking_phrases = [\n",
    "            \"the correct answer is\", \"the answer is definitely\", \"the choice is clearly\",\n",
    "            \"option A is the right one\", \"option B is the right one\", \"option C is the right one\",\n",
    "            \"option D is the right one\", \"we can conclude the answer is\", \n",
    "            \"the solution is A\", \"the solution is B\", \"the solution is C\", \"the solution is D\",\n",
    "            \"therefore the answer is\", \"so the correct choice is\", \"the final answer is\",\n",
    "            \"answer: A\", \"answer: B\", \"answer: C\", \"answer: D\"\n",
    "        ]\n",
    "\n",
    "        self.leak_embeddings = self.sentence_model.encode(self.answer_leaking_phrases)\n",
    "        self.fact_verification_system = AtomicFactVerificationSystem(\n",
    "            agreement_threshold=factual_agreement_threshold,\n",
    "            umls_api_key=umls_api_key\n",
    "        )\n",
    "\n",
    "    def extract_answer_choice(self, generation):\n",
    "        \"\"\"Extract the final answer choice from <answer> tags.\"\"\"\n",
    "        answer_match = re.search(r'<answer>(.*?)</answer>', generation, re.IGNORECASE | re.DOTALL)\n",
    "        if not answer_match:\n",
    "            return None\n",
    "        answer_content = answer_match.group(1).strip()\n",
    "        match = re.search(r'\\b([A-D])\\b', answer_content, re.IGNORECASE)\n",
    "        return match.group(1).upper() if match else None\n",
    "\n",
    "    def extract_think_content(self, generation):\n",
    "        think_match = re.search(r'<think>(.*?)</think>', generation, re.DOTALL | re.IGNORECASE)\n",
    "        if think_match and think_match.group(1).strip():\n",
    "            return re.sub(r'^Reasoning:\\s*', '', think_match.group(1).strip())\n",
    "        # Fallback: if no tags, try using the whole response\n",
    "        return generation.strip()\n",
    "\n",
    "    def extract_pre_think_content(self, generation):\n",
    "        \"\"\"Extract content that appears before the <think> tag.\"\"\"\n",
    "        think_start = re.search(r'<think>', generation, re.IGNORECASE)\n",
    "        return generation[:think_start.start()].strip() if think_start else generation.strip()\n",
    "\n",
    "    def validate_format(self, generation) -> bool:\n",
    "        \"\"\"Check if generation follows required format with content.\"\"\"\n",
    "        think_match = re.search(r'<think>\\s*(.*?)\\s*</think>', generation, re.DOTALL | re.IGNORECASE)\n",
    "        has_think_content = think_match and think_match.group(1).strip()\n",
    "        has_answer = bool(re.search(r'<answer>\\s*[A-D]\\s*</answer>', generation, re.IGNORECASE))\n",
    "        return bool(has_think_content) and has_answer\n",
    "\n",
    "    def compute_binary_reward(self, generation, correct_answer):\n",
    "        predicted_answer = self.extract_answer_choice(generation)\n",
    "        if predicted_answer is None:\n",
    "            return 0.0\n",
    "        return 1.0 if predicted_answer == correct_answer.upper() else 0.0\n",
    "\n",
    "    def compute_answer_penalty(self, generation):\n",
    "        think_content = self.extract_think_content(generation)\n",
    "        if not think_content:\n",
    "            return 0.0\n",
    "        think_embedding = self.sentence_model.encode([think_content])\n",
    "        similarities = cosine_similarity(think_embedding, self.leak_embeddings)[0]\n",
    "        max_similarity = np.max(similarities)\n",
    "        return float(max_similarity) if max_similarity > self.tau_answer else 0.0\n",
    "\n",
    "    def compute_structural_penalty(self, generation):\n",
    "        \"\"\"Compute p_structural penalty based on pre-think word count.\"\"\"\n",
    "        pre_think_content = self.extract_pre_think_content(generation)\n",
    "        word_count = len(pre_think_content.split())\n",
    "        return self.lambda_s if word_count > self.tau_preamble else 0.0\n",
    "\n",
    "    def compute_factual_reward(self, generation, context=\"\"):\n",
    "        try:\n",
    "            reasoning_content = self.extract_think_content(generation)\n",
    "\n",
    "            if not reasoning_content:\n",
    "                print(f\"[FactExtractor] No reasoning found in generation:\\n{generation}\\n\")\n",
    "                return {\n",
    "                    \"factual_reward\": 0.0,\n",
    "                    \"factual_analysis\": {},\n",
    "                    \"error\": \"No reasoning content found\"\n",
    "                }\n",
    "\n",
    "            result = self.fact_verification_system.process_response(\n",
    "                reasoning_content, context\n",
    "            )\n",
    "\n",
    "            factual_reward = result.get(\"factual_analysis\", {}).get(\"factual_reward\", 0.0)\n",
    "\n",
    "            return {\n",
    "                \"factual_reward\": factual_reward,\n",
    "                \"factual_analysis\": result.get(\"factual_analysis\", {}),\n",
    "                \"facts\": result.get(\"facts\", []),\n",
    "                \"error\": result.get(\"error\", None)\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"factual_reward\": 0.0,\n",
    "                \"factual_analysis\": {},\n",
    "                \"facts\": [],\n",
    "                \"error\": f\"Factual verification failed: {str(e)}\"\n",
    "            }\n",
    "\n",
    "    def compute_total_reward(self, generation, correct_answer, context=\"\"):\n",
    "        if not self.validate_format(generation):\n",
    "            return {\n",
    "                'r_binary': -1.0,\n",
    "                'p_answer': 0.0,\n",
    "                'p_structural': 0.0,\n",
    "                'r_factual': 0.0,\n",
    "                'factual_analysis': {},\n",
    "                'extracted_facts': [],\n",
    "                'factual_error': \"Invalid format - computation terminated\",\n",
    "                'r_total': -1.0,\n",
    "                'r_normalized': np.tanh(-1.0),\n",
    "                'format_valid': False\n",
    "            }\n",
    "\n",
    "        r_binary = self.compute_binary_reward(generation, correct_answer)\n",
    "        p_answer = self.compute_answer_penalty(generation)\n",
    "        p_structural = self.compute_structural_penalty(generation)\n",
    "\n",
    "        factual_result = self.compute_factual_reward(generation, context)\n",
    "        r_factual = factual_result[\"factual_reward\"]\n",
    "\n",
    "        # Compute total reward\n",
    "        r_total = self.w_b * r_binary - self.w_a * p_answer - self.w_s * p_structural + self.w_fact * r_factual\n",
    "        r_normalized = np.tanh(r_total)\n",
    "\n",
    "        return {\n",
    "            'r_binary': r_binary,\n",
    "            'p_answer': p_answer,\n",
    "            'p_structural': p_structural,\n",
    "            'r_factual': r_factual,\n",
    "            'factual_analysis': factual_result[\"factual_analysis\"],\n",
    "            'extracted_facts': factual_result.get(\"facts\", []),\n",
    "            'factual_error': factual_result.get(\"error\", None),\n",
    "            'r_total': r_total,\n",
    "            'r_normalized': r_normalized,\n",
    "            'format_valid': True\n",
    "        }\n",
    "\n",
    "    def calculate_hacking_rate(self, responses_with_rewards):\n",
    "        total_responses = len(responses_with_rewards)\n",
    "        valid_responses = 0\n",
    "        answer_violations = 0\n",
    "        structural_violations = 0\n",
    "        factual_violations = 0\n",
    "        any_violation = 0\n",
    "\n",
    "        factual_threshold = 0.3\n",
    "\n",
    "        for item in responses_with_rewards:\n",
    "            response = item['response']\n",
    "            reward_info = item.get('reward_info', {})\n",
    "            if not reward_info.get('format_valid', True):\n",
    "                continue\n",
    "            valid_responses += 1\n",
    "\n",
    "            answer_penalty = self.compute_answer_penalty(response)\n",
    "            structural_penalty = self.compute_structural_penalty(response)\n",
    "            factual_reward = reward_info.get('r_factual', 1.0)\n",
    "\n",
    "            answer_violation = answer_penalty > self.violation_threshold_answer\n",
    "            structural_violation = structural_penalty > self.violation_threshold_structural\n",
    "            factual_violation = factual_reward < factual_threshold\n",
    "\n",
    "            if answer_violation:\n",
    "                answer_violations += 1\n",
    "            if structural_violation:\n",
    "                structural_violations += 1\n",
    "            if factual_violation:\n",
    "                factual_violations += 1\n",
    "            if answer_violation or structural_violation or factual_violation:\n",
    "                any_violation += 1\n",
    "\n",
    "        return {\n",
    "            'total_responses': total_responses,\n",
    "            'valid_responses': valid_responses,\n",
    "            'answer_violation_count': answer_violations,\n",
    "            'invalid_format_count': total_responses - valid_responses,\n",
    "            'structural_violation_count': structural_violations,\n",
    "            'factual_violation_count': factual_violations,\n",
    "            'any_violation_count': any_violation,\n",
    "            'answer_violation_rate': answer_violations / valid_responses if valid_responses > 0 else 0.0,\n",
    "            'structural_violation_rate': structural_violations / valid_responses if valid_responses > 0 else 0.0,\n",
    "            'factual_violation_rate': factual_violations / valid_responses if valid_responses > 0 else 0.0,\n",
    "            'overall_violation_rate': any_violation / valid_responses if valid_responses > 0 else 0.0\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63290635-388d-4221-a01a-2295d6f74768",
   "metadata": {},
   "source": [
    "# Data Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dbc551-3c17-4ff8-a30c-be2b32886468",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MedQADataProcessor:\n",
    "    \"\"\"Process MedQA dataset for training.\"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def format_prompt(self, question, options):\n",
    "        formatted_options = \"\"\n",
    "        for letter, option in options.items():\n",
    "            formatted_options += f\"{letter}. {option}\\n\"\n",
    "        input_text = f\"{question}\\n\\n{formatted_options.strip()}\"\n",
    "        prompt = f\"\"\"You are a medical expert taking the USMLE exam. Given the clinical scenario below, respond with your reasoning in a <think></think> tag and your final answer choice (A, B, C, or D) in an <answer></answer> tag.\n",
    "        Scenario:\n",
    "        {input_text}\n",
    "        Format:\n",
    "        <think>your step-by-step clinical reasoning goes here</think>\n",
    "        <answer>A</answer>  # Replace A with your final answer choice\n",
    "        Here is a sample prompt and response:\n",
    "        Prompt/Question: A man is brought into the emergency department by the police department. The officer state that the man has been arrested multiple times for public alcohol intoxication, but recently became homeless. On exam, the man is behaving erratically. His vitals are all within normal limits. He appears confused and has a slurred speech. On gait exam, the patient is ataxic and cannot stand without support for more than a few seconds. Labs return with the following values: Na 140, K 4, Cl 106, BUN 8, Cr 2. His ABG has pH 7.3, PaCO2 13mm, PaO2 130mm, HCO3 7. His urinalysis is shown in Figure 1. Blood salicylate levels return as normal. While you await other diagnostic tests, which of the following should be administered next to treat this patient?       \n",
    "        <think>Consider the symptoms and lab values presented in the scenario. The patient is showing signs of salicylate poisoning, which is consistent with the lab values of metabolic acidosis, elevated anion gap, and hyperventilation leading to respiratory alkalosis. Fomepizole is used to treat methanol and ethylene glycol poisoning, so it is not a suitable choice in this scenario. Salicylate poisoning is a known cause of respiratory alkalosis, so the patient's hyperventilation is consistent with this diagnosis. Ethanol is a common treatment for salicylate poisoning as it is thought to inhibit the enzyme aldehyde dehydrogenase and slow the metabolism of salicylate. Naloxone is an opioid antagonist, which would be used in the case of an opioid overdose, not salicylate poisoning. Naltrexone is an opioid antagonist that is often used for the treatment of opioid addiction, but it is not indicated in this scenario. Fomepizole is a medication used to treat methanol and ethylene glycol poisoning, but it is not indicated in this scenario as the patient's lab values are consistent with salicylate poisoning. Therefore, ethanol is the most appropriate choice to treat this patient's condition.</think>\n",
    "        <answer>A</answer>\n",
    "        Your response:\"\"\"\n",
    "        return prompt\n",
    "\n",
    "    def load_medqa_data(self, file_path: str):\n",
    "        \"\"\"Load and process MedQA dataset from a JSON file.\"\"\"\n",
    "        with open(file_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        processed_data = []\n",
    "        for item in data:\n",
    "            processed_item = {\n",
    "                'question': item['question'],\n",
    "                'options': item['options'],\n",
    "                'correct_answer': item['answer_idx'],\n",
    "                'prompt': self.format_prompt(item['question'], item['options'])\n",
    "            }\n",
    "            processed_data.append(processed_item)\n",
    "        return processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d36d8473-7b7d-4861-a823-69d3de3dec5f",
   "metadata": {},
   "source": [
    "# Baseline Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8ba93b-9aa5-41c3-869f-5642bd59e645",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        # Add input normalization layer\n",
    "        self.input_norm = nn.LayerNorm(input_dim)\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "            nn.LayerNorm(hidden_dim // 2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim // 2, 1)\n",
    "        )\n",
    "\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.xavier_uniform_(module.weight, gain=0.1)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        x = torch.clamp(x, min=-10.0, max=10.0)\n",
    "        x = self.input_norm(x)\n",
    "        output = self.network(x)\n",
    "        output = torch.clamp(output.squeeze(-1), min=-5.0, max=5.0)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18752cc-7086-4d5b-a566-2b78c6cf6ac8",
   "metadata": {},
   "source": [
    "# Policy Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71001eb6-af81-4e36-8182-c2a8beb88865",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyTrainer:\n",
    "    def __init__(self, model_path, reward_config=None, use_baseline=True, umls_api_key=None, log_file=None):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "        print(f\"Loading tokenizer and model from local path: {model_path}\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        if self.tokenizer.eos_token is None:\n",
    "            self.tokenizer.eos_token = self.tokenizer.pad_token or self.tokenizer.unk_token\n",
    "        if self.tokenizer.eos_token_id is None:\n",
    "            self.tokenizer.eos_token_id = self.tokenizer.convert_tokens_to_ids(self.tokenizer.eos_token)\n",
    "\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "        )\n",
    "        dtype = torch.float16 if quantization_config is None else None\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            torch_dtype=dtype,\n",
    "            quantization_config=quantization_config\n",
    "        )\n",
    "        #attn_implementation=attn_impl,\n",
    "        self.metrics_tracker = MetricsTracker()\n",
    "        \n",
    "        self._baseline_cache_path = \"logs/baseline_responses.json\"\n",
    "        self._baseline_model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path,\n",
    "            device_map=\"cpu\",\n",
    "            torch_dtype=None,  # cpu stable\n",
    "            quantization_config=BitsAndBytesConfig(load_in_8bit=True)\n",
    "        )\n",
    "        self._baseline_model.eval()\n",
    "        print(\"Frozen baseline model initialized for epoch comparisons\")\n",
    "\n",
    "        self.use_learnable_reward = True\n",
    "\n",
    "        hidden_dim = self.model.config.hidden_size\n",
    "        print(f\"Model hidden dimension: {hidden_dim}\")\n",
    "\n",
    "        if self.use_learnable_reward:\n",
    "            self.reward_head = nn.Sequential(\n",
    "                nn.Linear(hidden_dim, hidden_dim // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.1),\n",
    "                nn.Linear(hidden_dim // 2, 1)\n",
    "            ).to(self.device).float()\n",
    "\n",
    "            self.reward_optimizer = torch.optim.Adam(self.reward_head.parameters(), lr=1e-4)\n",
    "            print(\"Learnable reward head initialized\")\n",
    "\n",
    "        self.device = next(self.model.parameters()).device\n",
    "        print(f\"Model automatically placed on device: {self.device}\")\n",
    "        if hasattr(self.model, 'gradient_checkpointing_enable'):\n",
    "            self.model.gradient_checkpointing_enable()\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "        reward_config = reward_config or {}\n",
    "        self.data_processor = MedQADataProcessor(self.tokenizer)\n",
    "        reward_config['umls_api_key'] = umls_api_key\n",
    "        self.reward_function = RewardModel(**reward_config)\n",
    "        self.reward_function.fact_verification_system.training_mode = False\n",
    "\n",
    "        self.use_baseline = use_baseline\n",
    "        if self.use_baseline:\n",
    "            self.baseline_network = BaselineNetwork(input_dim=hidden_dim)\n",
    "            self.baseline_network = self.baseline_network.to(self.device)\n",
    "            self.baseline_optimizer = optim.Adam(self.baseline_network.parameters(), lr=1e-4)\n",
    "            print(\"Baseline network initialized\")\n",
    "\n",
    "        self.policy_optimizer = bnb_optim.AdamW8bit(self.model.parameters(), lr=1.41e-5)\n",
    "        self.reward_history = []\n",
    "\n",
    "        adversarial_config = {\n",
    "            'temperature': 1.2,\n",
    "            'max_examples': 50,\n",
    "            'preference_margin': 0.5,\n",
    "            'validation_threshold': 0.7\n",
    "        }\n",
    "        self.adversarial_trainer = AdversarialTrainer(self, adversarial_config)\n",
    "\n",
    "        # Set up logging\n",
    "        self.log_file = log_file or f\"training_log_{time.strftime('%Y%m%d_%H%M%S')}.txt\"\n",
    "\n",
    "        # Clear the log file at start\n",
    "        with open(self.log_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(f\"Training Log Started: {time.strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "            f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "\n",
    "        print(\"PolicyTrainer initialization complete!\")\n",
    "\n",
    "\n",
    "    def _ensure_baseline_responses(self, eval_prompts):\n",
    "        if os.path.exists(self._baseline_cache_path):\n",
    "            with open(self._baseline_cache_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                return json.load(f)\n",
    "\n",
    "        print(\"Generating baseline responses using existing model...\")\n",
    "        self._baseline_model.eval()\n",
    "        baseline_results = []\n",
    "        for prompt in eval_prompts:\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "            inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = self._baseline_model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=100,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.7,\n",
    "                    top_p=0.9,\n",
    "                    pad_token_id=self.tokenizer.pad_token_id,\n",
    "                    eos_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "            base_text = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:],\n",
    "                                              skip_special_tokens=True).strip()\n",
    "            baseline_results.append(base_text)\n",
    "        \n",
    "        \n",
    "        os.makedirs(\"logs\", exist_ok=True)\n",
    "        with open(self._baseline_cache_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(baseline_results, f, indent=2)\n",
    "        return baseline_results\n",
    "\n",
    "\n",
    "    def log_epoch_comparisons(self, eval_prompts, epoch, correct_answers=None, context_list=None):\n",
    "        results = []\n",
    "        if epoch % 5 != 0:\n",
    "            return []\n",
    "        for i, prompt in enumerate(eval_prompts):\n",
    "            context = context_list[i] if context_list else \"\"\n",
    "            correct_answer = correct_answers[i] if correct_answers else None\n",
    "\n",
    "            self.model.eval()\n",
    "            baseline_texts = self._ensure_baseline_responses(eval_prompts)\n",
    "            base_text = baseline_texts[i]\n",
    "\n",
    "            adv_text, _, _, _ = self.generate_response_with_logprobs(prompt)\n",
    "\n",
    "            # Compute rewards for adversarial response\n",
    "            reward_info = self.reward_function.compute_total_reward(\n",
    "                adv_text, correct_answer, context\n",
    "            )\n",
    "\n",
    "            entry = {\n",
    "                \"epoch\": epoch,\n",
    "                \"prompt\": prompt,\n",
    "                \"baseline_response\": base_text,\n",
    "                \"adversarial_response\": adv_text,\n",
    "                \"correct_answer\": correct_answer,\n",
    "                \"reward_info\": reward_info,\n",
    "            }\n",
    "            results.append(entry)\n",
    "\n",
    "            # Console logging\n",
    "            print(\"=\" * 80)\n",
    "            print(f\"[Epoch {epoch}] Prompt: {prompt}\")\n",
    "            print(f\"Correct Answer: {correct_answer}\")\n",
    "            print(\"\\n--- Baseline Response ---\")\n",
    "            print(base_text)\n",
    "            print(\"\\n--- Adversarial Response ---\")\n",
    "            print(adv_text)\n",
    "            print(\"\\nReward breakdown:\")\n",
    "            print(reward_info)\n",
    "\n",
    "        # Save to JSON file for later analysis\n",
    "        os.makedirs(\"logs\", exist_ok=True)\n",
    "        log_path = f\"logs/epoch_{epoch}_comparisons.json\"\n",
    "        with open(log_path, \"w\") as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "        return results\n",
    "\n",
    "    def generate_response_with_logprobs(self, prompt, max_new_tokens = 2048):\n",
    "        \"\"\"Memory-efficient generation with log probabilities and enhanced error handling.\"\"\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        input_length = inputs['input_ids'].shape[1]\n",
    "\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=min(max_new_tokens, 512),\n",
    "                do_sample=True,\n",
    "                temperature=0.2,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True\n",
    "            )\n",
    "\n",
    "        generated_ids = outputs.sequences[0][input_length:]\n",
    "        response_text = self.tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
    "\n",
    "        if len(generated_ids) == 0:\n",
    "            return response_text, torch.tensor([], device=self.device, requires_grad=True), torch.zeros(\n",
    "                self.model.config.hidden_size, device=self.device, dtype=torch.float16, requires_grad=True)\n",
    "\n",
    "        self.model.train()\n",
    "\n",
    "        log_probs = []\n",
    "        # chunk_size = min(128, len(generated_ids))\n",
    "        if len(generated_ids) > 0:\n",
    "            full_context = outputs.sequences[0]\n",
    "            full_inputs = {'input_ids': full_context.unsqueeze(0), \n",
    "                           'attention_mask': torch.ones_like(full_context).unsqueeze(0)}\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                full_outputs = self.model(**full_inputs)\n",
    "                full_logits = full_outputs.logits[0]\n",
    "            \n",
    "            # Extract log probs for generated tokens only\n",
    "            for i, token_id in enumerate(generated_ids):\n",
    "                logit_idx = input_length + i - 1\n",
    "                if 0 <= logit_idx < full_logits.shape[0]:\n",
    "                    logits = full_logits[logit_idx].clamp(min=-50, max=50)\n",
    "                    log_prob = torch.log_softmax(logits, dim=-1)[token_id]\n",
    "                    log_probs.append(log_prob)\n",
    "\n",
    "        # Get hidden states for baseline computation\n",
    "        final_inputs = {\n",
    "            'input_ids': outputs.sequences[0][-50:].unsqueeze(0),\n",
    "            'attention_mask': torch.ones(1, min(50, len(outputs.sequences[0]))).to(self.device)\n",
    "        }\n",
    "        final_outputs = self.model(**final_inputs, output_hidden_states=True)\n",
    "\n",
    "        if final_outputs.hidden_states:\n",
    "            hidden_state = final_outputs.hidden_states[-1][0]\n",
    "\n",
    "            # Enhanced validity checks\n",
    "            if torch.any(torch.isnan(hidden_state)) or torch.any(torch.isinf(hidden_state)):\n",
    "                print(\"Warning: Invalid hidden states detected\")\n",
    "                avg_hidden_state = torch.zeros(self.model.config.hidden_size,\n",
    "                                               device=self.device,\n",
    "                                               dtype=torch.float16,\n",
    "                                               requires_grad=True)\n",
    "            else:\n",
    "                hidden_state_clamped = torch.clamp(hidden_state, min=-10, max=10)\n",
    "                avg_hidden_state = hidden_state_clamped.mean(dim=0)\n",
    "                avg_hidden_state.requires_grad_(True)\n",
    "        else:\n",
    "            avg_hidden_state = torch.zeros(self.model.config.hidden_size,\n",
    "                                           device=self.device,\n",
    "                                           dtype=torch.float16,\n",
    "                                           requires_grad=True)\n",
    "\n",
    "        log_probs_tensor = torch.stack(log_probs) if log_probs else torch.tensor([], device=self.device,\n",
    "                                                                                 requires_grad=True,\n",
    "                                                                                 dtype=torch.float16)\n",
    "        return response_text, log_probs_tensor, avg_hidden_state\n",
    "\n",
    "    def generate_and_evaluate_with_facts(self, prompt, correct_answer, context=\"\"):\n",
    "        response_text, log_probs, hidden_state = self.generate_response_with_logprobs(prompt)\n",
    "\n",
    "        # Get rule-based reward (your existing system)\n",
    "        rule_reward_info = self.reward_function.compute_total_reward(\n",
    "            response_text, correct_answer, context\n",
    "        )\n",
    "\n",
    "        # Add learnable reward component\n",
    "        if hasattr(self, 'use_learnable_reward') and self.use_learnable_reward:\n",
    "            with torch.no_grad():\n",
    "                learnable_reward = self.reward_head(hidden_state.detach().float()).item()\n",
    "\n",
    "            # Combine rule-based and learnable rewards\n",
    "            combined_reward = 0.7 * rule_reward_info['r_normalized'] + 0.3 * learnable_reward\n",
    "\n",
    "            # Add to reward info\n",
    "            reward_info = rule_reward_info.copy()\n",
    "            reward_info['r_learnable'] = learnable_reward\n",
    "            reward_info['r_combined'] = combined_reward\n",
    "            reward_info['r_normalized'] = combined_reward  # Use combined as main reward\n",
    "        else:\n",
    "            reward_info = rule_reward_info\n",
    "\n",
    "        return response_text, log_probs, hidden_state, reward_info\n",
    "\n",
    "    def compute_baseline_value(self, hidden_state, training_mode=False):\n",
    "        \"\"\"Compute baseline value with enhanced error handling.\"\"\"\n",
    "        if not self.use_baseline or hidden_state is None:\n",
    "            return torch.tensor(0.0, device=self.device, requires_grad=training_mode, dtype=torch.float16)\n",
    "\n",
    "        # Enhanced validity checks\n",
    "        if torch.any(torch.isnan(hidden_state)) or torch.any(torch.isinf(hidden_state)):\n",
    "            print(\"Warning: Invalid hidden state input to baseline, using fallback\")\n",
    "            return torch.tensor(0.0, device=self.device, requires_grad=training_mode, dtype=torch.float16)\n",
    "\n",
    "        # Clamp input to prevent extreme values\n",
    "        hidden_state_input = torch.clamp(hidden_state, min=-10.0, max=10.0)\n",
    "\n",
    "        try:\n",
    "            baseline_value = self.baseline_network(hidden_state_input)\n",
    "\n",
    "            # Check for invalid baseline output\n",
    "            if torch.isnan(baseline_value) or torch.isinf(baseline_value):\n",
    "                print(\"Warning: Baseline network produced invalid output, using fallback\")\n",
    "                baseline_value = torch.tensor(0.0, device=self.device, requires_grad=training_mode, dtype=torch.float16)\n",
    "            else:\n",
    "                baseline_value = torch.clamp(baseline_value, min=-5.0, max=5.0)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error in baseline network: {e}, using fallback\")\n",
    "            baseline_value = torch.tensor(0.0, device=self.device, requires_grad=training_mode, dtype=torch.float16)\n",
    "\n",
    "        if training_mode:\n",
    "            return baseline_value\n",
    "        else:\n",
    "            return baseline_value.item()\n",
    "\n",
    "    def _generate_conservative_response(self, prompt, correct_answer):\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=100,\n",
    "                do_sample=True,\n",
    "                temperature=0.5,\n",
    "                top_p=0.8,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        response = self.tokenizer.decode(\n",
    "            outputs[0][inputs['input_ids'].shape[1]:],\n",
    "            skip_special_tokens=True\n",
    "        ).strip()\n",
    "\n",
    "        return response\n",
    "        \n",
    "    def _generate_high_quality_response(self, prompt, correct_answer):\n",
    "        \"\"\"Generate a high-quality response that should score well\"\"\"\n",
    "\n",
    "        # Use conservative generation parameters for high quality\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                do_sample=True,\n",
    "                temperature=0.6,  # Lower temperature for more focused responses\n",
    "                top_p=0.9,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        response = self.tokenizer.decode(\n",
    "            outputs[0][inputs['input_ids'].shape[1]:],\n",
    "            skip_special_tokens=True\n",
    "        ).strip()\n",
    "\n",
    "        # Ensure proper format\n",
    "        if '<think>' not in response:\n",
    "            response = f\"<think>Let me analyze this medical scenario systematically. {response}</think>\"\n",
    "        if '<answer>' not in response:\n",
    "            response += f\"<answer>{correct_answer}</answer>\"\n",
    "\n",
    "        return response\n",
    "\n",
    "    def _analyze_response_quality(self, response: str, reward_info: Dict, correct_answer: str) -> Dict:\n",
    "        \"\"\"Analyze response against your 5 quality goals\"\"\"\n",
    "\n",
    "        violations = {\n",
    "            'correctness_violations': 0,\n",
    "            'answer_leaking_violations': 0,\n",
    "            'format_violations': 0,\n",
    "            'factual_violations': 0,\n",
    "            'bad_ood_high_rewards': 0\n",
    "        }\n",
    "\n",
    "        # Goal 1: Improved Correctness\n",
    "        if reward_info.get('r_binary', 0) <= 0:\n",
    "            violations['correctness_violations'] = 1\n",
    "\n",
    "        # Goal 2: Less Answer Leaking\n",
    "        if reward_info.get('p_answer', 0) > 0.1:\n",
    "            violations['answer_leaking_violations'] = 1\n",
    "\n",
    "        # Goal 3: Less Format Violations\n",
    "        if reward_info.get('p_structural', 0) > 0.1 or not self.reward_function.validate_format(response):\n",
    "            violations['format_violations'] = 1\n",
    "\n",
    "        # Goal 4: Less Factual Errors\n",
    "        if reward_info.get('r_factual', 1.0) < 0.3:\n",
    "            violations['factual_violations'] = 1\n",
    "\n",
    "        # Goal 5: Less Rewards to Bad OOD Reasoning\n",
    "        # High total reward despite violations = bad OOD getting undeserved high reward\n",
    "        has_violations = sum(violations.values()) > 0\n",
    "        high_reward = reward_info.get('r_total', 0) > 0.5\n",
    "        if has_violations and high_reward:\n",
    "            violations['bad_ood_high_rewards'] = 1\n",
    "\n",
    "        return violations\n",
    "\n",
    "    def evaluate_model(self, test_data_path, max_examples):\n",
    "        test_dataset = self.data_processor.load_medqa_data(test_data_path)\n",
    "        self.reward_function.fact_verification_system.training_mode = False\n",
    "    \n",
    "        if max_examples is not None:\n",
    "            test_dataset = test_dataset[:max_examples]\n",
    "    \n",
    "        print(f\"Evaluating on {max_examples} test examples...\")\n",
    "    \n",
    "        self.model.eval()\n",
    "        if self.use_baseline:\n",
    "            self.baseline_network.eval()\n",
    "    \n",
    "        correct_predictions = 0\n",
    "        total_examples = len(test_dataset)\n",
    "    \n",
    "        # Track individual violations for standard deviation calculation\n",
    "        violation_records = {\n",
    "            'correctness_violations': [],\n",
    "            'answer_leaking_violations': [],\n",
    "            'format_violations': [],\n",
    "            'factual_violations': [],\n",
    "            'bad_ood_high_rewards': []\n",
    "        }\n",
    "    \n",
    "        # Enhanced reward tracking including factual scores\n",
    "        reward_components = {'r_binary': [], 'p_answer': [], 'p_structural': [], 'r_factual': []}\n",
    "        all_rewards = []\n",
    "        format_violations = 0\n",
    "        evaluation_data = []\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for i, test_item in enumerate(test_dataset):\n",
    "                prompt = test_item['prompt']\n",
    "                correct_answer = test_item['correct_answer']\n",
    "    \n",
    "                response = self.generate_response(prompt)\n",
    "    \n",
    "                # Enhanced reward computation with factual verification\n",
    "                reward_info = self.reward_function.compute_total_reward(response, correct_answer)\n",
    "    \n",
    "                evaluation_data.append({\n",
    "                    'response': response,\n",
    "                    'reward_info': reward_info,\n",
    "                    'correct_answer': correct_answer\n",
    "                })\n",
    "    \n",
    "                # Track reward components\n",
    "                for key in reward_components:\n",
    "                    reward_components[key].append(reward_info[key])\n",
    "                \n",
    "                all_rewards.append(reward_info['r_normalized'])\n",
    "    \n",
    "                predicted_answer = self.reward_function.extract_answer_choice(response)\n",
    "                is_correct = predicted_answer and predicted_answer.upper() == correct_answer.upper()\n",
    "                if is_correct:\n",
    "                    correct_predictions += 1\n",
    "    \n",
    "                format_ok = self.reward_function.validate_format(response)\n",
    "                format_violations += int(not format_ok)\n",
    "    \n",
    "                # Track individual violations for std calculation\n",
    "                violations = self._analyze_response_quality(response, reward_info, correct_answer)\n",
    "                for key in violation_records.keys():\n",
    "                    violation_records[key].append(violations[key])\n",
    "    \n",
    "                if i % 100 == 0:\n",
    "                    print(f\"Evaluated {i + 1}/{total_examples} examples...\")\n",
    "    \n",
    "        format_violation_rate = format_violations / total_examples\n",
    "        accuracy = correct_predictions / total_examples\n",
    "        avg_rewards = {key: np.mean(values) for key, values in reward_components.items()}\n",
    "        hacking_stats = self.reward_function.calculate_hacking_rate(evaluation_data)\n",
    "    \n",
    "        # Calculate standard deviations\n",
    "        reward_std = np.std(all_rewards) if len(all_rewards) > 1 else 0.0\n",
    "    \n",
    "        # Map to the same keys MetricsTracker expects with std deviations\n",
    "        test_metrics = {\n",
    "            \"correctness_violations_rate\": 1 - accuracy,\n",
    "            \"answer_leaking_violations_rate\": hacking_stats[\"answer_violation_rate\"],\n",
    "            \"format_violations_rate\": format_violation_rate,\n",
    "            \"factual_violations_rate\": hacking_stats[\"factual_violation_rate\"],\n",
    "            \"bad_ood_high_rewards_rate\": hacking_stats[\"overall_violation_rate\"],\n",
    "            \"avg_reward\": np.mean(all_rewards),\n",
    "            \"avg_factual\": avg_rewards[\"r_factual\"],\n",
    "            \"accuracy\": accuracy,\n",
    "            \"correct_predictions\": correct_predictions,\n",
    "            \"total_examples\": total_examples,\n",
    "            \"reward_history\": self.reward_history,\n",
    "            \n",
    "            # Add standard deviations\n",
    "            \"std_reward\": reward_std,\n",
    "            \"std_factual\": np.std(reward_components['r_factual']) if len(reward_components['r_factual']) > 1 else 0.0\n",
    "        }\n",
    "    \n",
    "        # Add violation standard deviations\n",
    "        for key in violation_records.keys():\n",
    "            rate_key = f\"{key}_rate\"\n",
    "            std_key = f\"{key}_std\"\n",
    "            \n",
    "            # Calculate violation rate and std\n",
    "            test_metrics[rate_key] = test_metrics.get(rate_key, np.mean(violation_records[key]))\n",
    "            test_metrics[std_key] = np.std(violation_records[key]) if len(violation_records[key]) > 1 else 0.0\n",
    "    \n",
    "        # Use MetricsTracker instead of manual printing\n",
    "        self.metrics_tracker.print_metrics_with_std(test_metrics, \"Test Evaluation Results\")\n",
    "    \n",
    "        return test_metrics\n",
    "\n",
    "    def generate_response(self, prompt: str, max_new_tokens = 2048) -> str:\n",
    "        self.model.eval()\n",
    "\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=400)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "\n",
    "        response = self.tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)\n",
    "        return response.strip()\n",
    "\n",
    "    def analyze_hacking_sensitivity(self, test_data_path, max_examples, tau_answer_range=None, tau_preamble_range=None):\n",
    "        if tau_answer_range is None:\n",
    "            tau_answer_range = [0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5]\n",
    "        if tau_preamble_range is None:\n",
    "            tau_preamble_range = [5, 10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "\n",
    "        test_dataset = self.data_processor.load_medqa_data(test_data_path)\n",
    "        if max_examples is not None:\n",
    "            test_dataset = test_dataset[:max_examples]\n",
    "\n",
    "        print(f\"Running sensitivity analysis on {len(test_dataset)} examples...\")\n",
    "        print(f\"Testing tau_answer: {tau_answer_range}\")\n",
    "        print(f\"Testing tau_preamble: {tau_preamble_range}\")\n",
    "\n",
    "        print(\"Generating responses...\")\n",
    "        self.model.eval()\n",
    "        if self.use_baseline:\n",
    "            self.baseline_network.eval()\n",
    "\n",
    "        evaluation_data = []\n",
    "        with torch.no_grad():\n",
    "            for i, test_item in enumerate(test_dataset):\n",
    "                prompt = test_item['prompt']\n",
    "                correct_answer = test_item['correct_answer']\n",
    "                response = self.generate_response(prompt)\n",
    "\n",
    "                evaluation_data.append({\n",
    "                    'response': response,\n",
    "                    'correct_answer': correct_answer,\n",
    "                    'prompt': prompt\n",
    "                })\n",
    "\n",
    "        # Store original thresholds\n",
    "        original_tau_answer = self.reward_function.tau_answer\n",
    "        original_tau_preamble = self.reward_function.tau_preamble\n",
    "\n",
    "        sensitivity_results = []\n",
    "\n",
    "        print(\"\\nTesting threshold combinations...\")\n",
    "        for tau_answer in tau_answer_range:\n",
    "            for tau_structural in tau_preamble_range:\n",
    "                print(f\"Testing tau_answer={tau_answer}, tau_preamble={tau_structural}\")\n",
    "\n",
    "                self.reward_function.tau_answer = tau_answer\n",
    "                self.reward_function.tau_preamble = tau_structural\n",
    "\n",
    "                responses_with_rewards = []\n",
    "                for item in evaluation_data:\n",
    "                    # Enhanced reward computation with factual verification\n",
    "                    reward_info = self.reward_function.compute_total_reward(\n",
    "                        item['response'], item['correct_answer']\n",
    "                    )\n",
    "                    responses_with_rewards.append({\n",
    "                        'response': item['response'],\n",
    "                        'reward_info': reward_info,\n",
    "                        'correct_answer': item['correct_answer']\n",
    "                    })\n",
    "\n",
    "                hacking_stats = self.reward_function.calculate_hacking_rate(responses_with_rewards)\n",
    "\n",
    "                positive_rewards = sum(1 for item in responses_with_rewards\n",
    "                                       if item['reward_info']['r_total'] > 0)\n",
    "                avg_answer_penalty = np.mean([item['reward_info']['p_answer']\n",
    "                                              for item in responses_with_rewards])\n",
    "                avg_structural_penalty = np.mean([item['reward_info']['p_structural']\n",
    "                                                  for item in responses_with_rewards])\n",
    "                avg_factual_reward = np.mean([item['reward_info']['r_factual']\n",
    "                                              for item in responses_with_rewards])\n",
    "\n",
    "                sensitivity_results.append({\n",
    "                    'tau_answer': tau_answer,\n",
    "                    'tau_preamble': tau_structural,\n",
    "                    'answer_violation_rate': hacking_stats['answer_violation_rate'],\n",
    "                    'structural_violation_rate': hacking_stats['structural_violation_rate'],\n",
    "                    'factual_violation_rate': hacking_stats['factual_violation_rate'],\n",
    "                    'overall_violation_rate': hacking_stats['overall_violation_rate'],\n",
    "                    'answer_violation_count': hacking_stats['answer_violation_count'],\n",
    "                    'structural_violation_count': hacking_stats['structural_violation_count'],\n",
    "                    'factual_violation_count': hacking_stats['factual_violation_count'],\n",
    "                    'positive_reward_count': positive_rewards,\n",
    "                    'positive_reward_rate': positive_rewards / len(responses_with_rewards),\n",
    "                    'avg_answer_penalty': avg_answer_penalty,\n",
    "                    'avg_structural_penalty': avg_structural_penalty,\n",
    "                    'avg_factual_reward': avg_factual_reward\n",
    "                })\n",
    "\n",
    "        # Restore original thresholds\n",
    "        self.reward_function.tau_answer = original_tau_answer\n",
    "        self.reward_function.tau_preamble = original_tau_preamble\n",
    "\n",
    "        return {\n",
    "            'sensitivity_results': sensitivity_results,\n",
    "            'tau_answer_range': tau_answer_range,\n",
    "            'tau_preamble_range': tau_preamble_range,\n",
    "            'total_examples': len(test_dataset)\n",
    "\n",
    "\n",
    "    def _train_reward_model_stage1(self, train_data_path, num_epochs, batch_size):\n",
    "        \"\"\"Stage 1: Train only the learnable reward components\"\"\"\n",
    "        if not hasattr(self, 'reward_head'):\n",
    "            print(\"No learnable reward head found - skipping reward model training\")\n",
    "            return\n",
    "        \n",
    "        train_dataset = self.data_processor.load_medqa_data(train_data_path)\n",
    "        print(f\"Stage 1: Training reward model on {len(train_dataset)} examples\")\n",
    "        \n",
    "        # Freeze policy model\n",
    "        self.model.eval()\n",
    "        self.reward_head.train()\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            total_loss = 0\n",
    "            num_batches = 0\n",
    "            \n",
    "            for batch_start in range(0, len(train_dataset), batch_size):\n",
    "                batch_end = min(batch_start + batch_size, len(train_dataset))\n",
    "                batch_items = train_dataset[batch_start:batch_end]\n",
    "                \n",
    "                batch_losses = []\n",
    "                \n",
    "                for item in batch_items:\n",
    "                    try:\n",
    "                        prompt = item['prompt']\n",
    "                        correct_answer = item['correct_answer']\n",
    "                        \n",
    "                        # Generate response (no gradients for policy)\n",
    "                        with torch.no_grad():\n",
    "                            response_text, _, hidden_state = self.generate_response_with_logprobs(prompt)\n",
    "                        \n",
    "                        # Compute target score using rule-based reward\n",
    "                        rule_reward_info = self.reward_function.compute_total_reward(\n",
    "                            response_text, correct_answer\n",
    "                        )\n",
    "                        target_score = rule_reward_info['r_normalized']\n",
    "                        \n",
    "                        # Predict score using learnable reward head\n",
    "                        predicted_score = self.reward_head(hidden_state.detach().float())\n",
    "                        if predicted_score.dim() > 0:\n",
    "                            predicted_score = predicted_score.squeeze()\n",
    "                        \n",
    "                        # Compute loss\n",
    "                        target_tensor = torch.tensor(target_score, device=self.device, dtype=torch.float32)\n",
    "                        loss = torch.nn.functional.mse_loss(predicted_score, target_tensor)\n",
    "                        batch_losses.append(loss)\n",
    "                        \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error in reward model training: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                if batch_losses:\n",
    "                    total_batch_loss = torch.stack(batch_losses).mean()\n",
    "                    total_batch_loss.backward()\n",
    "                    total_loss += total_batch_loss.item()\n",
    "                    num_batches += 1\n",
    "                \n",
    "                # Update reward model every batch\n",
    "                torch.nn.utils.clip_grad_norm_(self.reward_head.parameters(), 1.0)\n",
    "                self.reward_optimizer.step()\n",
    "                self.reward_optimizer.zero_grad()\n",
    "            \n",
    "            avg_loss = total_loss / num_batches if num_batches > 0 else 0\n",
    "            print(f\"Reward model epoch {epoch + 1}/{num_epochs}, Loss: {avg_loss:.2f}\")\n",
    "        \n",
    "        print(\"Stage 1 complete: Reward model training finished\")\n",
    "\n",
    "    def _train_policy_stage2(self, train_data_path, num_epochs, batch_size):\n",
    "        train_dataset = self.data_processor.load_medqa_data(train_data_path)\n",
    "        print(f\"Stage 2: Training policy on {len(train_dataset)} examples\")\n",
    "        \n",
    "        if hasattr(self, 'reward_head'):\n",
    "            self.reward_head.eval()\n",
    "        self.model.train()\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_rewards = []\n",
    "            epoch_advantages = []\n",
    "            epoch_policy_losses = []\n",
    "            epoch_baseline_values = []\n",
    "            \n",
    "            violation_records = {\n",
    "                'correctness_violations': [],\n",
    "                'answer_leaking_violations': [],\n",
    "                'format_violations': [],\n",
    "                'factual_violations': [],\n",
    "                'bad_ood_high_rewards': []\n",
    "            }\n",
    "            \n",
    "            epoch_metrics = {\n",
    "                'correctness_violations': 0,\n",
    "                'answer_leaking_violations': 0,\n",
    "                'format_violations': 0,\n",
    "                'factual_violations': 0,\n",
    "                'bad_ood_high_rewards': 0,\n",
    "                'total_examples': 0\n",
    "            }\n",
    "            \n",
    "            self.policy_optimizer.zero_grad()\n",
    "            if self.use_baseline:\n",
    "                self.baseline_optimizer.zero_grad()\n",
    "            \n",
    "            for batch_start in range(0, len(train_dataset), batch_size):\n",
    "                batch_end = min(batch_start + batch_size, len(train_dataset))\n",
    "                batch_items = train_dataset[batch_start:batch_end]\n",
    "                \n",
    "                batch_policy_losses = []\n",
    "                batch_baseline_losses = []\n",
    "                batch_rewards = []\n",
    "                batch_advantages = []\n",
    "                \n",
    "                for item in batch_items:\n",
    "                    try:\n",
    "                        prompt = item['prompt']\n",
    "                        correct_answer = item['correct_answer']\n",
    "                        \n",
    "                        response_text, log_probs, hidden_state, reward_info = self.generate_and_evaluate_with_facts(\n",
    "                            prompt, correct_answer\n",
    "                        )\n",
    "                        \n",
    "                        # Track violations for this individual example\n",
    "                        violations = self._analyze_response_quality(response_text, reward_info, correct_answer)\n",
    "                        \n",
    "                        # Record individual violations (0 or 1) for std calculation\n",
    "                        for key in violation_records.keys():\n",
    "                            violation_records[key].append(violations[key])\n",
    "                        \n",
    "                        # Aggregate totals for rates\n",
    "                        for key, value in violations.items():\n",
    "                            epoch_metrics[key] = epoch_metrics.get(key, 0) + value\n",
    "                        epoch_metrics[\"total_examples\"] = epoch_metrics.get(\"total_examples\", 0) + 1\n",
    "                        \n",
    "                        if log_probs.numel() == 0:\n",
    "                            continue\n",
    "                        \n",
    "                        # Get reward from frozen reward model\n",
    "                        with torch.no_grad():\n",
    "                            if hasattr(self, 'reward_head'):\n",
    "                                reward = self.reward_head(hidden_state.detach().float()).item()\n",
    "                            else:\n",
    "                                reward_info = self.reward_function.compute_total_reward(\n",
    "                                    response_text, correct_answer\n",
    "                                )\n",
    "                                reward = reward_info['r_normalized']\n",
    "                        \n",
    "                        # Baseline and advantage\n",
    "                        baseline_value = self.compute_baseline_value(hidden_state, training_mode=True)\n",
    "                        reward_tensor = torch.tensor(reward, device=self.device, dtype=torch.float16)\n",
    "                        advantage = reward_tensor - baseline_value\n",
    "                        \n",
    "                        batch_rewards.append(reward)\n",
    "                        batch_advantages.append(advantage.detach().item())\n",
    "                        \n",
    "                        policy_loss = -torch.sum(log_probs) * advantage\n",
    "                        batch_policy_losses.append(policy_loss)\n",
    "                        \n",
    "                        # Collect metrics for std calculation\n",
    "                        epoch_rewards.append(reward)\n",
    "                        epoch_advantages.append(advantage.detach().item())\n",
    "                        epoch_policy_losses.append(policy_loss.detach().item())\n",
    "                        if hasattr(baseline_value, 'item'):\n",
    "                            epoch_baseline_values.append(baseline_value.item())\n",
    "                        else:\n",
    "                            epoch_baseline_values.append(float(baseline_value))\n",
    "                        \n",
    "                        if self.use_baseline:\n",
    "                            baseline_loss = ((baseline_value - reward_tensor) ** 2)\n",
    "                            batch_baseline_losses.append(baseline_loss)\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error in policy training: {e}\")\n",
    "                        continue\n",
    "                \n",
    "                # Update policy\n",
    "                if batch_policy_losses:\n",
    "                    total_policy_loss = torch.stack(batch_policy_losses).mean()\n",
    "                    total_loss = total_policy_loss\n",
    "                    \n",
    "                    if self.use_baseline and batch_baseline_losses:\n",
    "                        total_baseline_loss = torch.stack(batch_baseline_losses).mean()\n",
    "                        total_loss += total_baseline_loss\n",
    "                    \n",
    "                    total_loss.backward()\n",
    "            \n",
    "            # Update weights\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            if self.use_baseline:\n",
    "                torch.nn.utils.clip_grad_norm_(self.baseline_network.parameters(), max_norm=1.0)\n",
    "            \n",
    "            self.policy_optimizer.step()\n",
    "            self.policy_optimizer.zero_grad()\n",
    "            if self.use_baseline:\n",
    "                self.baseline_optimizer.step()\n",
    "                self.baseline_optimizer.zero_grad()\n",
    "            \n",
    "            # Calculate means and standard deviations for continuous metrics\n",
    "            avg_reward = np.mean(epoch_rewards) if epoch_rewards else 0\n",
    "            std_reward = np.std(epoch_rewards) if len(epoch_rewards) > 1 else 0\n",
    "            \n",
    "            avg_advantage = np.mean(epoch_advantages) if epoch_advantages else 0\n",
    "            std_advantage = np.std(epoch_advantages) if len(epoch_advantages) > 1 else 0\n",
    "            \n",
    "            avg_policy_loss = np.mean(epoch_policy_losses) if epoch_policy_losses else 0\n",
    "            std_policy_loss = np.std(epoch_policy_losses) if len(epoch_policy_losses) > 1 else 0\n",
    "            \n",
    "            avg_baseline_value = np.mean(epoch_baseline_values) if epoch_baseline_values else 0\n",
    "            std_baseline_value = np.std(epoch_baseline_values) if len(epoch_baseline_values) > 1 else 0\n",
    "            \n",
    "            print(f\"Policy epoch {epoch + 1}/{num_epochs}\")\n",
    "            print(f\"Avg Reward: {avg_reward:.2f} (±{std_reward:.2f})\\n\")\n",
    "            # print(f\"  Avg Advantage: {avg_advantage:.2f} (±{std_advantage:.2f})\")\n",
    "            # print(f\"  Avg Policy Loss: {avg_policy_loss:.2f} (±{std_policy_loss:.2f})\")\n",
    "            if self.use_baseline:\n",
    "                print(f\"  Avg Baseline Value: {avg_baseline_value:.2f} (±{std_baseline_value:.2f})\")\n",
    "            \n",
    "            if epoch_rewards and epoch_metrics[\"total_examples\"] > 0:\n",
    "                # Calculate violation rates and their standard deviations\n",
    "                for key in [\"correctness_violations\", \"answer_leaking_violations\",\n",
    "                            \"format_violations\", \"factual_violations\", \"bad_ood_high_rewards\"]:\n",
    "                    rate_key = f\"{key}_rate\"\n",
    "                    std_key = f\"{key}_std\"\n",
    "                    \n",
    "                    # Rate is the mean of 0s and 1s\n",
    "                    epoch_metrics[rate_key] = epoch_metrics[key] / epoch_metrics[\"total_examples\"]\n",
    "                    \n",
    "                    # Standard deviation of binary values (0s and 1s)\n",
    "                    if len(violation_records[key]) > 1:\n",
    "                        epoch_metrics[std_key] = np.std(violation_records[key])\n",
    "                    else:\n",
    "                        epoch_metrics[std_key] = 0.0\n",
    "                \n",
    "                # Store other metrics\n",
    "                epoch_metrics[\"avg_reward\"] = avg_reward\n",
    "                epoch_metrics[\"std_reward\"] = std_reward\n",
    "                epoch_metrics[\"avg_advantage\"] = avg_advantage \n",
    "                epoch_metrics[\"std_advantage\"] = std_advantage\n",
    "                epoch_metrics[\"avg_policy_loss\"] = avg_policy_loss\n",
    "                epoch_metrics[\"std_policy_loss\"] = std_policy_loss\n",
    "                epoch_metrics[\"avg_baseline_value\"] = avg_baseline_value\n",
    "                epoch_metrics[\"std_baseline_value\"] = std_baseline_value\n",
    "\n",
    "                self.metrics_tracker.print_metrics_with_std(epoch_metrics, f\"POLICY EPOCH {epoch + 1}\")\n",
    "        \n",
    "        print(\"Stage 2 complete: Policy training finished\")\n",
    "    \n",
    "    def _train_adversarial_stage3(self, train_data_path, num_cycles=3):\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"STAGE 3: ADVERSARIAL ROBUSTNESS TRAINING\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        train_dataset = self.data_processor.load_medqa_data(train_data_path)\n",
    "        prompts = [item['prompt'] for item in train_dataset[:100]]\n",
    "        answers = [item['correct_answer'] for item in train_dataset[:100]]\n",
    "        \n",
    "        print(\"Evaluating model BEFORE adversarial training...\")\n",
    "        pre_adversarial_metrics = self.evaluate_model(train_data_path, 50)\n",
    "        \n",
    "        print(\"Pre-Adversarial Performance:\")\n",
    "        self.metrics_tracker.print_metrics_with_std(pre_adversarial_metrics, \"PRE-ADVERSARIAL METRICS\")\n",
    "        \n",
    "        # Run adversarial training cycles\n",
    "        results = self.adversarial_trainer.run_adversarial_training_cycle(\n",
    "            prompts, answers, num_cycles\n",
    "        )\n",
    "        \n",
    "        print(\"Adversarial training complete:\")\n",
    "        print(f\"  Final robustness score: {results['final_robustness']:.2f}\")\n",
    "        print(f\"  Total cycles completed: {results['total_cycles']}\")\n",
    "        \n",
    "        print(\"\\nEvaluating model AFTER adversarial training...\")\n",
    "        post_adversarial_metrics = self.evaluate_model(train_data_path, 50)\n",
    "        \n",
    "        print(\"Post-Adversarial Performance:\")\n",
    "        self.metrics_tracker.print_metrics_with_std(post_adversarial_metrics, \"POST-ADVERSARIAL METRICS\")\n",
    "        \n",
    "        # Store both sets of metrics and print comprehensive analysis\n",
    "        self.metrics_tracker.add_adversarial_stage_metrics(pre_adversarial_metrics, post_adversarial_metrics)\n",
    "        self.metrics_tracker.print_adversarial_impact_analysis()\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "    def train_reward_policy(self, train_data_path, test_data_path, stage1_epochs, stage2_epochs, batch_size=8, max_eval_examples=100):\n",
    "        print(\"TRAINING: Reward and Policy\")\n",
    "        print(\"=\"*40)\n",
    "        \n",
    "        self._train_reward_model_stage1(train_data_path, stage1_epochs, batch_size)\n",
    "        self._train_policy_stage2(train_data_path, stage2_epochs, batch_size)\n",
    "        \n",
    "        print(\"\\n Evaluating Reward and Policy Training Results:\")\n",
    "        stage1_metrics = self.evaluate_model(test_data_path, max_eval_examples)\n",
    "        self.metrics_tracker.print_metrics_with_std(stage1_metrics, \"STAGE 1 RESULTS\")\n",
    "        \n",
    "        return stage1_metrics\n",
    "    \n",
    "    def train_adversarial(self, train_data_path, test_data_path, stage1_epochs, stage2_epochs, batch_size=8, max_eval_examples=100):\n",
    "        \"\"\"Train all stages including adversarial and evaluate\"\"\"\n",
    "        print(\"TRAINING: All Stages (Including Adversarial)\")\n",
    "        print(\"=\"*50)\n",
    "        \n",
    "        self._train_reward_model_stage1(train_data_path, stage1_epochs, batch_size)\n",
    "        self._train_policy_stage2(train_data_path, stage2_epochs, batch_size)\n",
    "        self._train_adversarial_stage3(train_data_path, num_cycles=3)\n",
    "        \n",
    "        print(\"\\nEVALUATING FINAL RESULTS:\")\n",
    "        final_metrics = self.evaluate_model(test_data_path, max_eval_examples)\n",
    "        self.metrics_tracker.print_metrics_with_std(final_metrics, \"STAGE 1 RESULTS\")\n",
    "        \n",
    "        return final_metrics\n",
    "    \n",
    "    def compare_training_approaches(self, train_data_path, test_data_path, stage1_epochs, stage2_epochs, batch_size=8, max_eval_examples=100):\n",
    "        \"\"\"Compare both training approaches\"\"\"\n",
    "        \n",
    "        # Train and evaluate stage 1 only\n",
    "        stage1_metrics = self.train_reward_policy(train_data_path, test_data_path, stage1_epochs, stage2_epochs, batch_size, max_eval_examples)\n",
    "                \n",
    "        # Train and evaluate with adversarial\n",
    "        final_metrics = self.train_adversarial(train_data_path, test_data_path, stage1_epochs, stage2_epochs, batch_size, max_eval_examples)\n",
    "        \n",
    "        # Compare results\n",
    "        self._compare_training_stages(stage1_metrics, final_metrics)\n",
    "        \n",
    "        return {\n",
    "            'stage1_only': stage1_metrics,\n",
    "            'with_adversarial': final_metrics\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fa3399-d74b-4944-a4f6-4515127969d1",
   "metadata": {},
   "source": [
    "# Adversarial Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd990912-c7f4-4ef7-b5f2-5f17e91cdd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdversarialTrainer:\n",
    "    \n",
    "    def __init__(self, base_trainer, adversarial_config=None):\n",
    "\n",
    "        self.base_trainer = base_trainer\n",
    "        self.model = base_trainer.model\n",
    "        self.tokenizer = base_trainer.tokenizer\n",
    "        self.reward_function = base_trainer.reward_function  # This exists in your trainer\n",
    "        self.device = base_trainer.device\n",
    "        \n",
    "        config = adversarial_config or {}\n",
    "        self.adversarial_temperature = config.get('temperature', 1.2)\n",
    "        self.max_adversarial_examples = config.get('max_examples', 50)\n",
    "        self.preference_margin = config.get('preference_margin', 0.5)\n",
    "        self.validation_threshold = config.get('validation_threshold', 0.7)\n",
    "        \n",
    "        # Buffers for adversarial examples\n",
    "        self.adversarial_examples_buffer = []\n",
    "        self.preference_pairs_buffer = []\n",
    "\n",
    "        print(\"AdversarialTrainer initialized\")\n",
    "    \n",
    "    def generate_adversarial_examples(self, prompts, correct_answers=None, target_reward=0.5):\n",
    "        adversarial_examples = []\n",
    "        print(f\"Generating adversarial examples from {len(prompts)} prompts...\")\n",
    "    \n",
    "        generation_strategies = [\n",
    "            {'temperature': 1.5, 'top_p': 0.95, 'do_sample': True},\n",
    "            {'temperature': 0.3, 'top_p': 0.8, 'do_sample': True},\n",
    "            {'temperature': 1.0, 'top_p': 0.9, 'do_sample': True},\n",
    "        ]\n",
    "    \n",
    "        shown = 0  # counter for how many we print\n",
    "    \n",
    "        for strategy_idx, strategy in enumerate(generation_strategies):\n",
    "            print(f\"  Strategy {strategy_idx + 1}: temp={strategy['temperature']}\")\n",
    "    \n",
    "            for i, prompt in enumerate(prompts[:self.max_adversarial_examples]):\n",
    "                try:\n",
    "                    response = self._generate_with_strategy(prompt, strategy)\n",
    "                    correct_answer = correct_answers[i] if correct_answers else \"A\"\n",
    "                    reward_info = self.reward_function.compute_total_reward(response, correct_answer)\n",
    "    \n",
    "                    if self._is_adversarial_example(response, reward_info, target_reward):\n",
    "                        adv = {\n",
    "                            'prompt': prompt,\n",
    "                            'response': response,\n",
    "                            'reward_info': reward_info,\n",
    "                            'correct_answer': correct_answer,\n",
    "                            'strategy': strategy_idx,\n",
    "                            'vulnerability_type': self._classify_vulnerability(reward_info)\n",
    "                        }\n",
    "                        adversarial_examples.append(adv)\n",
    "    \n",
    "                        # Show first 1–2 adversarial examples for inspection\n",
    "                        if shown < 2:\n",
    "                            print(\"\\n=== Adversarial Example ===\")\n",
    "                            print(f\"Prompt:\\n{prompt}\\n\")\n",
    "                            print(f\"Response:\\n{response}\\n\")\n",
    "                            print(f\"Reward info: {reward_info}\")\n",
    "                            print(f\"Vulnerability: {adv['vulnerability_type']}\")\n",
    "                            print(\"===========================\\n\")\n",
    "                            shown += 1\n",
    "    \n",
    "                except Exception as e:\n",
    "                    print(f\"Error generating adversarial example: {e}\")\n",
    "                    continue\n",
    "    \n",
    "        print(f\"Found {len(adversarial_examples)} adversarial examples\")\n",
    "        self.adversarial_examples_buffer.extend(adversarial_examples)\n",
    "        return adversarial_examples\n",
    "\n",
    "    \n",
    "    def _generate_with_strategy(self, prompt, strategy):\n",
    "        \"\"\"Generate response using specific strategy parameters.\"\"\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id,\n",
    "                **strategy\n",
    "            )\n",
    "        \n",
    "        response = self.tokenizer.decode(\n",
    "            outputs[0][inputs['input_ids'].shape[1]:],\n",
    "            skip_special_tokens=True\n",
    "        ).strip()\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def _is_adversarial_example(self, response, reward_info, target_reward):\n",
    "        total_reward = reward_info.get('r_total', 0)\n",
    "        \n",
    "        # Issue: This might miss adversarial examples that exploit the reward function\n",
    "        # but don't necessarily have these specific penalty types\n",
    "        if total_reward < target_reward:\n",
    "            return False\n",
    "        \n",
    "        # Better approach: Check if reward is suspiciously high given response quality\n",
    "        has_issues = (\n",
    "            reward_info.get('p_answer', 0) > 0.05 or\n",
    "            reward_info.get('p_structural', 0) > 0.05 or \n",
    "            reward_info.get('r_factual', 1.0) < 0.4 or\n",
    "            not self.reward_function.validate_format(response)\n",
    "        )\n",
    "        \n",
    "        # Adversarial: High reward despite quality issues\n",
    "        return has_issues and total_reward > target_reward\n",
    "    \n",
    "    def _classify_vulnerability(self, reward_info):\n",
    "        \"\"\"Classify the type of vulnerability being exploited.\"\"\"\n",
    "        if reward_info.get('p_answer', 0) > 0.05:\n",
    "            return \"answer_leaking\"\n",
    "        elif reward_info.get('p_structural', 0) > 0.05:\n",
    "            return \"structural_gaming\"\n",
    "        elif reward_info.get('r_factual', 1.0) < 0.4:\n",
    "            return \"factual_exploitation\"\n",
    "        else:\n",
    "            return \"format_gaming\"\n",
    "    \n",
    "    def create_preference_pairs(self, adversarial_examples):\n",
    "        preference_pairs = []\n",
    "        \n",
    "        for adv_example in adversarial_examples:\n",
    "            clean_response = self._generate_clean_response(\n",
    "                adv_example['prompt'], \n",
    "                adv_example['correct_answer']\n",
    "            )\n",
    "            \n",
    "            clean_reward_info = self.reward_function.compute_total_reward(\n",
    "                clean_response, adv_example['correct_answer']\n",
    "            )\n",
    "            \n",
    "            clean_quality = self._assess_response_quality(clean_response, clean_reward_info)\n",
    "            adv_quality = self._assess_response_quality(adv_example['response'], adv_example['reward_info'])\n",
    "            \n",
    "            if clean_quality > adv_quality:\n",
    "                    preference_pairs.append({\n",
    "                        'prompt': adv_example['prompt'],\n",
    "                        'chosen': clean_response,\n",
    "                        'rejected': adv_example['response'],\n",
    "                        'correct_answer': adv_example['correct_answer'],\n",
    "                        'chosen_reward': clean_reward_info,\n",
    "                        'rejected_reward': adv_example['reward_info'],\n",
    "                        'vulnerability_type': adv_example['vulnerability_type']\n",
    "                    })\n",
    "        \n",
    "        print(f\"Created {len(preference_pairs)} preference pairs\")\n",
    "        self.preference_pairs_buffer.extend(preference_pairs)\n",
    "        \n",
    "        return preference_pairs\n",
    "    \n",
    "    def _generate_clean_response(self, prompt, correct_answer):\n",
    "        \"\"\"Generate a high-quality, non-adversarial response.\"\"\"\n",
    "        # Use conservative parameters for clean generation\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=200,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.pad_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        response = self.tokenizer.decode(\n",
    "            outputs[0][inputs['input_ids'].shape[1]:],\n",
    "            skip_special_tokens=True\n",
    "        ).strip()\n",
    "        \n",
    "        # Ensure proper format\n",
    "        response = self._ensure_proper_format(response, correct_answer)\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def _ensure_proper_format(self, response, correct_answer):\n",
    "        \"\"\"Ensure response has proper <think> and <answer> tags.\"\"\"\n",
    "        # Add think tags if missing\n",
    "        if '<think>' not in response.lower():\n",
    "            reasoning_part = response.split('<answer>')[0].strip()\n",
    "            answer_part = response.split('<answer>')[1] if '<answer>' in response else ''\n",
    "            response = f\"<think>{reasoning_part}</think>\"\n",
    "            if answer_part:\n",
    "                response += f\"<answer>{answer_part}\"\n",
    "        \n",
    "        # Add answer tags if missing\n",
    "        if '<answer>' not in response.lower():\n",
    "            response += f\"\\n<answer>{correct_answer}</answer>\"\n",
    "        \n",
    "        return response\n",
    "    \n",
    "    def update_reward_model(self, preference_pairs):\n",
    "        if not hasattr(self.base_trainer, 'reward_head'):\n",
    "            print(\"No learnable reward component found - skipping adversarial update\")\n",
    "            return {'loss': 0.0, 'accuracy': 0.0}\n",
    "        \n",
    "        print(f\"Updating reward model with {len(preference_pairs)} preference pairs...\")\n",
    "        self.base_trainer.reward_optimizer.zero_grad()\n",
    "        self.base_trainer.reward_head.train()\n",
    "        total_loss = 0\n",
    "        correct_rankings = 0\n",
    "        \n",
    "        for pair in preference_pairs:\n",
    "            try:\n",
    "                # Get hidden states for both responses\n",
    "                chosen_hidden = self._get_hidden_state_for_response(\n",
    "                    pair['prompt'], pair['chosen']\n",
    "                )\n",
    "                rejected_hidden = self._get_hidden_state_for_response(\n",
    "                    pair['prompt'], pair['rejected']\n",
    "                )\n",
    "                \n",
    "                # Compute predicted rewards\n",
    "                chosen_reward_pred = self.base_trainer.reward_head(chosen_hidden.detach().float())\n",
    "                rejected_reward_pred = self.base_trainer.reward_head(rejected_hidden.detach().float())\n",
    "                \n",
    "                # Preference loss: chosen should have higher reward\n",
    "                target = torch.ones_like(chosen_reward_pred)\n",
    "                loss = F.margin_ranking_loss(\n",
    "                    chosen_reward_pred, rejected_reward_pred, \n",
    "                    target, margin=self.preference_margin\n",
    "                )\n",
    "                \n",
    "                loss.backward()\n",
    "                total_loss += loss.item()\n",
    "                \n",
    "                # Track accuracy\n",
    "                if chosen_reward_pred.item() > rejected_reward_pred.item():\n",
    "                    correct_rankings += 1\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error in preference update: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Update parameters\n",
    "        if total_loss > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(self.base_trainer.reward_head.parameters(), 1.0)\n",
    "            self.base_trainer.reward_optimizer.step()\n",
    "        \n",
    "        avg_loss = total_loss / len(preference_pairs) if preference_pairs else 0\n",
    "        accuracy = correct_rankings / len(preference_pairs) if preference_pairs else 0\n",
    "        \n",
    "        print(f\"  Adversarial loss: {avg_loss:.4f}\")\n",
    "        print(f\"  Preference accuracy: {accuracy:.2f}\")\n",
    "        \n",
    "        return {'loss': avg_loss, 'accuracy': accuracy}\n",
    "\n",
    "    def analyze_response_quality(self, response, reward_info, correct_answer):\n",
    "        \"\"\"Analyze response against your 5 quality goals\"\"\"\n",
    "\n",
    "        violations = {\n",
    "            'correctness_violations': 0,\n",
    "            'answer_leaking_violations': 0,\n",
    "            'format_violations': 0,\n",
    "            'factual_violations': 0,\n",
    "            'bad_ood_high_rewards': 0\n",
    "        }\n",
    "\n",
    "        # Goal 1: Improved Correctness\n",
    "        if reward_info.get('r_binary', 0) <= 0:\n",
    "            violations['correctness_violations'] = 1\n",
    "\n",
    "        # Goal 2: Less Answer Leaking\n",
    "        if reward_info.get('p_answer', 0) > 0.1:\n",
    "            violations['answer_leaking_violations'] = 1\n",
    "\n",
    "        # Goal 3: Less Format Violations\n",
    "        if reward_info.get('p_structural', 0) > 0.1 or not self.reward_function.validate_format(response):\n",
    "            violations['format_violations'] = 1\n",
    "\n",
    "        # Goal 4: Less Factual Errors\n",
    "        if reward_info.get('r_factual', 1.0) < 0.3:\n",
    "            violations['factual_violations'] = 1\n",
    "\n",
    "        # Goal 5: Less Rewards to Bad OOD Reasoning\n",
    "        # High total reward despite violations = bad OOD getting undeserved high reward\n",
    "        has_violations = sum(violations.values()) > 0\n",
    "        high_reward = reward_info.get('r_total', 0) > 0.5\n",
    "        if has_violations and high_reward:\n",
    "            violations['bad_ood_high_rewards'] = 1\n",
    "\n",
    "        return violations\n",
    "\n",
    "    def _assess_response_quality(self, response, reward_info, correct_answer=None):\n",
    "        \"\"\"\n",
    "        Assess overall response quality using existing analysis method.\n",
    "        \n",
    "        Returns:\n",
    "            Quality score between 0 and 1 (higher = better quality)\n",
    "        \"\"\"\n",
    "        # Use existing violation analysis\n",
    "        violations = self.analyze_response_quality(response, reward_info, correct_answer or \"A\")\n",
    "        \n",
    "        # Convert violations to quality score\n",
    "        total_violations = sum(violations.values())\n",
    "        max_violations = len(violations)  # 5 possible violation types\n",
    "        \n",
    "        # Quality = 1 - (violation_ratio)\n",
    "        quality_score = 1.0 - (total_violations / max_violations)\n",
    "        \n",
    "        return quality_score\n",
    "    \n",
    "    def _get_hidden_state_for_response(self, prompt, response):\n",
    "        \"\"\"Get hidden state for a specific response.\"\"\"\n",
    "        full_text = prompt + \" \" + response\n",
    "        inputs = self.tokenizer(full_text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs, output_hidden_states=True)\n",
    "            hidden_state = outputs.hidden_states[-1][0].mean(dim=0)  # Average over sequence\n",
    "        \n",
    "        return hidden_state\n",
    "    \n",
    "    def validate_robustness(self, test_prompts, test_answers):\n",
    "        \"\"\"\n",
    "        Validate that reward model has become more robust.\n",
    "        \n",
    "        Args:\n",
    "            test_prompts: List of test prompts\n",
    "            test_answers: List of correct answers\n",
    "            \n",
    "        Returns:\n",
    "            Robustness metrics\n",
    "        \"\"\"\n",
    "        print(\"Validating reward model robustness...\")\n",
    "        \n",
    "        # Generate new adversarial examples\n",
    "        new_adversarial = self.generate_adversarial_examples(\n",
    "            test_prompts[:20], test_answers[:20]\n",
    "        )\n",
    "        \n",
    "        # Check if reward model correctly identifies them as problematic\n",
    "        correct_identifications = 0\n",
    "        \n",
    "        for adv_example in new_adversarial:\n",
    "            reward_info = adv_example['reward_info']\n",
    "            \n",
    "            # If reward model is robust, adversarial examples should get lower rewards\n",
    "            if reward_info['r_total'] < 0.3:  # Threshold for \"correctly identified as bad\"\n",
    "                correct_identifications += 1\n",
    "        \n",
    "        robustness_score = correct_identifications / len(new_adversarial) if new_adversarial else 0\n",
    "        \n",
    "        print(f\"Robustness validation: {robustness_score:.2f}\")\n",
    "        print(f\"  Found {len(new_adversarial)} new adversarial examples\")\n",
    "        print(f\"  Correctly identified {correct_identifications} as problematic\")\n",
    "        \n",
    "        return {\n",
    "            'robustness_score': robustness_score,\n",
    "            'adversarial_found': len(new_adversarial),\n",
    "            'correctly_identified': correct_identifications\n",
    "        }\n",
    "    \n",
    "    def run_adversarial_training_cycle(self, prompts, correct_answers, num_cycles=3):\n",
    "        \"\"\"\n",
    "        Complete adversarial training cycle.\n",
    "        \n",
    "        Args:\n",
    "            prompts: Training prompts\n",
    "            correct_answers: Correct answers\n",
    "            num_cycles: Number of adversarial cycles to run\n",
    "            \n",
    "        Returns:\n",
    "            Training summary\n",
    "        \"\"\"\n",
    "        print(f\"Starting {num_cycles} adversarial training cycles...\")\n",
    "        \n",
    "        cycle_results = []\n",
    "        \n",
    "        for cycle in range(num_cycles):\n",
    "            print(f\"\\n=== Adversarial Cycle {cycle + 1}/{num_cycles} ===\")\n",
    "            \n",
    "            # Step 1: Generate adversarial examples\n",
    "            adversarial_examples = self.generate_adversarial_examples(\n",
    "                prompts, correct_answers\n",
    "            )\n",
    "            \n",
    "            if not adversarial_examples:\n",
    "                print(\"No adversarial examples found\")\n",
    "                break\n",
    "            \n",
    "            # Step 2: Create preference pairs\n",
    "            preference_pairs = self.create_preference_pairs(adversarial_examples)\n",
    "            \n",
    "            if not preference_pairs:\n",
    "                print(\"No valid preference pairs created\")\n",
    "                continue\n",
    "            \n",
    "            # Step 3: Update reward model\n",
    "            update_metrics = self.update_reward_model(preference_pairs)\n",
    "            \n",
    "            # Step 4: Validate improvement\n",
    "            validation_metrics = self.validate_robustness(\n",
    "                prompts[:10], correct_answers[:10]\n",
    "            )\n",
    "            \n",
    "            cycle_results.append({\n",
    "                'cycle': cycle + 1,\n",
    "                'adversarial_found': len(adversarial_examples),\n",
    "                'preference_pairs': len(preference_pairs),\n",
    "                'update_loss': update_metrics['loss'],\n",
    "                'update_accuracy': update_metrics['accuracy'],\n",
    "                'robustness_score': validation_metrics['robustness_score']\n",
    "            })\n",
    "            \n",
    "            print(f\"Cycle {cycle + 1} complete:\")\n",
    "            print(f\"  Adversarial examples: {len(adversarial_examples)}\")\n",
    "            print(f\"  Update accuracy: {update_metrics['accuracy']:.2f}\")\n",
    "            print(f\"  Robustness score: {validation_metrics['robustness_score']:.2f}\")\n",
    "        \n",
    "        return {\n",
    "            'total_cycles': len(cycle_results),\n",
    "            'cycle_results': cycle_results,\n",
    "            'final_robustness': cycle_results[-1]['robustness_score'] if cycle_results else 0\n",
    "        }\n",
    "    \n",
    "    def clear_buffers(self):\n",
    "        \"\"\"Clear adversarial example and preference pair buffers.\"\"\"\n",
    "        self.adversarial_examples_buffer.clear()\n",
    "        self.preference_pairs_buffer.clear()\n",
    "        print(\"Adversarial training buffers cleared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de9c71e-ea19-4213-9d8c-e89135bc8061",
   "metadata": {},
   "source": [
    "# Fact Extraction and LLM Judge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b972ec3a-99a6-4350-ab64-33c418d65dd4",
   "metadata": {},
   "source": [
    "## AtomicFactExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e36edf-a9cd-45ad-8dd7-b2bfdfb37458",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class AtomicFact:\n",
    "    text: str\n",
    "    category: str\n",
    "    confidence: float = 0.0\n",
    "    llm_score: float = 0.0\n",
    "    kb_score: float = 0.0\n",
    "    source_sentence= \"\"\n",
    "\n",
    "\n",
    "class FactCategory:\n",
    "    \"\"\"Categories for atomic facts as defined in the methodology\"\"\"\n",
    "    CONDITION = \"patient_condition\"\n",
    "    MEDICATION = \"medication\"\n",
    "    ANATOMY = \"anatomy\"\n",
    "    RELATIONSHIP = \"relationship\"\n",
    "\n",
    "\n",
    "class AtomicFactExtractor:\n",
    "    def __init__(self, model_name, region_name):\n",
    "        self.model_name = model_name\n",
    "        self.extraction_prompt = self._build_extraction_prompt()\n",
    "        self.client = ClaudeBedrockClient(model_name, region_name)\n",
    "\n",
    "    def _build_extraction_prompt(self):\n",
    "        return \"\"\"You are a medical fact extraction specialist. Extract discrete, \n",
    "        verifiable clinical facts from the given reasoning text.\n",
    "\n",
    "        FACT CATEGORIES:\n",
    "        1. patient_condition: Statements about patient conditions, symptoms, or disease states\n",
    "        2. medication: Assertions about medications, procedures, or therapeutic interventions  \n",
    "        3. anatomy: Facts about anatomical structures, biological processes, or pathophysiology\n",
    "        4. relationship: Relationships between symptoms, conditions, and underlying mechanisms\n",
    "\n",
    "        EXTRACTION RULES:\n",
    "        - Each fact should be self-contained and independently verifiable\n",
    "        - Focus on specific medical claims, not general reasoning steps\n",
    "        - Exclude subjective statements or reasoning connections\n",
    "        - Each fact should be 1-2 sentences maximum\n",
    "\n",
    "        INPUT TEXT: {reasoning_text}\n",
    "\n",
    "        You must respond with valid JSON only, wrapped strictly between START_JSON and END_JSON markers. Example format:\n",
    "        START_JSON \n",
    "        {\"facts\": [{{\"text\": \"Ampicillin is effective against E. coli infections\",\"category\": \"medication\",\"source_sentence\": \"Ampicillin is a first-line antibiotic for UTIs\"}}]}\n",
    "        END_JSON\n",
    "\n",
    "        EXTRACTED FACTS (JSON only):\"\"\"\n",
    "\n",
    "    def extract_facts(self, reasoning_text):\n",
    "        try:\n",
    "            cleaned_text = self._clean_reasoning_text(reasoning_text)\n",
    "            prompt = self.extraction_prompt.format(reasoning_text=cleaned_text)\n",
    "            response = self._call_extraction_llm(prompt)\n",
    "            facts = self._parse_extraction_response(response)\n",
    "            return facts\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in fact extraction: {e}\")\n",
    "            return []\n",
    "\n",
    "    def _clean_reasoning_text(self, text):\n",
    "        print(f\"Original text: '{text}'\")\n",
    "        \n",
    "        think_match = re.search(r'<think>(.*?)</think>', text, flags=re.DOTALL)\n",
    "        if think_match:\n",
    "            text = think_match.group(1)  # Extract the captured group\n",
    "            print(f\"Extracted from <think> tags: '{text}'\")\n",
    "        else:\n",
    "            print(\"No <think> tags found, using original text\")\n",
    "        \n",
    "        # Remove <answer> tags entirely\n",
    "        text = re.sub(r'<answer>.*?</answer>', '', text, flags=re.DOTALL)\n",
    "        \n",
    "        # Clean whitespace\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        print(f\"Final cleaned text: '{text}'\")\n",
    "        return text.strip()\n",
    "\n",
    "    def _call_extraction_llm(self, prompt):\n",
    "        full_prompt = f\"\"\"You are a medical fact extraction specialist.\\n\\n{prompt}\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = self.client.chat_completion(\n",
    "                messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
    "                temperature=0.1,\n",
    "                max_tokens=1000\n",
    "            )\n",
    "\n",
    "            content = response['choices'][0]['message']['content']\n",
    "            return content\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"API call failed: {e}\")\n",
    "            return \"\"\n",
    "\n",
    "    def _fallback_extraction(self, text):\n",
    "        facts = []\n",
    "\n",
    "        # Extract medical sentences using simple patterns\n",
    "        sentences = re.split(r'[.!?]+', text)\n",
    "        medical_keywords = [\n",
    "            'patient', 'diagnosis', 'treatment', 'symptom', 'medication',\n",
    "            'therapy', 'condition', 'disease', 'clinical', 'medical'\n",
    "        ]\n",
    "\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            if len(sentence) > 15:  # Minimum length\n",
    "                # Check if sentence contains medical content\n",
    "                if any(keyword in sentence.lower() for keyword in medical_keywords):\n",
    "                    fact = AtomicFact(\n",
    "                        text=sentence,\n",
    "                        category=\"medical_statement\",\n",
    "                        source_sentence=sentence\n",
    "                    )\n",
    "                    facts.append(fact)\n",
    "\n",
    "        return facts[:5]\n",
    "\n",
    "    def _parse_extraction_response(self, response):\n",
    "        facts = []\n",
    "        \n",
    "        try:\n",
    "            print(f\"Parsing response: '{response}'\")\n",
    "            \n",
    "            # Step 1: Clean the response\n",
    "            cleaned_response = response.strip()\n",
    "            \n",
    "            # Remove code block markers if present\n",
    "            cleaned_response = re.sub(r'```json\\s*', '', cleaned_response)\n",
    "            cleaned_response = re.sub(r'```\\s*$', '', cleaned_response)\n",
    "            \n",
    "            # Step 2: Extract JSON - FIXED PATTERNS\n",
    "            json_str = None\n",
    "            \n",
    "            # Try to find JSON between START_JSON and END_JSON markers\n",
    "            start_end_match = re.search(r'START_JSON\\s*(.+?)\\s*END_JSON', cleaned_response, re.DOTALL)\n",
    "            if start_end_match:\n",
    "                json_str = start_end_match.group(1).strip()\n",
    "                print(f\"Found JSON between markers: '{json_str}'\")\n",
    "            else:\n",
    "                # Look for JSON object pattern - IMPROVED REGEX\n",
    "                json_match = re.search(r'\\{[^{}]*\"facts\"[^{}]*\\[[^\\]]*\\][^{}]*\\}', cleaned_response, re.DOTALL)\n",
    "                if json_match:\n",
    "                    json_str = json_match.group(0)\n",
    "                    print(f\"Found JSON with facts pattern: '{json_str}'\")\n",
    "                else:\n",
    "                    # Fallback: any JSON-like structure\n",
    "                    json_match = re.search(r'\\{.*?\\}', cleaned_response, re.DOTALL)\n",
    "                    if json_match:\n",
    "                        json_str = json_match.group(0)\n",
    "                        print(f\"Found JSON with fallback pattern: '{json_str}'\")\n",
    "            \n",
    "            if not json_str:\n",
    "                print(\"No JSON pattern found, using fallback extraction\")\n",
    "                return self._fallback_extraction(response)\n",
    "            \n",
    "            # Step 3: Fix common JSON issues\n",
    "            json_str = self._fix_json_issues(json_str)\n",
    "            print(f\"After fixing JSON: '{json_str}'\")\n",
    "            \n",
    "            # Step 4: Parse JSON\n",
    "            try:\n",
    "                data = json.loads(json_str)\n",
    "                print(f\"Successfully parsed JSON: {data}\")\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"JSON decode failed: {e}\")\n",
    "                print(\"Attempting repair...\")\n",
    "                \n",
    "                repaired_json = self._repair_json_advanced(json_str)\n",
    "                try:\n",
    "                    data = json.loads(repaired_json)\n",
    "                    print(\"Successfully repaired and parsed JSON\")\n",
    "                except Exception as repair_e:\n",
    "                    print(f\"JSON repair also failed: {repair_e}\")\n",
    "                    return self._fallback_extraction(response)\n",
    "            \n",
    "            # Step 5: Extract facts from parsed data\n",
    "            if isinstance(data, dict) and \"facts\" in data:\n",
    "                facts_list = data[\"facts\"]\n",
    "                if isinstance(facts_list, list):\n",
    "                    print(f\"Found {len(facts_list)} facts in data\")\n",
    "                    \n",
    "                    for i, fact_data in enumerate(facts_list):\n",
    "                        if isinstance(fact_data, dict):\n",
    "                            text = fact_data.get(\"text\", \"\").strip()\n",
    "                            if text:\n",
    "                                fact = AtomicFact(\n",
    "                                    text=text,\n",
    "                                    category=fact_data.get(\"category\", \"unknown\"),\n",
    "                                    source_sentence=fact_data.get(\"source_sentence\", \"\")\n",
    "                                )\n",
    "                                facts.append(fact)\n",
    "                                print(f\"Created fact {i+1}: {fact.text}\")\n",
    "                            else:\n",
    "                                print(f\"Skipping fact {i+1}: no text content\")\n",
    "                else:\n",
    "                    print(f\"'facts' is not a list: {type(facts_list)}\")\n",
    "            else:\n",
    "                print(f\"No 'facts' key found. Available keys: {list(data.keys()) if isinstance(data, dict) else 'not a dict'}\")\n",
    "            \n",
    "            if not facts:\n",
    "                print(\"No valid facts extracted, using fallback\")\n",
    "                return self._fallback_extraction(response)\n",
    "            \n",
    "            return facts\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in _parse_extraction_response: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return self._fallback_extraction(response)\n",
    "\n",
    "    def _fix_incomplete_json(self, json_str):\n",
    "        \"\"\"Fix common incomplete JSON issues\"\"\"\n",
    "        # Remove trailing incomplete objects\n",
    "        if json_str.count('{') > json_str.count('}'):\n",
    "            # Add missing closing braces\n",
    "            missing_braces = json_str.count('{') - json_str.count('}')\n",
    "            json_str += '}' * missing_braces\n",
    "\n",
    "        # Fix incomplete arrays\n",
    "        if json_str.count('[') > json_str.count(']'):\n",
    "            missing_brackets = json_str.count('[') - json_str.count(']')\n",
    "            json_str += ']' * missing_brackets\n",
    "\n",
    "        # Remove trailing commas\n",
    "        json_str = re.sub(r',\\s*}', '}', json_str)\n",
    "        json_str = re.sub(r',\\s*]', ']', json_str)\n",
    "\n",
    "        return json_str\n",
    "\n",
    "    def _repair_json(self, json_str):\n",
    "        \"\"\"Attempt to repair malformed JSON\"\"\"\n",
    "        # Remove incomplete trailing objects\n",
    "        lines = json_str.split('\\n')\n",
    "        clean_lines = []\n",
    "\n",
    "        for line in lines:\n",
    "            # Skip lines that look incomplete (no closing quote)\n",
    "            if '\"text\":' in line and line.count('\"') % 2 != 0:\n",
    "                continue\n",
    "            clean_lines.append(line)\n",
    "\n",
    "        repaired = '\\n'.join(clean_lines)\n",
    "\n",
    "        # Ensure proper closing\n",
    "        if not repaired.strip().endswith('}'):\n",
    "            repaired = repaired.rstrip() + '\\n    ]\\n}'\n",
    "\n",
    "        return repaired"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c167b6-77f6-4414-b136-4276390cc419",
   "metadata": {},
   "source": [
    "## LLM Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0145196a-6097-4d29-9ceb-69d8d0c1fe6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use your own llm judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624fcafc-2ccf-4dd1-97bb-4aa52a4079ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMJudge:\n",
    "    def __init__(self, model_name, region_name):\n",
    "\n",
    "        self.claude_client = ClaudeBedrockClient(model_name, region_name)\n",
    "        self.model_name = model_name\n",
    "\n",
    "    def judge_fact_accuracy(self, fact, context=\"\", temperature=0.1):\n",
    "        full_prompt = f\"\"\"You are a medical expert evaluating the accuracy of medical facts. \n",
    "        Provide a confidence score between 0.0 and 1.0 where:\n",
    "        - 1.0 = Definitely accurate and well-established medical fact\n",
    "        - 0.8-0.9 = Very likely accurate with strong evidence\n",
    "        - 0.6-0.7 = Probably accurate but may have exceptions\n",
    "        - 0.4-0.5 = Uncertain or conflicting evidence\n",
    "        - 0.2-0.3 = Probably inaccurate\n",
    "        - 0.0-0.1 = Definitely inaccurate or harmful\n",
    "        Please evaluate this medical fact:\n",
    "\n",
    "        Fact: {fact}\n",
    "\n",
    "        Context: {context if context else \"No additional context provided\"}\n",
    "\n",
    "        Respond with just a JSON object containing:\n",
    "        {{\n",
    "            \"confidence_score\": <float between 0.0 and 1.0>,\n",
    "            \"reasoning\": \"<brief explanation>\",\n",
    "            \"medical_category\": \"<relevant medical specialty>\"\n",
    "        }}\n",
    "\n",
    "        Provide your assessment as JSON only.\"\"\"\n",
    "\n",
    "        messages = [{\"role\": \"user\", \"content\": full_prompt}]\n",
    "\n",
    "        try:\n",
    "            response = self.claude_client.chat_completion(\n",
    "                messages=messages,\n",
    "                temperature=temperature,\n",
    "                max_tokens=500\n",
    "            )\n",
    "\n",
    "            content = response['choices'][0]['message']['content']\n",
    "\n",
    "            try:\n",
    "                json_match = re.search(r'\\{.*\\}', content, re.DOTALL)\n",
    "                if json_match:\n",
    "                    result = json.loads(json_match.group())\n",
    "\n",
    "                    if 'confidence_score' not in result:\n",
    "                        result['confidence_score'] = 0.5\n",
    "\n",
    "                    result['model_used'] = self.model_name\n",
    "                    result['raw_response'] = content\n",
    "\n",
    "                    return result\n",
    "                else:\n",
    "                    raise ValueError(\"No JSON found in response\")\n",
    "\n",
    "            except (json.JSONDecodeError, ValueError) as e:\n",
    "                print(f\"Failed to parse Claude response as JSON: {e}\")\n",
    "                print(f\"Raw response: {content}\")\n",
    "\n",
    "                score_match = re.search(r'(\\d+\\.?\\d*)', content)\n",
    "                score = float(score_match.group(1)) if score_match else 0.5\n",
    "                if score > 1.0:\n",
    "                    score = score / 10.0\n",
    "\n",
    "                return {\n",
    "                    'confidence_score': min(1.0, max(0.0, score)),\n",
    "                    'reasoning': content[:200] + \"...\" if len(content) > 200 else content,\n",
    "                    'medical_category': 'unknown',\n",
    "                    'model_used': self.model_name,\n",
    "                    'raw_response': content,\n",
    "                    'parsing_error': str(e)\n",
    "                }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in Claude fact judgment: {e}\")\n",
    "            return {\n",
    "                'confidence_score': 0.0,\n",
    "                'reasoning': f'Error occurred: {str(e)}',\n",
    "                'medical_category': 'error',\n",
    "                'model_used': self.model_name,\n",
    "                'error': str(e)\n",
    "            }\n",
    "\n",
    "    def batch_judge_facts(self, facts, context=\"\"):\n",
    "        \"\"\"Judge multiple facts in batch\"\"\"\n",
    "        results = []\n",
    "\n",
    "        for i, fact in enumerate(facts):\n",
    "            print(f\"Judging fact {i + 1}/{len(facts)}: {fact[:100]}...\")\n",
    "            result = self.judge_fact_accuracy(fact, context)\n",
    "            results.append(result)\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a40c413-6bcb-4f6f-8b39-6f4411aa98d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMJudgeVerifier:\n",
    "    def __init__(self, model_name, region_name):\n",
    "        self.model_name = model_name\n",
    "        self.verification_prompt = self._build_verification_prompt()\n",
    "        self.client = ClaudeBedrockClient(model_name, region_name)\n",
    "\n",
    "    def _build_verification_prompt(self):\n",
    "        return \"\"\"You are a medical fact verification specialist. Evaluate the clinical accuracy of the given atomic fact.\n",
    "\n",
    "        VERIFICATION CRITERIA:\n",
    "        1. Medical Accuracy: Is the fact clinically correct?\n",
    "        2. Specificity: Is the claim specific enough to be verifiable?\n",
    "        3. Context Appropriateness: Does the fact make sense in the clinical context?\n",
    "        4. Evidence Base: Is this supported by established medical knowledge?\n",
    "\n",
    "        FACT TO VERIFY: {fact_text}\n",
    "        MEDICAL CONTEXT: {context}\n",
    "\n",
    "        You MUST respond with valid JSON in exactly this format (no extra text):\n",
    "        {{\n",
    "            \"confidence_score\": 0.85,\n",
    "            \"is_accurate\": true,\n",
    "            \"reasoning\": \"detailed explanation of your assessment\",\n",
    "            \"concerns\": [\"any specific concerns or caveats\"]\n",
    "        }}\n",
    "\n",
    "        CRITICAL: The confidence_score must be a decimal number between 0.0 and 1.0.\n",
    "\n",
    "        ASSESSMENT:\"\"\"\n",
    "\n",
    "    def verify_fact(self, fact, medical_context=\"\"):\n",
    "        \"\"\"\n",
    "        Verify a single atomic fact using LLM judge\n",
    "        Returns confidence score [0, 1]\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Format verification prompt\n",
    "            prompt = self.verification_prompt.format(\n",
    "                fact_text=fact.text,\n",
    "                context=medical_context\n",
    "            )\n",
    "            response = self._call_verification_llm(prompt)\n",
    "            confidence = self._parse_verification_response(response)\n",
    "            fact.llm_score = confidence\n",
    "            return confidence\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in LLM verification: {e}\")\n",
    "            return 0.0\n",
    "\n",
    "    def _call_verification_llm(self, prompt):\n",
    "        full_prompt = f\"\"\"You are a medical fact verification specialist.\n",
    "\n",
    "    {prompt}\"\"\"\n",
    "        response = self.client.chat_completion(\n",
    "            messages=[{\"role\": \"user\", \"content\": full_prompt}],\n",
    "            temperature=0.1,\n",
    "            max_tokens=500\n",
    "        )\n",
    "        return response['choices'][0]['message']['content']\n",
    "\n",
    "    def _parse_verification_response(self, response):\n",
    "        try:\n",
    "            if not response or response.strip() == \"\":\n",
    "                return 0.0\n",
    "\n",
    "            cleaned = response.strip()\n",
    "            cleaned = re.sub(r'```json\\s*', '', cleaned)\n",
    "            cleaned = re.sub(r'```\\s*', '', cleaned)\n",
    "\n",
    "            json_patterns = [\n",
    "                r'\\{[^{}]*\"confidence_score\"[^{}]*:[^{}]*[0-9.]+[^{}]*\\}',\n",
    "                r'\\{.*?\"confidence_score\".*?\\}',\n",
    "                r'\\{.*\\}',\n",
    "            ]\n",
    "            for pattern in json_patterns:\n",
    "                matches = re.findall(pattern, cleaned, re.DOTALL | re.IGNORECASE)\n",
    "                for match in matches:\n",
    "                    try:\n",
    "                        json_str = match.strip()\n",
    "                        json_str = re.sub(r',\\s*}', '}', json_str)\n",
    "                        json_str = re.sub(r',\\s*]', ']', json_str)\n",
    "\n",
    "                        data = json.loads(json_str)\n",
    "                        score = data.get(\"confidence_score\")\n",
    "                        if score is not None:\n",
    "                            return max(0.0, min(1.0, float(score)))\n",
    "\n",
    "                    except (json.JSONDecodeError, ValueError, TypeError):\n",
    "                        continue\n",
    "\n",
    "            score_patterns = [\n",
    "                r'\"confidence_score\"[:\\s]*([0-9.]+)',\n",
    "                r'confidence[_\\s]*score[:\\s]*([0-9.]+)',\n",
    "                r'score[:\\s]*([0-9.]+)',\n",
    "                r'confidence[:\\s]*([0-9.]+)'\n",
    "            ]\n",
    "\n",
    "            for pattern in score_patterns:\n",
    "                match = re.search(pattern, cleaned, re.IGNORECASE)\n",
    "                if match:\n",
    "                    try:\n",
    "                        score = float(match.group(1))\n",
    "                        return max(0.0, min(1.0, score))\n",
    "                    except (ValueError, IndexError):\n",
    "                        continue\n",
    "\n",
    "            number_match = re.search(r'([0-9.]+)', cleaned)\n",
    "            if number_match:\n",
    "                try:\n",
    "                    score = float(number_match.group(1))\n",
    "                    if 0.0 <= score <= 1.0:\n",
    "                        return score\n",
    "                    elif 0.0 <= score <= 5.0:\n",
    "                        return score / 5.0\n",
    "                    elif 0.0 <= score <= 10.0:\n",
    "                        return score / 10.0\n",
    "                except ValueError:\n",
    "                    pass\n",
    "\n",
    "            print(f\"Could not parse verification response: {response[:200]}\")\n",
    "            return 0.5\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing verification response: {e}\")\n",
    "            return 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c841de3-b999-4c26-a72d-83bde5637556",
   "metadata": {},
   "source": [
    "## KB Verify"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad4c9b6-ddef-44c7-9994-91ffbf4f5abe",
   "metadata": {},
   "source": [
    "### Local KB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7711777-a515-4b46-b75e-e285e5873c8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class LocalMedicalKnowledgeBase:\n",
    "    def __init__(self, \n",
    "                 embedding_model= 'all-MiniLM-L6-v2',\n",
    "                 cache_dir= \"local_kb_cache\"):\n",
    "\n",
    "        self.embedding_model = SentenceTransformer(embedding_model)\n",
    "        self.cache_dir = cache_dir\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "        self.knowledge_base = []\n",
    "        self.knowledge_embeddings = None\n",
    "\n",
    "        self._initialize_medical_knowledge()\n",
    "\n",
    "    def _initialize_medical_knowledge(self):\n",
    "        \"\"\"Initialize with comprehensive medical facts\"\"\"\n",
    "        cache_file = os.path.join(self.cache_dir, \"local_medical_knowledge.pkl\")\n",
    "\n",
    "        if os.path.exists(cache_file):\n",
    "            try:\n",
    "                with open(cache_file, 'rb') as f:\n",
    "                    cached_data = pickle.load(f)\n",
    "                    self.knowledge_base = cached_data['facts']\n",
    "                    self.knowledge_embeddings = cached_data['embeddings']\n",
    "                    print(f\"Loaded {len(self.knowledge_base)} medical facts from cache\")\n",
    "                    return\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to load cache: {e}\")\n",
    "        \n",
    "        print(\"Building local medical knowledge base...\")\n",
    "        self.knowledge_base = self._create_medical_facts()\n",
    "        \n",
    "        fact_texts = [fact['text'] for fact in self.knowledge_base]\n",
    "        print(\"Generating embeddings for medical facts...\")\n",
    "        self.knowledge_embeddings = self.embedding_model.encode(fact_texts)\n",
    "        \n",
    "        try:\n",
    "            with open(cache_file, 'wb') as f:\n",
    "                pickle.dump({\n",
    "                    'facts': self.knowledge_base,\n",
    "                    'embeddings': self.knowledge_embeddings\n",
    "                }, f)\n",
    "            print(f\"Cached {len(self.knowledge_base)} medical facts\")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to cache knowledge base: {e}\")\n",
    "    \n",
    "    def _create_medical_facts(self) -> List[Dict]:\n",
    "        \"\"\"Create comprehensive medical knowledge base\"\"\"\n",
    "        return [\n",
    "            # CARDIOLOGY\n",
    "            {\"text\": \"Myocardial infarction is caused by coronary artery occlusion leading to cardiac muscle death\", \"category\": \"cardiology\", \"source\": \"medical_textbook\", \"confidence\": 0.95},\n",
    "            {\"text\": \"Chest pain, shortness of breath, and diaphoresis are classic symptoms of myocardial infarction\", \"category\": \"cardiology\", \"source\": \"clinical_guidelines\", \"confidence\": 0.92},\n",
    "            {\"text\": \"Hypertension is defined as systolic blood pressure ≥140 mmHg or diastolic ≥90 mmHg\", \"category\": \"cardiology\", \"source\": \"AHA_guidelines\", \"confidence\": 0.96},\n",
    "            {\"text\": \"ACE inhibitors are first-line treatment for hypertension and heart failure\", \"category\": \"cardiology\", \"source\": \"treatment_guidelines\", \"confidence\": 0.90},\n",
    "            {\"text\": \"Atrial fibrillation significantly increases the risk of stroke\", \"category\": \"cardiology\", \"source\": \"clinical_studies\", \"confidence\": 0.94},\n",
    "            {\"text\": \"Beta-blockers reduce heart rate and myocardial oxygen demand\", \"category\": \"cardiology\", \"source\": \"pharmacology\", \"confidence\": 0.93},\n",
    "            {\"text\": \"Electrocardiography shows ST-elevation in acute myocardial infarction\", \"category\": \"cardiology\", \"source\": \"diagnostic_criteria\", \"confidence\": 0.91},\n",
    "            {\"text\": \"Cardiac catheterization is the gold standard for diagnosing coronary artery disease\", \"category\": \"cardiology\", \"source\": \"diagnostic_procedures\", \"confidence\": 0.89},\n",
    "            \n",
    "            # ENDOCRINOLOGY\n",
    "            {\"text\": \"Type 1 diabetes mellitus is caused by autoimmune destruction of pancreatic beta cells\", \"category\": \"endocrinology\", \"source\": \"pathophysiology\", \"confidence\": 0.96},\n",
    "            {\"text\": \"Type 2 diabetes mellitus is characterized by insulin resistance and relative insulin deficiency\", \"category\": \"endocrinology\", \"source\": \"pathophysiology\", \"confidence\": 0.95},\n",
    "            {\"text\": \"Metformin is the first-line treatment for type 2 diabetes mellitus\", \"category\": \"endocrinology\", \"source\": \"ADA_guidelines\", \"confidence\": 0.94},\n",
    "            {\"text\": \"HbA1c ≥6.5% indicates diabetes mellitus diagnosis\", \"category\": \"endocrinology\", \"source\": \"diagnostic_criteria\", \"confidence\": 0.96},\n",
    "            {\"text\": \"Insulin is absolutely required for type 1 diabetes management\", \"category\": \"endocrinology\", \"source\": \"treatment_standards\", \"confidence\": 0.98},\n",
    "            {\"text\": \"Diabetic ketoacidosis is a life-threatening complication of diabetes\", \"category\": \"endocrinology\", \"source\": \"emergency_medicine\", \"confidence\": 0.93},\n",
    "            {\"text\": \"Hypoglycemia symptoms include diaphoresis, tremor, and altered mental status\", \"category\": \"endocrinology\", \"source\": \"clinical_presentation\", \"confidence\": 0.91},\n",
    "            {\"text\": \"Thyroid stimulating hormone (TSH) is elevated in hypothyroidism\", \"category\": \"endocrinology\", \"source\": \"laboratory_medicine\", \"confidence\": 0.94},\n",
    "            \n",
    "            # PULMONOLOGY\n",
    "            {\"text\": \"Pneumonia causes consolidation visible on chest radiograph\", \"category\": \"pulmonology\", \"source\": \"radiology\", \"confidence\": 0.90},\n",
    "            {\"text\": \"Asthma is characterized by reversible airway obstruction and inflammation\", \"category\": \"pulmonology\", \"source\": \"pathophysiology\", \"confidence\": 0.93},\n",
    "            {\"text\": \"Albuterol is a short-acting beta-2 agonist bronchodilator\", \"category\": \"pulmonology\", \"source\": \"pharmacology\", \"confidence\": 0.96},\n",
    "            {\"text\": \"Chronic obstructive pulmonary disease (COPD) is primarily caused by tobacco smoking\", \"category\": \"pulmonology\", \"source\": \"epidemiology\", \"confidence\": 0.91},\n",
    "            {\"text\": \"Pulmonary embolism can cause sudden onset dyspnea and chest pain\", \"category\": \"pulmonology\", \"source\": \"clinical_presentation\", \"confidence\": 0.89},\n",
    "            {\"text\": \"Spirometry shows reduced FEV1/FVC ratio in obstructive lung disease\", \"category\": \"pulmonology\", \"source\": \"pulmonary_function\", \"confidence\": 0.92},\n",
    "            {\"text\": \"Oxygen saturation below 90% indicates significant hypoxemia\", \"category\": \"pulmonology\", \"source\": \"critical_care\", \"confidence\": 0.88},\n",
    "            \n",
    "            # INFECTIOUS DISEASE\n",
    "            {\"text\": \"Sepsis is a life-threatening organ dysfunction caused by dysregulated host response to infection\", \"category\": \"infectious_disease\", \"source\": \"sepsis_guidelines\", \"confidence\": 0.94},\n",
    "            {\"text\": \"Penicillin is effective against gram-positive bacterial infections\", \"category\": \"infectious_disease\", \"source\": \"microbiology\", \"confidence\": 0.92},\n",
    "            {\"text\": \"Viral infections do not respond to antibiotic treatment\", \"category\": \"infectious_disease\", \"source\": \"antimicrobial_stewardship\", \"confidence\": 0.97},\n",
    "            {\"text\": \"Urinary tract infections commonly present with dysuria and urinary frequency\", \"category\": \"infectious_disease\", \"source\": \"clinical_presentation\", \"confidence\": 0.88},\n",
    "            {\"text\": \"Blood cultures should be obtained before starting empiric antibiotic therapy\", \"category\": \"infectious_disease\", \"source\": \"diagnostic_guidelines\", \"confidence\": 0.85},\n",
    "            {\"text\": \"Methicillin-resistant Staphylococcus aureus (MRSA) requires vancomycin treatment\", \"category\": \"infectious_disease\", \"source\": \"antimicrobial_guidelines\", \"confidence\": 0.91},\n",
    "            \n",
    "            # NEUROLOGY\n",
    "            {\"text\": \"Stroke symptoms include sudden onset focal neurological deficits\", \"category\": \"neurology\", \"source\": \"clinical_criteria\", \"confidence\": 0.93},\n",
    "            {\"text\": \"Tissue plasminogen activator (tPA) is used for acute ischemic stroke within 4.5 hours\", \"category\": \"neurology\", \"source\": \"stroke_guidelines\", \"confidence\": 0.90},\n",
    "            {\"text\": \"Seizures can be focal or generalized based on their origin and spread\", \"category\": \"neurology\", \"source\": \"epilepsy_classification\", \"confidence\": 0.94},\n",
    "            {\"text\": \"Computed tomography (CT) can rapidly detect hemorrhagic stroke\", \"category\": \"neurology\", \"source\": \"neuroimaging\", \"confidence\": 0.87},\n",
    "            {\"text\": \"Lumbar puncture is contraindicated with increased intracranial pressure\", \"category\": \"neurology\", \"source\": \"procedural_guidelines\", \"confidence\": 0.89},\n",
    "            {\"text\": \"Multiple sclerosis causes demyelinating lesions in the central nervous system\", \"category\": \"neurology\", \"source\": \"pathophysiology\", \"confidence\": 0.92},\n",
    "            \n",
    "            # PSYCHIATRY\n",
    "            {\"text\": \"Major depressive disorder requires at least 2 weeks of depressive symptoms\", \"category\": \"psychiatry\", \"source\": \"DSM5\", \"confidence\": 0.96},\n",
    "            {\"text\": \"Selective serotonin reuptake inhibitors (SSRIs) are first-line treatment for depression\", \"category\": \"psychiatry\", \"source\": \"treatment_guidelines\", \"confidence\": 0.88},\n",
    "            {\"text\": \"Bipolar disorder includes both manic and depressive episodes\", \"category\": \"psychiatry\", \"source\": \"DSM5\", \"confidence\": 0.95},\n",
    "            {\"text\": \"Suicidal ideation requires immediate safety assessment and intervention\", \"category\": \"psychiatry\", \"source\": \"crisis_intervention\", \"confidence\": 0.97},\n",
    "            {\"text\": \"Antipsychotic medications are used to treat schizophrenia and psychotic disorders\", \"category\": \"psychiatry\", \"source\": \"psychopharmacology\", \"confidence\": 0.91},\n",
    "            \n",
    "            # PHARMACOLOGY\n",
    "            {\"text\": \"Warfarin requires regular INR monitoring due to narrow therapeutic window\", \"category\": \"pharmacology\", \"source\": \"anticoagulation_guidelines\", \"confidence\": 0.93},\n",
    "            {\"text\": \"Nonsteroidal anti-inflammatory drugs (NSAIDs) can cause gastric ulceration\", \"category\": \"pharmacology\", \"source\": \"adverse_effects\", \"confidence\": 0.90},\n",
    "            {\"text\": \"Statins reduce cholesterol synthesis by inhibiting HMG-CoA reductase\", \"category\": \"pharmacology\", \"source\": \"mechanism_of_action\", \"confidence\": 0.94},\n",
    "            {\"text\": \"Opioids can cause respiratory depression at high doses\", \"category\": \"pharmacology\", \"source\": \"toxicology\", \"confidence\": 0.91},\n",
    "            {\"text\": \"Drug interactions can alter medication effectiveness and safety\", \"category\": \"pharmacology\", \"source\": \"clinical_pharmacology\", \"confidence\": 0.87},\n",
    "            \n",
    "            # PEDIATRICS\n",
    "            {\"text\": \"Sudden infant death syndrome risk is reduced by supine sleeping position\", \"category\": \"pediatrics\", \"source\": \"AAP_guidelines\", \"confidence\": 0.92},\n",
    "            {\"text\": \"Febrile seizures are common in children aged 6 months to 5 years\", \"category\": \"pediatrics\", \"source\": \"pediatric_neurology\", \"confidence\": 0.89},\n",
    "            {\"text\": \"Vaccination schedules protect children from preventable diseases\", \"category\": \"pediatrics\", \"source\": \"immunization_guidelines\", \"confidence\": 0.95},\n",
    "            {\"text\": \"Growth charts assess normal pediatric development\", \"category\": \"pediatrics\", \"source\": \"developmental_medicine\", \"confidence\": 0.86},\n",
    "            \n",
    "            # SURGERY\n",
    "            {\"text\": \"Appendicitis typically presents with right lower quadrant abdominal pain\", \"category\": \"surgery\", \"source\": \"surgical_diagnosis\", \"confidence\": 0.90},\n",
    "            {\"text\": \"Cholecystitis causes right upper quadrant pain and Murphy's sign\", \"category\": \"surgery\", \"source\": \"surgical_examination\", \"confidence\": 0.88},\n",
    "            {\"text\": \"Surgical site infections are prevented by proper sterile technique\", \"category\": \"surgery\", \"source\": \"infection_control\", \"confidence\": 0.91},\n",
    "            {\"text\": \"Bowel obstruction can cause abdominal distension and vomiting\", \"category\": \"surgery\", \"source\": \"surgical_emergencies\", \"confidence\": 0.87},\n",
    "            \n",
    "            # OBSTETRICS/GYNECOLOGY\n",
    "            {\"text\": \"Preeclampsia is defined by hypertension and proteinuria in pregnancy\", \"category\": \"obstetrics\", \"source\": \"ACOG_guidelines\", \"confidence\": 0.94},\n",
    "            {\"text\": \"Folic acid supplementation prevents neural tube defects\", \"category\": \"obstetrics\", \"source\": \"preventive_medicine\", \"confidence\": 0.92},\n",
    "            {\"text\": \"Regular prenatal care improves maternal and fetal outcomes\", \"category\": \"obstetrics\", \"source\": \"prenatal_guidelines\", \"confidence\": 0.89},\n",
    "            {\"text\": \"Gestational diabetes increases risk of macrosomia\", \"category\": \"obstetrics\", \"source\": \"maternal_fetal_medicine\", \"confidence\": 0.86},\n",
    "            \n",
    "            # EMERGENCY MEDICINE\n",
    "            {\"text\": \"Advanced Cardiac Life Support (ACLS) protocols guide cardiac arrest management\", \"category\": \"emergency_medicine\", \"source\": \"AHA_guidelines\", \"confidence\": 0.95},\n",
    "            {\"text\": \"Trauma patients require primary and secondary survey assessment\", \"category\": \"emergency_medicine\", \"source\": \"ATLS_guidelines\", \"confidence\": 0.93},\n",
    "            {\"text\": \"Anaphylaxis is treated with intramuscular epinephrine\", \"category\": \"emergency_medicine\", \"source\": \"allergy_guidelines\", \"confidence\": 0.96},\n",
    "            {\"text\": \"Glasgow Coma Scale assesses level of consciousness\", \"category\": \"emergency_medicine\", \"source\": \"neurological_assessment\", \"confidence\": 0.91},\n",
    "            \n",
    "            # NEPHROLOGY\n",
    "            {\"text\": \"Chronic kidney disease is staged based on glomerular filtration rate\", \"category\": \"nephrology\", \"source\": \"KDIGO_guidelines\", \"confidence\": 0.93},\n",
    "            {\"text\": \"Acute kidney injury can be prerenal, intrinsic, or postrenal\", \"category\": \"nephrology\", \"source\": \"nephrology_classification\", \"confidence\": 0.90},\n",
    "            {\"text\": \"Dialysis is indicated for severe uremia or fluid overload\", \"category\": \"nephrology\", \"source\": \"renal_replacement_therapy\", \"confidence\": 0.88},\n",
    "            {\"text\": \"Proteinuria indicates glomerular kidney disease\", \"category\": \"nephrology\", \"source\": \"laboratory_findings\", \"confidence\": 0.85},\n",
    "            \n",
    "            # GASTROENTEROLOGY\n",
    "            {\"text\": \"Gastroesophageal reflux disease (GERD) causes heartburn and regurgitation\", \"category\": \"gastroenterology\", \"source\": \"clinical_presentation\", \"confidence\": 0.87},\n",
    "            {\"text\": \"Peptic ulcer disease is commonly caused by Helicobacter pylori\", \"category\": \"gastroenterology\", \"source\": \"etiology\", \"confidence\": 0.91},\n",
    "            {\"text\": \"Inflammatory bowel disease includes Crohn's disease and ulcerative colitis\", \"category\": \"gastroenterology\", \"source\": \"disease_classification\", \"confidence\": 0.93},\n",
    "            {\"text\": \"Cirrhosis can lead to portal hypertension and ascites\", \"category\": \"gastroenterology\", \"source\": \"hepatology\", \"confidence\": 0.89},\n",
    "            \n",
    "            # ONCOLOGY\n",
    "            {\"text\": \"Cancer staging determines prognosis and treatment approach\", \"category\": \"oncology\", \"source\": \"cancer_guidelines\", \"confidence\": 0.92},\n",
    "            {\"text\": \"Chemotherapy targets rapidly dividing cancer cells\", \"category\": \"oncology\", \"source\": \"cancer_treatment\", \"confidence\": 0.90},\n",
    "            {\"text\": \"Tumor markers can aid in cancer diagnosis and monitoring\", \"category\": \"oncology\", \"source\": \"laboratory_oncology\", \"confidence\": 0.84},\n",
    "            {\"text\": \"Radiation therapy delivers targeted energy to destroy cancer cells\", \"category\": \"oncology\", \"source\": \"radiation_oncology\", \"confidence\": 0.88},\n",
    "            \n",
    "            # DERMATOLOGY\n",
    "            {\"text\": \"Melanoma is the most dangerous form of skin cancer\", \"category\": \"dermatology\", \"source\": \"dermatopathology\", \"confidence\": 0.91},\n",
    "            {\"text\": \"Topical corticosteroids treat inflammatory skin conditions\", \"category\": \"dermatology\", \"source\": \"dermatologic_therapeutics\", \"confidence\": 0.86},\n",
    "            {\"text\": \"Skin biopsy provides definitive diagnosis of skin lesions\", \"category\": \"dermatology\", \"source\": \"diagnostic_procedures\", \"confidence\": 0.89},\n",
    "            \n",
    "            # RHEUMATOLOGY\n",
    "            {\"text\": \"Rheumatoid arthritis is an autoimmune inflammatory joint disease\", \"category\": \"rheumatology\", \"source\": \"autoimmune_diseases\", \"confidence\": 0.93},\n",
    "            {\"text\": \"Disease-modifying antirheumatic drugs (DMARDs) slow joint destruction\", \"category\": \"rheumatology\", \"source\": \"treatment_guidelines\", \"confidence\": 0.90},\n",
    "            {\"text\": \"Systemic lupus erythematosus affects multiple organ systems\", \"category\": \"rheumatology\", \"source\": \"connective_tissue_disorders\", \"confidence\": 0.88},\n",
    "        ]\n",
    "    \n",
    "    def verify_fact(self, fact, threshold: float = 0.7) -> float:\n",
    "        if not self.knowledge_base or self.knowledge_embeddings.size == 0:\n",
    "            return 0.5  # Neutral score if no knowledge base\n",
    "        \n",
    "        try:\n",
    "            # Generate embedding for the fact\n",
    "            fact_text = fact.text if hasattr(fact, 'text') else str(fact)\n",
    "            fact_embedding = self.embedding_model.encode([fact_text])\n",
    "            \n",
    "            # Calculate similarities\n",
    "            similarities = np.dot(fact_embedding, self.knowledge_embeddings.T)[0]\n",
    "            \n",
    "            # Get the best matching facts\n",
    "            best_matches_idx = np.argsort(similarities)[-3:][::-1]  # Top 3 matches\n",
    "            best_similarities = similarities[best_matches_idx]\n",
    "            \n",
    "            # Calculate confidence score based on similarity and source confidence\n",
    "            confidence_scores = []\n",
    "            for idx, similarity in zip(best_matches_idx, best_similarities):\n",
    "                kb_fact = self.knowledge_base[idx]\n",
    "                source_confidence = kb_fact.get('confidence', 0.8)\n",
    "                \n",
    "                # Combine similarity and source confidence\n",
    "                combined_score = similarity * source_confidence\n",
    "                confidence_scores.append(combined_score)\n",
    "            \n",
    "            # Use the best match\n",
    "            final_confidence = max(confidence_scores) if confidence_scores else 0.0\n",
    "            \n",
    "            # Apply threshold-based adjustment\n",
    "            if max(best_similarities) < threshold:\n",
    "                final_confidence *= 0.5  # Penalize low similarity\n",
    "            \n",
    "            # Update fact with KB score if it's an AtomicFact object\n",
    "            if hasattr(fact, 'kb_score'):\n",
    "                fact.kb_score = final_confidence\n",
    "            \n",
    "            return min(1.0, max(0.0, final_confidence))\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in local knowledge base verification: {e}\")\n",
    "            return 0.0\n",
    "    \n",
    "    def search_facts(self, query: str, limit = 10) -> List[Dict]:\n",
    "        \"\"\"Search for facts similar to the query\"\"\"\n",
    "        if not self.knowledge_base or self.knowledge_embeddings.size == 0:\n",
    "            return []\n",
    "        \n",
    "        try:\n",
    "            # Generate embedding for the query\n",
    "            query_embedding = self.embedding_model.encode([query])\n",
    "            \n",
    "            # Calculate similarities\n",
    "            similarities = np.dot(query_embedding, self.knowledge_embeddings.T)[0]\n",
    "            \n",
    "            # Get top matches\n",
    "            top_indices = np.argsort(similarities)[-limit:][::-1]\n",
    "            \n",
    "            results = []\n",
    "            for idx in top_indices:\n",
    "                fact = self.knowledge_base[idx].copy()\n",
    "                fact['similarity_score'] = similarities[idx]\n",
    "                results.append(fact)\n",
    "            \n",
    "            return results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error searching local knowledge base: {e}\")\n",
    "            return []\n",
    "    \n",
    "    def get_knowledge_stats(self):\n",
    "        \"\"\"Get statistics about the local knowledge base\"\"\"\n",
    "        if not self.knowledge_base:\n",
    "            return {\"total_facts\": 0, \"categories\": [], \"sources\": []}\n",
    "        \n",
    "        categories = {}\n",
    "        sources = {}\n",
    "        \n",
    "        for fact in self.knowledge_base:\n",
    "            cat = fact.get('category', 'unknown')\n",
    "            src = fact.get('source', 'unknown')\n",
    "            \n",
    "            categories[cat] = categories.get(cat, 0) + 1\n",
    "            sources[src] = sources.get(src, 0) + 1\n",
    "        \n",
    "        return {\n",
    "            \"total_facts\": len(self.knowledge_base),\n",
    "            \"categories\": categories,\n",
    "            \"sources\": sources,\n",
    "            \"has_embeddings\": self.knowledge_embeddings is not None and self.knowledge_embeddings.size > 0\n",
    "        }\n",
    "    \n",
    "    def add_facts(self, new_facts: List[Dict]):\n",
    "        \"\"\"Add new facts to the knowledge base\"\"\"\n",
    "        self.knowledge_base.extend(new_facts)\n",
    "\n",
    "        # Regenerate embeddings\n",
    "        fact_texts = [fact['text'] for fact in self.knowledge_base]\n",
    "        print(f\"Regenerating embeddings for {len(fact_texts)} facts...\")\n",
    "        self.knowledge_embeddings = self.embedding_model.encode(fact_texts)\n",
    "\n",
    "        print(f\"Added {len(new_facts)} new facts to knowledge base\")\n",
    "\n",
    "\n",
    "class LocalKnowledgeBaseVerifier:\n",
    "    \"\"\"Drop-in replacement using local knowledge base\"\"\"\n",
    "    \n",
    "    def __init__(self, knowledge_sources: List[str] = None):\n",
    "        self.knowledge_sources = knowledge_sources or [\"Local Medical DB\"]\n",
    "        self.local_kb = LocalMedicalKnowledgeBase()\n",
    "        \n",
    "        print(\"Local Knowledge Base Verifier initialized\")\n",
    "        stats = self.local_kb.get_knowledge_stats()\n",
    "        print(f\"Loaded {stats['total_facts']} facts across {len(stats['categories'])} categories\")\n",
    "    \n",
    "    def verify_fact(self, fact, threshold: float = 0.7) -> float:\n",
    "        \"\"\"Verify fact against local knowledge base\"\"\"\n",
    "        return self.local_kb.verify_fact(fact, threshold)\n",
    "    \n",
    "    def search_facts(self, query: str, limit = 10) -> List[Dict]:\n",
    "        \"\"\"Search for relevant facts\"\"\"\n",
    "        return self.local_kb.search_facts(query, limit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b356bf-16d8-4be2-8fbc-231ced7e9d74",
   "metadata": {},
   "source": [
    "## Fact Verifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8601f2a-8713-42a9-918e-e243e1bd3abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FactualRewardCalculator:\n",
    "    def __init__(self, agreement_threshold):\n",
    "        self.agreement_threshold = agreement_threshold\n",
    "\n",
    "    def extract_think_content(self, generation):\n",
    "        \"\"\"Extract content from <think> tags\"\"\"\n",
    "        think_match = re.search(r'<think>(.*?)</think>', generation, re.DOTALL | re.IGNORECASE)\n",
    "        if think_match:\n",
    "            content = think_match.group(1).strip()\n",
    "            return re.sub(r'^Reasoning:\\s*', '', content)\n",
    "        return \"\"\n",
    "\n",
    "    def compute_factual_reward(self, facts_list):\n",
    "        if not facts_list:\n",
    "            return {\n",
    "                \"factual_reward\": 0.0,\n",
    "                \"factual_analysis\": {\n",
    "                    \"factual_reward\": 0.0,\n",
    "                    \"individual_rewards\": [],\n",
    "                    \"agreement_rate\": 0.0,\n",
    "                    \"avg_llm_score\": 0.0,\n",
    "                    \"avg_kb_score\": 0.0,\n",
    "                    \"num_facts\": 0\n",
    "                },\n",
    "                \"error\": \"No facts provided\"\n",
    "            }\n",
    "        atomic_facts = []\n",
    "        for fact_item in facts_list:\n",
    "            if isinstance(fact_item, AtomicFact):\n",
    "                atomic_facts.append(fact_item)\n",
    "            else:\n",
    "                fact = AtomicFact(\n",
    "                    text=fact_item.get(\"text\", \"\"),\n",
    "                    category=fact_item.get(\"category\", \"unknown\"),\n",
    "                    source_sentence=fact_item.get(\"source_sentence\", \"\")\n",
    "                )\n",
    "                fact.llm_score = fact_item.get(\"llm_score\", 0.0)\n",
    "                fact.kb_score = fact_item.get(\"kb_score\", 0.0)\n",
    "                atomic_facts.append(fact)\n",
    "\n",
    "        individual_rewards = []\n",
    "        agreement_count = 0\n",
    "        llm_scores = []\n",
    "        kb_scores = []\n",
    "\n",
    "        for fact in atomic_facts:\n",
    "            fact_reward = self._compute_individual_fact_reward(fact)\n",
    "            individual_rewards.append(fact_reward)\n",
    "\n",
    "            if abs(fact.llm_score - fact.kb_score) <= self.agreement_threshold:\n",
    "                agreement_count += 1\n",
    "\n",
    "            llm_scores.append(fact.llm_score)\n",
    "            kb_scores.append(fact.kb_score)\n",
    "\n",
    "        factual_reward = sum(individual_rewards) / len(individual_rewards) if individual_rewards else 0.0\n",
    "\n",
    "        agreement_rate = agreement_count / len(atomic_facts) if atomic_facts else 0.0\n",
    "        avg_llm_score = sum(llm_scores) / len(llm_scores) if llm_scores else 0.0\n",
    "        avg_kb_score = sum(kb_scores) / len(kb_scores) if kb_scores else 0.0\n",
    "\n",
    "        factual_analysis = {\n",
    "            \"factual_reward\": factual_reward,\n",
    "            \"individual_rewards\": individual_rewards,\n",
    "            \"agreement_rate\": agreement_rate,\n",
    "            \"avg_llm_score\": avg_llm_score,\n",
    "            \"avg_kb_score\": avg_kb_score,\n",
    "            \"num_facts\": len(atomic_facts)\n",
    "        }\n",
    "\n",
    "        return {\n",
    "            \"factual_reward\": factual_reward,\n",
    "            \"factual_analysis\": factual_analysis,\n",
    "            \"error\": None\n",
    "        }\n",
    "\n",
    "    def _compute_individual_fact_reward(self, fact):\n",
    "        llm_score = fact.llm_score\n",
    "        kb_score = fact.kb_score\n",
    "\n",
    "        # Weight LLM more heavily (e.g., 70% LLM, 30% KB)\n",
    "        llm_weight = 0.7\n",
    "        kb_weight = 0.3\n",
    "\n",
    "        base_score = llm_weight * llm_score + kb_weight * kb_score\n",
    "\n",
    "        agreement = abs(llm_score - kb_score) <= self.agreement_threshold\n",
    "        if agreement:\n",
    "            return base_score\n",
    "        else:\n",
    "            return base_score * 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8132d217-b78c-49fb-b814-80555c323a06",
   "metadata": {},
   "source": [
    "## Atomic Fact Verification System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b910a441-b0d8-407f-aa6c-48ec504a0fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtomicFactVerificationSystem:\n",
    "    def __init__(self,\n",
    "                 agreement_threshold,\n",
    "                 umls_api_key=None):\n",
    "        self.extractor = AtomicFactExtractor(model_name=MODEL_NAME\n",
    "                                             region_name=\"us-east-1\")\n",
    "        self.llm_verifier = LLMJudgeVerifier(model_name=MODEL_NAME,\n",
    "                                             region_name=\"us-east-1\")\n",
    "        self.kb_verifier = LocalKnowledgeBaseVerifier()\n",
    "        self.reward_calculator = FactualRewardCalculator(agreement_threshold)\n",
    "        self.training_mode = False\n",
    "\n",
    "    def process_response(self, reasoning_text, context=\"\"):\n",
    "        try:\n",
    "            facts = self.extractor.extract_facts(reasoning_text)\n",
    "\n",
    "            if not facts:\n",
    "                return {\n",
    "                    \"facts\": [],\n",
    "                    \"factual_analysis\": self._empty_analysis(),\n",
    "                    \"error\": \"No facts extracted\"\n",
    "                }\n",
    "\n",
    "            # Check if we're in training mode to skip expensive verification\n",
    "            if hasattr(self, 'training_mode') and self.training_mode:\n",
    "                for fact in facts:\n",
    "                    fact.llm_score = self._get_training_heuristic_score(fact)\n",
    "                    fact.kb_score = self._get_training_heuristic_score(fact)\n",
    "            else:\n",
    "                for fact in facts:\n",
    "                    try:\n",
    "                        # LLM verification\n",
    "                        self.llm_verifier.verify_fact(fact, context)\n",
    "                        # KB verification\n",
    "                        self.kb_verifier.verify_fact(fact)\n",
    "                        # Small delay to avoid rate limits during evaluation\n",
    "                        time.sleep(4)\n",
    "                    except Exception as verification_error:\n",
    "                        print(f\"Verification error for fact: {verification_error}\")\n",
    "                        # Use fallback scores if verification fails\n",
    "                        fact.llm_score = getattr(fact, 'llm_score', 0.5)\n",
    "                        fact.kb_score = getattr(fact, 'kb_score', 0.5)\n",
    "\n",
    "            # Convert facts to dictionaries for consistent handling\n",
    "            facts_as_dicts = [self._fact_to_dict(fact) for fact in facts]\n",
    "\n",
    "            # Calculate factual reward\n",
    "            factual_analysis = self.reward_calculator.compute_factual_reward(facts_as_dicts)\n",
    "\n",
    "            return {\n",
    "                \"facts\": facts_as_dicts,\n",
    "                \"factual_analysis\": factual_analysis,\n",
    "                \"error\": None\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\n",
    "                \"facts\": [],\n",
    "                \"factual_analysis\": self._empty_analysis(),\n",
    "                \"error\": str(e)\n",
    "            }\n",
    "\n",
    "    def _get_training_heuristic_score(self, fact):\n",
    "        fact_text = fact.text.lower()\n",
    "        # Simple heuristics based on medical fact patterns\n",
    "        if any(keyword in fact_text for keyword in ['patient', 'symptom', 'diagnosis', 'treatment']):\n",
    "            # Medical context facts get moderate-high scores\n",
    "            return 0.7\n",
    "        elif any(keyword in fact_text for keyword in ['anatomy', 'physiology', 'mechanism']):\n",
    "            # Basic medical knowledge gets high scores\n",
    "            return 0.8\n",
    "        elif len(fact_text) < 20:\n",
    "            # Very short facts might be incomplete\n",
    "            return 0.5\n",
    "        elif any(keyword in fact_text for keyword in ['may', 'might', 'could', 'possibly']):\n",
    "            # Uncertain statements get lower scores\n",
    "            return 0.6\n",
    "        else:\n",
    "            # Default reasonable score\n",
    "            return 0.7\n",
    "\n",
    "    def _fact_to_dict(self, fact: AtomicFact):\n",
    "        return {\n",
    "            \"text\": fact.text,\n",
    "            \"category\": fact.category,\n",
    "            \"llm_score\": fact.llm_score,\n",
    "            \"kb_score\": fact.kb_score,\n",
    "            \"source_sentence\": fact.source_sentence\n",
    "        }\n",
    "\n",
    "    def _empty_analysis(self):\n",
    "        return {\n",
    "            \"factual_reward\": 0.0,\n",
    "            \"individual_rewards\": [],\n",
    "            \"agreement_rate\": 0.0,\n",
    "            \"avg_llm_score\": 0.0,\n",
    "            \"avg_kb_score\": 0.0,\n",
    "            \"num_facts\": 0\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed73176f-5fd9-470d-9f07-30a18a882574",
   "metadata": {},
   "source": [
    "# Training and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0441fff-c0cb-462d-a126-5ceb59547ee1",
   "metadata": {},
   "source": [
    "## MetricsTracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e94fef-8a09-4a0c-a91f-0b204f638a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MetricsTracker:\n",
    "    def __init__(self):\n",
    "        self.epoch_metrics = []\n",
    "\n",
    "    def add_epoch_metrics(self, epoch, metrics):\n",
    "        self.epoch_metrics.append({\n",
    "            'epoch': epoch,\n",
    "            'correctness_violations_rate': metrics.get('correctness_violations_rate', 0),\n",
    "            'answer_leaking_violations_rate': metrics.get('answer_leaking_violations_rate', 0), \n",
    "            'format_violations_rate': metrics.get('format_violations_rate', 0),\n",
    "            'factual_violations_rate': metrics.get('factual_violations_rate', 0),\n",
    "            'bad_ood_high_rewards_rate': metrics.get('bad_ood_high_rewards_rate', 0),\n",
    "            'avg_reward': metrics.get('avg_reward', 0),\n",
    "            'avg_factual': metrics.get('avg_factual', 0)\n",
    "        })\n",
    "\n",
    "    \n",
    "    def print_stage_summary(self, stage_name, start_epoch=0, end_epoch=None):\n",
    "        \"\"\"Print mean and std for a training stage\"\"\"\n",
    "        if end_epoch is None:\n",
    "            end_epoch = len(self.epoch_metrics)\n",
    "            \n",
    "        stage_data = self.epoch_metrics[start_epoch:end_epoch]\n",
    "        if not stage_data:\n",
    "            print(f\"No data for {stage_name}\")\n",
    "            return\n",
    "\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"{stage_name.upper()} SUMMARY (Epochs {start_epoch+1}-{end_epoch})\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Extract values for each metric\n",
    "        metrics = {\n",
    "            'Correctness Violations': [d['correctness_violations_rate'] for d in stage_data],\n",
    "            'Answer Leaking': [d['answer_leaking_violations_rate'] for d in stage_data],\n",
    "            'Format Violations': [d['format_violations_rate'] for d in stage_data], \n",
    "            'Factual Violations': [d['factual_violations_rate'] for d in stage_data],\n",
    "            'Bad OOD Rewards': [d['bad_ood_high_rewards_rate'] for d in stage_data],\n",
    "            'Average Reward': [d['avg_reward'] for d in stage_data],\n",
    "            'Factual Reward': [d['avg_factual'] for d in stage_data]\n",
    "        }\n",
    "        \n",
    "        # Print statistics\n",
    "        for metric_name, values in metrics.items():\n",
    "            mean_val = np.mean(values)\n",
    "            std_val = np.std(values)\n",
    "            print(f\"{metric_name:20s}: {mean_val:.2f} ± {std_val:.2f}\")\n",
    "            \n",
    "        # Calculate improvement (first vs last epoch)\n",
    "        if len(stage_data) > 1:\n",
    "            print(f\"\\n{stage_name} Improvements:\")\n",
    "            first_epoch = stage_data[0]\n",
    "            last_epoch = stage_data[-1]\n",
    "            \n",
    "            # For violation rates, improvement is decrease (negative change)\n",
    "            for metric in ['correctness_violations_rate', 'answer_leaking_violations_rate', \n",
    "                          'format_violations_rate', 'factual_violations_rate', 'bad_ood_high_rewards_rate']:\n",
    "                change = last_epoch[metric] - first_epoch[metric]\n",
    "                improvement = -change  # Negative change is improvement for violations\n",
    "                print(f\"  {metric.replace('_', ' ').title():25s}: {improvement:+.2f}\")\n",
    "            \n",
    "            # For rewards, improvement is increase (positive change) \n",
    "            for metric in ['avg_reward', 'avg_factual']:\n",
    "                change = last_epoch[metric] - first_epoch[metric]\n",
    "                print(f\"  {metric.replace('_', ' ').title():25s}: {change:+.2f}\")\n",
    "\n",
    "\n",
    "    def print_metrics_with_std(self, metrics, title=\"METRICS\", include_std=True):\n",
    "        \"\"\"Print metrics in consistent format with optional standard deviations\"\"\"\n",
    "        print(f\"\\n{title}\")\n",
    "        print(\"-\" * len(title))\n",
    "        \n",
    "        # Define metric display configurations\n",
    "        metric_configs = [\n",
    "            ('correctness_violations_rate', 'Correctness Violations', 'correctness_violations_std'),\n",
    "            ('answer_leaking_violations_rate', 'Answer Leakage Rate', 'answer_leaking_violations_std'),\n",
    "            ('format_violations_rate', 'Bad Format Rate', 'format_violations_std'),\n",
    "            ('factual_violations_rate', 'Factual Violations', 'factual_violations_std'),\n",
    "            ('bad_ood_high_rewards_rate', 'High Rewards Rate', 'bad_ood_high_rewards_std'),\n",
    "            ('avg_reward', 'Average Reward', 'std_reward'),\n",
    "            ('avg_factual', 'Factual Reward', None),\n",
    "            ('accuracy', 'Accuracy', None)\n",
    "        ]\n",
    "        \n",
    "        for metric_key, display_name, std_key in metric_configs:\n",
    "            if metric_key in metrics:\n",
    "                value = metrics[metric_key]\n",
    "                if include_std and std_key and std_key in metrics:\n",
    "                    std_value = metrics[std_key]\n",
    "                    print(f\"{display_name}: {value:.2f} (±{std_value:.2f})\")\n",
    "                else:\n",
    "                    print(f\"{display_name}: {value:.2f}\")\n",
    "    \n",
    "    def add_adversarial_stage_metrics(self, pre_metrics, post_metrics):\n",
    "        \"\"\"Add special adversarial training stage metrics\"\"\"\n",
    "        # Store pre-adversarial as epoch -1\n",
    "        self.add_epoch_metrics(-1, pre_metrics)\n",
    "        \n",
    "        # Store post-adversarial as epoch 999\n",
    "        self.add_epoch_metrics(999, post_metrics)\n",
    "        \n",
    "        # Calculate improvement metrics\n",
    "        improvement_metrics = {}\n",
    "        for key in pre_metrics:\n",
    "            if key in post_metrics and isinstance(pre_metrics[key], (int, float)):\n",
    "                improvement_metrics[f\"{key}_improvement\"] = post_metrics[key] - pre_metrics[key]\n",
    "        \n",
    "        # Store improvement metrics\n",
    "        self.add_epoch_metrics(1000, improvement_metrics)\n",
    "    \n",
    "    def print_adversarial_impact_analysis(self):\n",
    "        \"\"\"Print comprehensive adversarial training impact analysis\"\"\"\n",
    "        if -1 in self.epoch_metrics and 999 in self.epoch_metrics:\n",
    "            pre_metrics = self.epoch_metrics[-1]\n",
    "            post_metrics = self.epoch_metrics[999]\n",
    "            \n",
    "            print(\"\\n\" + \"=\" * 60)\n",
    "            print(\"ADVERSARIAL TRAINING IMPACT ANALYSIS\")\n",
    "            print(\"=\" * 60)\n",
    "            \n",
    "            print(\"\\nPRE-ADVERSARIAL PERFORMANCE:\")\n",
    "            self.print_metrics_with_std(pre_metrics, \"\", include_std=True)\n",
    "            \n",
    "            print(\"\\nPOST-ADVERSARIAL PERFORMANCE:\")\n",
    "            self.print_metrics_with_std(post_metrics, \"\", include_std=True)\n",
    "                            \n",
    "        else:\n",
    "            print(\"Adversarial training metrics not found\")\n",
    "    \n",
    "    \n",
    "    def save_metrics(self, filepath=\"training_metrics.json\"):\n",
    "        \"\"\"Save metrics to JSON file\"\"\"\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(self.epoch_metrics, f, indent=2)\n",
    "        print(f\"Metrics saved to {filepath}\")\n",
    "\n",
    "def monitor_memory():\n",
    "    if torch.cuda.is_available():\n",
    "        memory_allocated = torch.cuda.memory_allocated() / 1024 ** 3\n",
    "        memory_reserved = torch.cuda.memory_reserved() / 1024 ** 3\n",
    "        print(f\"GPU Memory - Allocated: {memory_allocated:.2f}GB, Reserved: {memory_reserved:.2f}GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03946959-47f3-49bb-bc0e-dd0b9903885d",
   "metadata": {},
   "source": [
    "## With Llama3.2-3B-Instruct-SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ad382b-02fe-4829-b7bc-2b3241debe9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "train_df = pd.read_json(\"medqa_train.json\")\n",
    "test_df = pd.read_json(\"medqa_test.json\")\n",
    "\n",
    "reward_config = {\n",
    "    'w_b': 1.0,\n",
    "    'w_a': 0.2,\n",
    "    'w_s': 0.2,\n",
    "    'w_fact': 0.2,\n",
    "    'tau_answer': 0.7,\n",
    "    'tau_preamble': 15,\n",
    "    'lambda_s': 1.0\n",
    "}\n",
    "print(\"Initial memory state:\")\n",
    "monitor_memory()\n",
    "model_url = \"Llama3.2-3B-Instruct-SFT\"\n",
    "\n",
    "trainer_1 = PolicyTrainer(\n",
    "    model_path=model_url,\n",
    "    reward_config=reward_config,\n",
    "    use_baseline=True,\n",
    "    umls_api_key=UMLS_API_KEY\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "train_subset_df = train_df.iloc[:50]\n",
    "train_subset_df.to_json(\"medqa_train_sample.json\", orient='records', indent=4)\n",
    "\n",
    "comparison_results = trainer_1.compare_training_approaches(\n",
    "    train_data_path=\"medqa_train_sample.json\",\n",
    "    test_data_path=\"medqa_test.json\",\n",
    "    stage1_epochs=3, \n",
    "    stage2_epochs=1,\n",
    "    max_eval_examples=50\n",
    ")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e80b934-c38b-4d4a-b526-9b45e18f2017",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "trainer = PolicyTrainer(\n",
    "    model_path=model_url,\n",
    "    reward_config=reward_config,\n",
    "    use_baseline=True,\n",
    "    umls_api_key=UMLS_API_KEY\n",
    ")\n",
    "\n",
    "\n",
    "print(\"=\" * 80)\n",
    "train_subset_df = train_df.iloc[:300]\n",
    "train_subset_df.to_json(\"medqa_train_sample.json\", orient='records', indent=4)\n",
    "\n",
    "trainer.train_two_stage_pipeline(\n",
    "    train_data_path=\"medqa_train_sample.json\",\n",
    "    stage1_epochs=2,\n",
    "    stage2_epochs=2,\n",
    "    batch_size=8,\n",
    "    adversarial_frequency=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e38c8c-968b-4629-95c7-4bbbd0717fb3",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## OOD with MMLU-PRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecd417f-4bf6-43e4-9992-98eb36913a90",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Local path where model was saved (download from S3 if needed)\n",
    "local_model_dir = LOCAL_MODEL_DIR\n",
    "\n",
    "# Configuration for reward function\n",
    "reward_config = {\n",
    "    'w_b': 1.0,\n",
    "    'w_a': 0.2,\n",
    "    'w_s': 0.2,\n",
    "    'w_f': 0.2,\n",
    "    'tau_answer': 0.7,\n",
    "    'tau_preamble': 15,\n",
    "    'lambda_s': 1.0\n",
    "}\n",
    "\n",
    "trainer = PolicyTrainer(\n",
    "    model_path=local_model_dir,\n",
    "    reward_config=reward_config,\n",
    "    use_baseline=True,\n",
    "    umls_api_key=UMLS_API_KEY\n",
    ")\n",
    "\n",
    "# 1. Reload model + tokenizer\n",
    "trainer.model = AutoModelForCausalLM.from_pretrained(local_model_dir).to(trainer.device)\n",
    "trainer.tokenizer = AutoTokenizer.from_pretrained(local_model_dir)\n",
    "\n",
    "# 2. Reload baseline network\n",
    "baseline_path = os.path.join(local_model_dir, \"baseline_network.pt\")\n",
    "trainer.baseline_network.load_state_dict(torch.load(baseline_path, map_location=trainer.device))\n",
    "trainer.baseline_network.to(trainer.device)\n",
    "trainer.baseline_network.eval()\n",
    "\n",
    "print(\"\\nRunning final evaluation on the test set...\")\n",
    "final_results = trainer.evaluate_model(\"mmlu_pro_health_test.json\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92398aa-2aea-444a-b650-cab6182765ca",
   "metadata": {},
   "source": [
    "## With Llama3.2-3B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08acddf3-b198-43d3-834e-e45316b79f89",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "train_df = pd.read_json(\"medqa_train.json\")\n",
    "test_df = pd.read_json(\"medqa_test.json\")\n",
    "\n",
    "reward_config = {\n",
    "    'w_b': 1.0,\n",
    "    'w_a': 0.2,\n",
    "    'w_s': 0.2,\n",
    "    'w_fact': 0.2,\n",
    "    'tau_answer': 0.7,\n",
    "    'tau_preamble': 15,\n",
    "    'lambda_s': 1.0\n",
    "}\n",
    "print(\"Initial memory state:\")\n",
    "monitor_memory()\n",
    "model_url = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "trainer_3 = PolicyTrainer(\n",
    "    model_path=model_url,\n",
    "    reward_config=reward_config,\n",
    "    use_baseline=True,\n",
    "    umls_api_key=UMLS_API_KEY\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "train_subset_df = train_df.iloc[:50]\n",
    "train_subset_df.to_json(\"medqa_train_sample.json\", orient='records', indent=4)\n",
    "\n",
    "comparison_results = trainer_3.compare_training_approaches(\n",
    "    train_data_path=\"medqa_train_sample.json\",\n",
    "    test_data_path=\"medqa_test.json\",\n",
    "    stage1_epochs=3, \n",
    "    stage2_epochs=1,\n",
    "    max_eval_examples=50\n",
    ")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7269f56a-0a8a-42d1-a4a1-7e661d4d4954",
   "metadata": {},
   "source": [
    "## OOD with MMLU-PRO (OG llama)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef834932-6382-42eb-bb64-fbbfd43541ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "local_model_dir = LOCAL_MODEL_DIR\n",
    "\n",
    "# Configuration for reward function\n",
    "reward_config = {\n",
    "    'w_b': 1.0,\n",
    "    'w_a': 0.2,\n",
    "    'w_s': 0.2,\n",
    "    'w_f': 0.2,\n",
    "    'tau_answer': 0.7,\n",
    "    'tau_preamble': 15,\n",
    "    'lambda_s': 1.0\n",
    "}\n",
    "\n",
    "trainer_2 = PolicyTrainer(\n",
    "    model_path=local_model_dir,\n",
    "    reward_config=reward_config,\n",
    "    use_baseline=True,\n",
    "    umls_api_key=UMLS_API_KEY\n",
    ")\n",
    "\n",
    "# 1. Reload model + tokenizer\n",
    "trainer_2.model = AutoModelForCausalLM.from_pretrained(local_model_dir)\n",
    "trainer_2.tokenizer = AutoTokenizer.from_pretrained(local_model_dir)\n",
    "\n",
    "# 2. Reload baseline network\n",
    "baseline_path = os.path.join(local_model_dir, \"baseline_network.pt\")\n",
    "trainer_2.baseline_network.load_state_dict(torch.load(baseline_path, map_location=trainer_2.device))\n",
    "trainer_2.baseline_network.to(trainer.device)\n",
    "trainer_2.baseline_network.eval()\n",
    "\n",
    "print(\"\\nRunning final evaluation on the test set...\")\n",
    "final_results = trainer_2.evaluate_model(\"mmlu_pro_health_test.json\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf5d241-2b03-4356-a78a-0f96b63f24c1",
   "metadata": {},
   "source": [
    "## With Qwen2.5-3B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a49359-1ca4-4283-ac99-ce94c85c5d65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "train_df = pd.read_json(\"medqa_train.json\")\n",
    "test_df = pd.read_json(\"medqa_test.json\")\n",
    "\n",
    "reward_config = {\n",
    "    'w_b': 1.0,\n",
    "    'w_a': 0.2,\n",
    "    'w_s': 0.2,\n",
    "    'w_fact': 0.2,\n",
    "    'tau_answer': 0.7,\n",
    "    'tau_preamble': 15,\n",
    "    'lambda_s': 1.0\n",
    "}\n",
    "print(\"Initial memory state:\")\n",
    "monitor_memory()\n",
    "model_url = \"sft_Qwen2_5_3B_Instruct\"\n",
    "\n",
    "trainer_3 = PolicyTrainer(\n",
    "    model_path=model_url,\n",
    "    reward_config=reward_config,\n",
    "    use_baseline=True,\n",
    "    umls_api_key=UMLS_API_KEY\n",
    ")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "train_subset_df = train_df.iloc[:50]\n",
    "train_subset_df.to_json(\"medqa_train_sample.json\", orient='records', indent=4)\n",
    "\n",
    "comparison_results = trainer_3.compare_training_approaches(\n",
    "    train_data_path=\"medqa_train_sample.json\",\n",
    "    test_data_path=\"medqa_test.json\",\n",
    "    stage1_epochs=2,\n",
    "    stage2_epochs=1,\n",
    "    max_eval_examples=50\n",
    ")\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TRAINING COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
